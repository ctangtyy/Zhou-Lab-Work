{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca474f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "if (!requireNamespace(\"BiocManager\", quietly = TRUE)) {\n",
    "  install.packages(\"BiocManager\")\n",
    "}\n",
    "BiocManager::install(c(\"minfi\", \"limma\"))\n",
    "install.packages(\"caret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30298d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into ‘/u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669b202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436de3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: BiocGenerics\n",
      "\n",
      "\n",
      "Attaching package: ‘BiocGenerics’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    IQR, mad, sd, var, xtabs\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n",
      "    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n",
      "    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n",
      "    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n",
      "    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n",
      "    table, tapply, union, unique, unsplit, which.max, which.min\n",
      "\n",
      "\n",
      "Loading required package: GenomicRanges\n",
      "\n",
      "Loading required package: stats4\n",
      "\n",
      "Loading required package: S4Vectors\n",
      "\n",
      "\n",
      "Attaching package: ‘S4Vectors’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:utils’:\n",
      "\n",
      "    findMatches\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    expand.grid, I, unname\n",
      "\n",
      "\n",
      "Loading required package: IRanges\n",
      "\n",
      "Loading required package: GenomeInfoDb\n",
      "\n",
      "Loading required package: SummarizedExperiment\n",
      "\n",
      "Loading required package: MatrixGenerics\n",
      "\n",
      "Loading required package: matrixStats\n",
      "\n",
      "\n",
      "Attaching package: ‘MatrixGenerics’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n",
      "    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n",
      "    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n",
      "    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n",
      "    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n",
      "    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n",
      "    colWeightedMeans, colWeightedMedians, colWeightedSds,\n",
      "    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n",
      "    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n",
      "    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n",
      "    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n",
      "    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n",
      "    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n",
      "    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n",
      "    rowWeightedSds, rowWeightedVars\n",
      "\n",
      "\n",
      "Loading required package: Biobase\n",
      "\n",
      "Welcome to Bioconductor\n",
      "\n",
      "    Vignettes contain introductory material; view with\n",
      "    'browseVignettes()'. To cite Bioconductor, see\n",
      "    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘Biobase’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:MatrixGenerics’:\n",
      "\n",
      "    rowMedians\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:matrixStats’:\n",
      "\n",
      "    anyMissing, rowMedians\n",
      "\n",
      "\n",
      "Loading required package: Biostrings\n",
      "\n",
      "Loading required package: XVector\n",
      "\n",
      "\n",
      "Attaching package: ‘Biostrings’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    strsplit\n",
      "\n",
      "\n",
      "Loading required package: bumphunter\n",
      "\n",
      "Loading required package: foreach\n",
      "\n",
      "Loading required package: iterators\n",
      "\n",
      "Loading required package: parallel\n",
      "\n",
      "Loading required package: locfit\n",
      "\n",
      "locfit 1.5-9.9 \t 2024-03-01\n",
      "\n",
      "Setting options('download.file.method.GEOquery'='auto')\n",
      "\n",
      "Setting options('GEOquery.inmemory.gpl'=FALSE)\n",
      "\n",
      "\n",
      "Attaching package: ‘limma’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:BiocGenerics’:\n",
      "\n",
      "    plotMA\n",
      "\n",
      "\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Loading required package: lattice\n",
      "\n",
      "\n",
      "Attaching package: ‘survival’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:caret’:\n",
      "\n",
      "    cluster\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:minfi’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:Biostrings’:\n",
      "\n",
      "    collapse, intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:XVector’:\n",
      "\n",
      "    slice\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:Biobase’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:matrixStats’:\n",
      "\n",
      "    count\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:GenomicRanges’:\n",
      "\n",
      "    intersect, setdiff, union\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:GenomeInfoDb’:\n",
      "\n",
      "    intersect\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:IRanges’:\n",
      "\n",
      "    collapse, desc, intersect, setdiff, slice, union\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:S4Vectors’:\n",
      "\n",
      "    first, intersect, rename, setdiff, setequal, union\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:BiocGenerics’:\n",
      "\n",
      "    combine, intersect, setdiff, union\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘data.table’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:SummarizedExperiment’:\n",
      "\n",
      "    shift\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:GenomicRanges’:\n",
      "\n",
      "    shift\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:IRanges’:\n",
      "\n",
      "    shift\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:S4Vectors’:\n",
      "\n",
      "    first, second\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(minfi)\n",
    "library(limma)\n",
    "library(caret) # For data splitting\n",
    "library(survival)\n",
    "library(dplyr) \n",
    "\n",
    "library(data.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b8b326",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# transformed_methylation_data <- read.csv(\"/u/home/c/ctang04/Liver Project/data/transformed_log1k_methylation_data.csv\", row.names = 1, check.names=FALSE)\n",
    "# # Load phenotype data\n",
    "# phenotype_matrix <- read.csv(\"/u/home/c/ctang04/Liver Project/data/patients.info.for.liver.study_with.seq.data_2024.07.21.csv\", check.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6680a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>144965</li><li>5610</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 144965\n",
       "\\item 5610\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 144965\n",
       "2. 5610\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 144965   5610"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>647</li><li>69</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 647\n",
       "\\item 69\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 647\n",
       "2. 69\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 647  69"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the transformed methylation data\n",
    "transformed_methylation_data <- readRDS(\"/u/home/c/ctang04/Liver Project/data/transformed_methyl_data_frame.rds\")\n",
    "\n",
    "# Read the phenotype matrix\n",
    "phenotype_matrix <- readRDS(\"/u/home/c/ctang04/Liver Project/data/phenotype_matrix.rds\")\n",
    "\n",
    "dim(transformed_methylation_data)\n",
    "dim(phenotype_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02fdea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]    578 144965\n",
      "[1]    578 145034\n",
      "[1]    497 145034\n"
     ]
    }
   ],
   "source": [
    "# Extract unique sample aliases from phenotype data\n",
    "phenotype_sample_names <- phenotype_matrix$plasma_alias\n",
    "split_pheno_names <- unlist(strsplit(as.character(phenotype_sample_names), \"; \"))\n",
    "unique_sample_aliases <- unique(split_pheno_names)\n",
    "\n",
    "# Match sample names between methylation and phenotype data\n",
    "sample_names <- colnames(transformed_methylation_data)\n",
    "matches <- sapply(sample_names, function(sample) any(grepl(sample, unique_sample_aliases)))\n",
    "matched_sample_names <- sample_names[matches]\n",
    "\n",
    "# Filter methylation data to include only matched samples\n",
    "filtered_methyl_data <- transformed_methylation_data[, matched_sample_names, drop = FALSE]\n",
    "\n",
    "# Split phenotype data to match the filtered methylation data\n",
    "split_phenotype_data <- phenotype_matrix[rep(seq_len(nrow(phenotype_matrix)), sapply(phenotype_matrix$plasma_alias, function(x) length(strsplit(x, \"; \")[[1]]))), ]\n",
    "split_phenotype_data$plasma_alias <- unlist(strsplit(as.character(phenotype_matrix$plasma_alias), \"; \"))\n",
    "split_phenotype_data <- split_phenotype_data[!is.na(split_phenotype_data$plasma_alias), ]\n",
    "\n",
    "# Match common samples between methylation and phenotype data\n",
    "common_samples <- intersect(colnames(filtered_methyl_data), split_phenotype_data$plasma_alias)\n",
    "filtered_pheno_data <- split_phenotype_data[split_phenotype_data$plasma_alias %in% common_samples, ]\n",
    "filtered_pheno_data <- filtered_pheno_data[match(colnames(filtered_methyl_data), filtered_pheno_data$plasma_alias), ]\n",
    "\n",
    "# Combine methylation and phenotype data\n",
    "transposed_methyl_data <- as.data.frame(t(filtered_methyl_data))\n",
    "print(dim(transposed_methyl_data))\n",
    "combined_data <- cbind(transposed_methyl_data, filtered_pheno_data)\n",
    "print(dim(combined_data))\n",
    "\n",
    "# Ensure no NA values in 'child_pugh_score'\n",
    "combined_data_clean <- combined_data[!is.na(combined_data$child_pugh_score), ]\n",
    "print(dim(combined_data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f164c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_data_clean_aliases <- combined_data_clean[,\"plasma_alias\"]\n",
    "rownames(combined_data_clean) <- combined_data_clean_aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113a6cdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove duplicates in place by selecting the first sample for each donor\n",
    "combined_data_clean_dt <- as.data.table(combined_data_clean)\n",
    "\n",
    "# Assuming combined_data_clean has rownames that you want to preserve\n",
    "rownames_combined <- rownames(combined_data_clean)\n",
    "\n",
    "combined_data_clean <- combined_data_clean %>%\n",
    "  mutate(rowname = rownames_combined) %>%\n",
    "  group_by(donor_id) %>%\n",
    "  slice(1) %>%\n",
    "  ungroup()\n",
    "\n",
    "\n",
    "#head(combined_data_clean, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f564953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]    407 145035\n",
      "  [1] \"plasma-2333-P9-LV\"            \"plasma-2300-P9-LV\"           \n",
      "  [3] \"plasma-2270-P9-LV\"            \"plasma-1969-P9-LV\"           \n",
      "  [5] \"plasma-1976-P9-N\"             \"plasma-2742-P9-N\"            \n",
      "  [7] \"plasma-2800-P9-N\"             \"plasma-3931-P9-N\"            \n",
      "  [9] \"plasma-2518-P9-N\"             \"plasma-2357-P9-N\"            \n",
      " [11] \"plasma-2873-P9-N\"             \"plasma-1959-P9-N\"            \n",
      " [13] \"plasma-3038-P9-LV\"            \"plasma-2091-P9-N\"            \n",
      " [15] \"plasma-3390-P9-LV\"            \"plasma-2741-P9-N\"            \n",
      " [17] \"plasma-2104-P9-N\"             \"plasma-2188-P9-BD\"           \n",
      " [19] \"plasma-3627-P9-LV\"            \"plasma-3134-P9-N\"            \n",
      " [21] \"plasma-2733-P9-N\"             \"plasma-2743-P9-N\"            \n",
      " [23] \"plasma-2778-P9-PR\"            \"plasma-2370-P9-N\"            \n",
      " [25] \"plasma-2248-P9-N\"             \"plasma-3427-P9-N\"            \n",
      " [27] \"plasma-2549-P9-N\"             \"plasma-2090-P9-N\"            \n",
      " [29] \"plasma-2441-P9-N\"             \"plasma-2349-P9-N\"            \n",
      " [31] \"plasma-1991-P9-N\"             \"plasma-2296-P9-N\"            \n",
      " [33] \"plasma-2222-P9-N\"             \"plasma-2273-P9-N\"            \n",
      " [35] \"plasma-3620-P9-BD\"            \"plasma-2770-P9-N\"            \n",
      " [37] \"plasma-1970-P9-N\"             \"plasma-3608-P9-LV\"           \n",
      " [39] \"plasma-2794-P9-N\"             \"plasma-2757-P9-N\"            \n",
      " [41] \"plasma-2072-P9-N\"             \"plasma-3077-P9-N\"            \n",
      " [43] \"plasma-4027-P9-N\"             \"plasma-4036-P9-N\"            \n",
      " [45] \"plasma-3956-P9-N\"             \"plasma-3880-P9-N\"            \n",
      " [47] \"plasma-3629-P9-LV\"            \"plasma-3050-P9-N\"            \n",
      " [49] \"plasma-3901-P9-N\"             \"plasma-3013-P9-N\"            \n",
      " [51] \"plasma-3462-P9-LV\"            \"plasma-3599-P9-LV\"           \n",
      " [53] \"plasma-3835-P9-N\"             \"plasma-3637-P9-LV\"           \n",
      " [55] \"plasma-3450-P9-BC\"            \"plasma-3377-P9-N\"            \n",
      " [57] \"plasma-632-P9-CH\"             \"plasma-636-r1-t7-4day-P9-CH\" \n",
      " [59] \"plasma-637-P9-CH\"             \"plasma-640-P9-CH\"            \n",
      " [61] \"plasma-641-P9-CH\"             \"plasma-642-P9-CH\"            \n",
      " [63] \"plasma-644-P9-CH\"             \"plasma-646-P9-CH\"            \n",
      " [65] \"plasma-649-t8-6day-P9-CH\"     \"plasma-650-r1-t7-4day-P9-CH\" \n",
      " [67] \"plasma-652-P9-CH\"             \"plasma-654-P9-CH\"            \n",
      " [69] \"plasma-655-P9-CH\"             \"plasma-657-P9-CH\"            \n",
      " [71] \"plasma-658-P9-CH\"             \"plasma-2198-t7-5day-P9-CH\"   \n",
      " [73] \"plasma-2026-P9-CH\"            \"plasma-2569-t7-5day-P9-CH\"   \n",
      " [75] \"plasma-3967-P9-CH\"            \"plasma-770-P9-LV\"            \n",
      " [77] \"plasma-1215-P9-CH\"            \"plasma-4000-P9-CH\"           \n",
      " [79] \"plasma-1279-P9-LV\"            \"plasma-1621-P9-CH\"           \n",
      " [81] \"plasma-746-P9-CH\"             \"plasma-3163-P9-CH\"           \n",
      " [83] \"plasma-2411-P9-CH\"            \"plasma-748-P9-CH\"            \n",
      " [85] \"plasma-1422-P9-CH\"            \"plasma-1622-P9-CH\"           \n",
      " [87] \"plasma-749-P9-CH\"             \"plasma-1623-P9-CH\"           \n",
      " [89] \"plasma-750-P9-CH\"             \"plasma-1993-P9-CH\"           \n",
      " [91] \"plasma-1624-P9-LV\"            \"plasma-2099-P9-CH\"           \n",
      " [93] \"plasma-1265-P9-LV\"            \"plasma-771-P9-LV\"            \n",
      " [95] \"plasma-2776-P9-CH\"            \"plasma-1327-P9-CH\"           \n",
      " [97] \"plasma-752-P9-CH\"             \"plasma-753-P9-CH\"            \n",
      " [99] \"plasma-754-P9-CH\"             \"plasma-1625-P9-CH\"           \n",
      "[101] \"plasma-1626-P9-CH\"            \"plasma-1627-P9-CH\"           \n",
      "[103] \"plasma-1423-P9-CH\"            \"plasma-1424-P9-CH\"           \n",
      "[105] \"plasma-1425-P9-CH\"            \"plasma-1628-P9-CH\"           \n",
      "[107] \"plasma-2970-P9-CH\"            \"plasma-756-r1-t4-4day-P9-CH\" \n",
      "[109] \"plasma-2566-r1-t7-4day-P9-CH\" \"plasma-1426-P9-CH\"           \n",
      "[111] \"plasma-1427-P9-CH\"            \"plasma-1348-P9-CH\"           \n",
      "[113] \"plasma-1428-P9-CH\"            \"plasma-1629-P9-CH\"           \n",
      "[115] \"plasma-1630-P9-CH\"            \"plasma-2446-P9-CH\"           \n",
      "[117] \"plasma-2102-P9-CH\"            \"plasma-1429-P9-CH\"           \n",
      "[119] \"plasma-2505-P9-CH\"            \"plasma-1631-P9-CH\"           \n",
      "[121] \"plasma-1632-P9-LV\"            \"plasma-775-P9-LV\"            \n",
      "[123] \"plasma-1633-P9-CH\"            \"plasma-743-P9-CH\"            \n",
      "[125] \"plasma-1244-P9-CH\"            \"plasma-1634-P9-CH\"           \n",
      "[127] \"plasma-1635-P9-CH\"            \"plasma-1430-P9-CH\"           \n",
      "[129] \"plasma-1273-P9-CH\"            \"plasma-2307-P9-CH\"           \n",
      "[131] \"plasma-742-P9-CH\"             \"plasma-1431-P9-CH\"           \n",
      "[133] \"plasma-1636-P9-CH\"            \"plasma-1432-P9-CH\"           \n",
      "[135] \"plasma-757-P9-CH\"             \"plasma-3869-P9-CH\"           \n",
      "[137] \"plasma-1433-P9-CH\"            \"plasma-2083-P9-CH\"           \n",
      "[139] \"plasma-2751-P9-CH\"            \"plasma-2050-P9-CH\"           \n",
      "[141] \"plasma-780-P9-LV\"             \"plasma-2047-P9-CH\"           \n",
      "[143] \"plasma-1310-P9-CH\"            \"plasma-1637-P9-CH\"           \n",
      "[145] \"plasma-3112-P9-CH\"            \"plasma-759-P9-CH\"            \n",
      "[147] \"plasma-760-P9-CH\"             \"plasma-762-P9-CH\"            \n",
      "[149] \"plasma-1434-P9-CH\"            \"plasma-763-P9-CH\"            \n",
      "[151] \"plasma-764-P9-CH\"             \"plasma-3922-P9-CH\"           \n",
      "[153] \"plasma-2932-P9-CH\"            \"plasma-765-P9-CH\"            \n",
      "[155] \"plasma-766-P9-CH\"             \"plasma-740-P9-CH\"            \n",
      "[157] \"plasma-739-P9-CH\"             \"plasma-1224-P9-CH\"           \n",
      "[159] \"plasma-767-P9-CH\"             \"plasma-1435-P9-CH\"           \n",
      "[161] \"plasma-2208-P9-CH\"            \"plasma-1638-P9-CH\"           \n",
      "[163] \"plasma-735-P9-CH\"             \"plasma-738-P9-CH\"            \n",
      "[165] \"plasma-768-P9-CH\"             \"plasma-1639-P9-CH\"           \n",
      "[167] \"plasma-1640-P9-CH\"            \"plasma-737-P9-LV\"            \n",
      "[169] \"plasma-1641-P9-CH\"            \"plasma-1246-P9-CH\"           \n",
      "[171] \"plasma-1217-P9-CH\"            \"plasma-769-P9-LV\"            \n",
      "[173] \"plasma-1137-P9-CH\"            \"plasma-1642-P9-CH\"           \n",
      "[175] \"plasma-1436-P9-CH\"            \"plasma-1643-P9-CH\"           \n",
      "[177] \"plasma-1236-P9-CH\"            \"plasma-1644-P9-LV\"           \n",
      "[179] \"plasma-1645-P9-LV\"            \"plasma-1990-P9-CH\"           \n",
      "[181] \"plasma-1127-P9-CH\"            \"plasma-1125-P9-CH\"           \n",
      "[183] \"plasma-1066-P9-CH\"            \"plasma-1646-P9-CH\"           \n",
      "[185] \"plasma-1647-P9-CH\"            \"plasma-1437-P9-CH\"           \n",
      "[187] \"plasma-1438-P9-CH\"            \"plasma-1439-P9-CH\"           \n",
      "[189] \"plasma-3132-P9-CH\"            \"plasma-1208-P9-CH\"           \n",
      "[191] \"plasma-1440-P9-CH\"            \"plasma-2312-P9-CH\"           \n",
      "[193] \"plasma-1136-P9-CH\"            \"plasma-2057-P9-CH\"           \n",
      "[195] \"plasma-1648-P9-CH\"            \"plasma-1441-P9-CH\"           \n",
      "[197] \"plasma-1649-P9-CH\"            \"plasma-2260-P9-CH\"           \n",
      "[199] \"plasma-1140-P9-CH\"            \"plasma-2450-P9-CH\"           \n",
      "[201] \"plasma-1069-P9-CH\"            \"plasma-1442-P9-CH\"           \n",
      "[203] \"plasma-1981-P9-CH\"            \"plasma-2740-P9-CH\"           \n",
      "[205] \"plasma-1206-P9-CH\"            \"plasma-1280-P9-CH\"           \n",
      "[207] \"plasma-2156-P9-CH\"            \"plasma-1138-P9-CH\"           \n",
      "[209] \"plasma-1124-P9-CH\"            \"plasma-1135-P9-CH\"           \n",
      "[211] \"plasma-1188-r1-P9-CH\"         \"plasma-1443-P9-CH\"           \n",
      "[213] \"plasma-2517-P9-CH\"            \"plasma-2486-P9-CH\"           \n",
      "[215] \"plasma-1189-r1-P9-CH\"         \"plasma-1650-P9-CH\"           \n",
      "[217] \"plasma-2791-P9-CH\"            \"plasma-2823-P9-CH\"           \n",
      "[219] \"plasma-2008-P9-CH\"            \"plasma-2802-P9-CH\"           \n",
      "[221] \"plasma-2544-P9-CH\"            \"plasma-2227-P9-CH\"           \n",
      "[223] \"plasma-1205-P9-CH\"            \"plasma-842-P9-LV\"            \n",
      "[225] \"plasma-2427-P9-CH\"            \"plasma-3321-P9-CH\"           \n",
      "[227] \"plasma-1202-P9-CH\"            \"plasma-1207-P9-CH\"           \n",
      "[229] \"plasma-1651-P9-CH\"            \"plasma-1652-P9-CH\"           \n",
      "[231] \"plasma-1126-P9-CH\"            \"plasma-2511-P9-CH\"           \n",
      "[233] \"plasma-1653-P9-CH\"            \"plasma-1445-P9-CH\"           \n",
      "[235] \"plasma-1203-P9-CH\"            \"plasma-2477-P9-CH\"           \n",
      "[237] \"plasma-1869-P9-CH\"            \"plasma-2221-P9-CH\"           \n",
      "[239] \"plasma-2483-P9-CH\"            \"plasma-2041-P9-CH\"           \n",
      "[241] \"plasma-2172-P9-CH\"            \"plasma-2241-P9-CH\"           \n",
      "[243] \"plasma-2748-P9-CH\"            \"plasma-2831-P9-CH\"           \n",
      "[245] \"plasma-1123-P9-CH\"            \"plasma-1654-P9-CH\"           \n",
      "[247] \"plasma-1655-P9-LV\"            \"plasma-1958-P9-CH\"           \n",
      "[249] \"plasma-1656-P9-CH\"            \"plasma-2907-P9-CH\"           \n",
      "[251] \"plasma-1329-P9-CH\"            \"plasma-1139-r1-P9-CH\"        \n",
      "[253] \"plasma-1336-P9-LV\"            \"plasma-1129-P9-CH\"           \n",
      "[255] \"plasma-2237-P9-CH\"            \"plasma-1192-P9-CH\"           \n",
      "[257] \"plasma-1204-P9-CH\"            \"plasma-2246-P9-CH\"           \n",
      "[259] \"plasma-2432-P9-CH\"            \"plasma-1657-P9-LV\"           \n",
      "[261] \"plasma-1248-P9-CH\"            \"plasma-1446-P9-CH\"           \n",
      "[263] \"plasma-2758-P9-CH\"            \"plasma-2286-P9-CH\"           \n",
      "[265] \"plasma-2435-P9-CH\"            \"plasma-1658-P9-CH\"           \n",
      "[267] \"plasma-1659-P9-LV\"            \"plasma-2256-P9-CH\"           \n",
      "[269] \"plasma-3059-P9-CH\"            \"plasma-2768-P9-CH\"           \n",
      "[271] \"plasma-2503-P9-CH\"            \"plasma-2920-P9-CH\"           \n",
      "[273] \"plasma-2461-P9-CH\"            \"plasma-2490-P9-CH\"           \n",
      "[275] \"plasma-2870-P9-CH\"            \"plasma-2337-P9-CH\"           \n",
      "[277] \"plasma-1660-P9-LV\"            \"plasma-1661-P9-LV\"           \n",
      "[279] \"plasma-2462-P9-CH\"            \"plasma-2541-P9-CH\"           \n",
      "[281] \"plasma-2409-P9-CH\"            \"plasma-1448-P9-CH\"           \n",
      "[283] \"plasma-2381-P9-CH\"            \"plasma-2135-P9-CH\"           \n",
      "[285] \"plasma-1965-P9-CH\"            \"plasma-2038-P9-CH\"           \n",
      "[287] \"plasma-2173-P9-CH\"            \"plasma-2017-P9-CH\"           \n",
      "[289] \"plasma-2285-P9-CH\"            \"plasma-2282-P9-CH\"           \n",
      "[291] \"plasma-1880-P9-CH\"            \"plasma-2078-P9-CH\"           \n",
      "[293] \"plasma-1662-P9-CH\"            \"plasma-1964-P9-CH\"           \n",
      "[295] \"plasma-2323-P9-CH\"            \"plasma-2202-P9-CH\"           \n",
      "[297] \"plasma-2212-P9-CH\"            \"plasma-2143-P9-CH\"           \n",
      "[299] \"plasma-2485-P9-CH\"            \"plasma-2510-P9-CH\"           \n",
      "[301] \"plasma-2533-P9-CH\"            \"plasma-2258-P9-CH\"           \n",
      "[303] \"plasma-2564-r1-t4-4day-P9-CH\" \"plasma-2459-P9-CH\"           \n",
      "[305] \"plasma-1859-P9-CH\"            \"plasma-2787-P9-CH\"           \n",
      "[307] \"plasma-2779-P9-CH\"            \"plasma-1663-P9-CH\"           \n",
      "[309] \"plasma-2782-P9-CH\"            \"plasma-2074-P9-CH\"           \n",
      "[311] \"plasma-2419-P9-CH\"            \"plasma-2501-P9-CH\"           \n",
      "[313] \"plasma-2898-P9-CH\"            \"plasma-2275-P9-CH\"           \n",
      "[315] \"plasma-1980-P9-CH\"            \"plasma-1664-P9-LV\"           \n",
      "[317] \"plasma-2025-P9-CH\"            \"plasma-2096-P9-CH\"           \n",
      "[319] \"plasma-2391-P9-CH\"            \"plasma-2559-r1-t4-4day-P9-CH\"\n",
      "[321] \"plasma-1956-P9-CH\"            \"plasma-2521-P9-CH\"           \n",
      "[323] \"plasma-2268-P9-CH\"            \"plasma-2480-P9-CH\"           \n",
      "[325] \"plasma-2331-P9-CH\"            \"plasma-2389-P9-CH\"           \n",
      "[327] \"plasma-2065-P9-CH\"            \"plasma-2440-P9-CH\"           \n",
      "[329] \"plasma-2431-P9-CH\"            \"plasma-2067-P9-CH\"           \n",
      "[331] \"plasma-2760-P9-CH\"            \"plasma-2495-P9-CH\"           \n",
      "[333] \"plasma-1302-P9-CH\"            \"plasma-2352-P9-CH\"           \n",
      "[335] \"plasma-2463-P9-CH\"            \"plasma-2321-P9-CH\"           \n",
      "[337] \"plasma-2235-P9-CH\"            \"plasma-2488-P9-CH\"           \n",
      "[339] \"plasma-2379-P9-CH\"            \"plasma-1879-P9-CH\"           \n",
      "[341] \"plasma-2082-P9-CH\"            \"plasma-2804-P9-CH\"           \n",
      "[343] \"plasma-2347-P9-CH\"            \"plasma-2030-P9-CH\"           \n",
      "[345] \"plasma-2565-r1-t3-4day-P9-CH\" \"plasma-1222-P9-LV\"           \n",
      "[347] \"plasma-1665-P9-LV\"            \"plasma-2405-P9-CH\"           \n",
      "[349] \"plasma-2176-P9-CH\"            \"plasma-2182-P9-CH\"           \n",
      "[351] \"plasma-1982-P9-CH\"            \"plasma-2449-P9-CH\"           \n",
      "[353] \"plasma-2294-P9-CH\"            \"plasma-2493-P9-CH\"           \n",
      "[355] \"plasma-2451-P9-CH\"            \"plasma-2437-P9-CH\"           \n",
      "[357] \"plasma-2259-P9-CH\"            \"plasma-2848-P9-CH\"           \n",
      "[359] \"plasma-2215-P9-CH\"            \"plasma-2796-P9-CH\"           \n",
      "[361] \"plasma-2355-P9-CH\"            \"plasma-2160-P9-CH\"           \n",
      "[363] \"plasma-2332-P9-CH\"            \"plasma-2407-P9-CH\"           \n",
      "[365] \"plasma-2277-P9-CH\"            \"plasma-2018-P9-CH\"           \n",
      "[367] \"plasma-2458-P9-CH\"            \"plasma-2785-P9-CH\"           \n",
      "[369] \"plasma-2274-P9-CH\"            \"plasma-2239-P9-CH\"           \n",
      "[371] \"plasma-2489-P9-CH\"            \"plasma-2367-P9-CH\"           \n",
      "[373] \"plasma-2363-P9-CH\"            \"plasma-2106-P9-CH\"           \n",
      "[375] \"plasma-2572-t2-6day-P9-CH\"    \"plasma-2563-r1-t2-4day-P9-CH\"\n",
      "[377] \"plasma-2039-P9-CH\"            \"plasma-2250-P9-CH\"           \n",
      "[379] \"plasma-2428-P9-CH\"            \"plasma-2297-P9-CH\"           \n",
      "[381] \"plasma-3409-P9-CH\"            \"plasma-2371-P9-CH\"           \n",
      "[383] \"plasma-3331-P9-CH\"            \"plasma-2010-P9-CH\"           \n",
      "[385] \"plasma-2867-P9-CH\"            \"plasma-2502-P9-CH\"           \n",
      "[387] \"plasma-2094-P9-CH\"            \"plasma-2738-P9-CH\"           \n",
      "[389] \"plasma-2573-t2-5day-P9-CH\"    \"plasma-2536-P9-CH\"           \n",
      "[391] \"plasma-2304-P9-CH\"            \"plasma-2368-P9-CH\"           \n",
      "[393] \"plasma-2567-r1-t1-4day-P9-CH\" \"plasma-2560-r1-t1-4day-P9-CH\"\n",
      "[395] \"plasma-3087-P9-CH\"            \"plasma-3896-P9-CH\"           \n",
      "[397] \"plasma-3944-P9-CH\"            \"plasma-3366-P9-CH\"           \n",
      "[399] \"plasma-2574-5day-P9-CH\"       \"plasma-3964-P9-CH\"           \n",
      "[401] \"plasma-2575-5day-P9-CH\"       \"plasma-3448-P9-CH\"           \n",
      "[403] \"plasma-2576-5day-P9-CH\"       \"plasma-2577-5day-P9-CH\"      \n",
      "[405] \"plasma-3915-P9-CH\"            \"plasma-2568-r1-4day-P9-CH\"   \n",
      "[407] \"plasma-3917-P9-N\"            \n"
     ]
    }
   ],
   "source": [
    "print(dim(combined_data_clean))\n",
    "combined_data_clean_aliases_nodup <- combined_data_clean$plasma_alias\n",
    "print(combined_data_clean_aliases_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea746f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 407 × 145035</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>chr10_100027865_100027984</th><th scope=col>chr10_100028045_100028164</th><th scope=col>chr10_100028161_100028280</th><th scope=col>chr10_100028371_100028490</th><th scope=col>chr10_100069285_100069404</th><th scope=col>chr10_100108994_100109113</th><th scope=col>chr10_100174700_100174819</th><th scope=col>chr10_100174780_100174899</th><th scope=col>chr10_100206002_100206121</th><th scope=col>chr10_100206118_100206237</th><th scope=col>⋯</th><th scope=col>hbv_load_IU_L</th><th scope=col>hbv_antigen</th><th scope=col>hbv_hepatitis_delta_antibody</th><th scope=col>hbv_treatment</th><th scope=col>meld_score</th><th scope=col>meld_na_score</th><th scope=col>child_pugh_score</th><th scope=col>child_pugh_class</th><th scope=col>site_id_cc</th><th scope=col>rowname</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>1.379512</td><td>4.468386</td><td>2.764103</td><td>2.556595</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>11</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2333-P9-LV</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>1.613601</td><td>3.718647</td><td>2.201849</td><td>2.201849</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2300-P9-LV</td></tr>\n",
       "\t<tr><td>1.515843</td><td>0</td><td>0</td><td>0</td><td>1.515843</td><td>4.497935</td><td>3.994503</td><td>3.495880</td><td>2.092668</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2270-P9-LV</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.102887</td><td>2.655036</td><td>2.390516</td><td>3.036790</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>19</td><td>NA</td><td>6</td><td>A</td><td>UCLA</td><td>plasma-1969-P9-LV</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>3.661828</td><td>3.661828</td><td>3.484630</td><td>1.990893</td><td>1.990893</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>40</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-1976-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>1.253560</td><td>4.152138</td><td>4.565452</td><td>4.331834</td><td>2.141051</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td> 9</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2742-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.431948</td><td>2.989978</td><td>3.163883</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2800-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.532344</td><td>5.506428</td><td>5.373476</td><td>3.455014</td><td>3.065219</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>6</td><td>A</td><td>UCLA</td><td>plasma-3931-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>3.025081</td><td>3.844242</td><td>3.975094</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2518-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>3.897022</td><td>2.569852</td><td>2.569852</td><td>2.569852</td><td>1.950463</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td> 8</td><td> 8</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2357-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.157183</td><td>2.898790</td><td>3.564006</td><td>1.904700</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2873-P9-N </td></tr>\n",
       "\t<tr><td>1.740319</td><td>0</td><td>0</td><td>0</td><td>2.985512</td><td>4.670313</td><td>2.714528</td><td>1.208835</td><td>2.859171</td><td>2.341644</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td> 9</td><td>NA</td><td>4</td><td>4</td><td>UCLA</td><td>plasma-1959-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.279820</td><td>1.868571</td><td>1.868571</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>Nonreactive</td><td>Positive</td><td>No</td><td>10</td><td>10</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-3038-P9-LV</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.204385</td><td>3.073552</td><td>2.593098</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2091-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.306454</td><td>2.579926</td><td>2.579926</td><td>1.959825</td><td>1.959825</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td> 8</td><td> 8</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-3390-P9-LV</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.819086</td><td>3.792327</td><td>2.737814</td><td>3.398070</td><td>2.107371</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2741-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.534692</td><td>3.180081</td><td>1.724504</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td> 8</td><td> 8</td><td>4</td><td>4</td><td>UCLA</td><td>plasma-2104-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.296123</td><td>4.126823</td><td>4.242812</td><td>3.320676</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2188-P9-BD</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.625183</td><td>3.302877</td><td>3.154837</td><td>3.431800</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-3627-P9-LV</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>3.551991</td><td>3.729523</td><td>3.729523</td><td>3.880238</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-3134-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.419081</td><td>1.270637</td><td>1.812582</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>22</td><td>17</td><td>7</td><td>B</td><td>UCLA</td><td>plasma-2733-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>1.329632</td><td>3.985523</td><td>2.701245</td><td>1.880885</td><td>2.234195</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>5</td><td>A</td><td>UCLA</td><td>plasma-2743-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.607908</td><td>3.584572</td><td>3.407785</td><td>2.539971</td><td>1.922737</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2778-P9-PR</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>2.050633</td><td>4.489160</td><td>4.410053</td><td>4.324146</td><td>3.059573</td><td>2.050633</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>14</td><td>14</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2370-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>1.721745</td><td>4.427203</td><td>1.721745</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>11</td><td>11</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2248-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.737773</td><td>4.443548</td><td>4.364485</td><td>2.634390</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-3427-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.237773</td><td>2.658265</td><td>2.149867</td><td>1.063306</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td> 7</td><td>13</td><td>5</td><td>A</td><td>UCLA</td><td>plasma-2549-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.289419</td><td>1.451445</td><td>1.451445</td><td>1.451445</td><td>2.380271</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>14</td><td>12</td><td>6</td><td>A</td><td>UCLA</td><td>plasma-2090-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>0.000000</td><td>4.439955</td><td>2.941854</td><td>2.562431</td><td>1.943572</td><td>0.000000</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>3</td><td>3</td><td>UCLA</td><td>plasma-2441-P9-N </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0</td><td>0</td><td>0</td><td>2.076831</td><td>4.155946</td><td>5.458740</td><td>5.443767</td><td>4.259738</td><td>2.439623</td><td>⋯</td><td>NA</td><td>NA         </td><td>NA      </td><td>NA</td><td>NA</td><td>NA</td><td>5</td><td>A</td><td>UCLA</td><td>plasma-2349-P9-N </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋱</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>3.905578</td><td>3.057900</td><td>2.577918</td><td>1.620687</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>19</td><td>21</td><td> 8</td><td>B</td><td>UCLA</td><td>plasma-2250-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.318940</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>22</td><td>26</td><td> 6</td><td>A</td><td>UCLA</td><td>plasma-2428-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>2.688058</td><td>3.958650</td><td>2.422793</td><td>1.487445</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>199568.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2297-P9-CH           </td></tr>\n",
       "\t<tr><td>2.363638</td><td>1.437427</td><td>0</td><td>1.437427</td><td>1.437427</td><td>4.397485</td><td>2.363638</td><td>2.004139</td><td>2.363638</td><td>1.437427</td><td>⋯</td><td>152953.00</td><td>Reactive</td><td>Unknown </td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-3409-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>3.997759</td><td>3.499097</td><td>3.499097</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>    19.99</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2371-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>2.377990</td><td>4.372805</td><td>1.773528</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>  1302.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-3331-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.590949</td><td>0.000000</td><td>0.000000</td><td>2.580103</td><td>0.000000</td><td>⋯</td><td>    19.99</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2010-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.777729</td><td>0.000000</td><td>2.124013</td><td>3.632922</td><td>0.000000</td><td>⋯</td><td>124864.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2867-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>2.313131</td><td>3.737124</td><td>2.955537</td><td>2.955537</td><td>1.714340</td><td>0.000000</td><td>⋯</td><td>  7443.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2502-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>2.437047</td><td>4.037347</td><td>3.361690</td><td>3.085501</td><td>2.074363</td><td>2.074363</td><td>⋯</td><td>404758.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 5</td><td>A</td><td>UCLA</td><td>plasma-2094-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>2.352478</td><td>4.659918</td><td>3.385567</td><td>2.996890</td><td>2.352478</td><td>0.000000</td><td>⋯</td><td>213939.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2738-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.396099</td><td>1.535241</td><td>1.535241</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>   253.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2573-t2-5day-P9-CH   </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>1.937298</td><td>4.681385</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>   589.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2536-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>2.818081</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td> 8</td><td> 8</td><td> 5</td><td>A</td><td>UCLA</td><td>plasma-2304-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.324752</td><td>2.766907</td><td>2.392386</td><td>2.392386</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>Negative</td><td>Yes</td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2368-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.087463</td><td>1.332347</td><td>1.884013</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>22</td><td>22</td><td> 7</td><td>B</td><td>UCLA</td><td>plasma-2567-r1-t1-4day-P9-CH</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>2.049996</td><td>4.409331</td><td>2.049996</td><td>2.676610</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td> 7</td><td> 9</td><td> 5</td><td>A</td><td>UCLA</td><td>plasma-2560-r1-t1-4day-P9-CH</td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>1.384675</td><td>4.556989</td><td>4.308668</td><td>4.386124</td><td>3.431897</td><td>2.138161</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>15</td><td>15</td><td> 7</td><td>B</td><td>UCLA</td><td>plasma-3087-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>1.206613</td><td>3.867923</td><td>2.542446</td><td>2.856184</td><td>2.082741</td><td>1.206613</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>19</td><td>19</td><td> 9</td><td>B</td><td>UCLA</td><td>plasma-3896-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>1.716082</td><td>3.792012</td><td>2.059753</td><td>1.716082</td><td>2.518238</td><td>1.188253</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>11</td><td>15</td><td> 9</td><td>B</td><td>UCLA</td><td>plasma-3944-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>2.216250</td><td>4.274324</td><td>1.863860</td><td>1.314876</td><td>1.314876</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>14</td><td>17</td><td> 6</td><td>A</td><td>UCLA</td><td>plasma-3366-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.166403</td><td>0.000000</td><td>2.086129</td><td>2.715182</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>15</td><td>15</td><td> 8</td><td>B</td><td>UCLA</td><td>plasma-2574-5day-P9-CH      </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>3.999747</td><td>0.000000</td><td>0.000000</td><td>3.501061</td><td>0.000000</td><td>⋯</td><td>    19.99</td><td>Reactive</td><td>Negative</td><td>Yes</td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-3964-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>2.011834</td><td>0.000000</td><td>3.966817</td><td>3.017056</td><td>2.011834</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>    19.99</td><td>Reactive</td><td>Positive</td><td>Yes</td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2575-5day-P9-CH      </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.488949</td><td>2.440563</td><td>2.608259</td><td>1.647080</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>17</td><td>17</td><td> 8</td><td>B</td><td>UCLA</td><td>plasma-3448-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.717095</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>Reactive</td><td>Negative</td><td>Yes</td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2576-5day-P9-CH      </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.662506</td><td>0.000000</td><td>0.000000</td><td>2.139235</td><td>0.000000</td><td>⋯</td><td>  6932.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2577-5day-P9-CH      </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.341140</td><td>3.493889</td><td>3.608291</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>17</td><td>19</td><td>11</td><td>C</td><td>UCLA</td><td>plasma-3915-P9-CH           </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>3.542540</td><td>1.398820</td><td>0.000000</td><td>0.000000</td><td>0.000000</td><td>⋯</td><td>  1680.00</td><td>Reactive</td><td>Negative</td><td>No </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-2568-r1-4day-P9-CH   </td></tr>\n",
       "\t<tr><td>0.000000</td><td>0.000000</td><td>0</td><td>0.000000</td><td>0.000000</td><td>4.415264</td><td>3.734135</td><td>3.583959</td><td>2.918199</td><td>0.000000</td><td>⋯</td><td>       NA</td><td>NA      </td><td>NA      </td><td>NA </td><td>NA</td><td>NA</td><td> 3</td><td>3</td><td>UCLA</td><td>plasma-3917-P9-N            </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 407 × 145035\n",
       "\\begin{tabular}{lllllllllllllllllllll}\n",
       " chr10\\_100027865\\_100027984 & chr10\\_100028045\\_100028164 & chr10\\_100028161\\_100028280 & chr10\\_100028371\\_100028490 & chr10\\_100069285\\_100069404 & chr10\\_100108994\\_100109113 & chr10\\_100174700\\_100174819 & chr10\\_100174780\\_100174899 & chr10\\_100206002\\_100206121 & chr10\\_100206118\\_100206237 & ⋯ & hbv\\_load\\_IU\\_L & hbv\\_antigen & hbv\\_hepatitis\\_delta\\_antibody & hbv\\_treatment & meld\\_score & meld\\_na\\_score & child\\_pugh\\_score & child\\_pugh\\_class & site\\_id\\_cc & rowname\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <chr> & <chr> & <chr> & <int> & <int> & <int> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t 0.000000 & 0 & 0 & 0 & 1.379512 & 4.468386 & 2.764103 & 2.556595 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & 11 & NA & 3 & 3 & UCLA & plasma-2333-P9-LV\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 1.613601 & 3.718647 & 2.201849 & 2.201849 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2300-P9-LV\\\\\n",
       "\t 1.515843 & 0 & 0 & 0 & 1.515843 & 4.497935 & 3.994503 & 3.495880 & 2.092668 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2270-P9-LV\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.102887 & 2.655036 & 2.390516 & 3.036790 & 0.000000 & ⋯ & NA & NA          & NA       & NA & 19 & NA & 6 & A & UCLA & plasma-1969-P9-LV\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 3.661828 & 3.661828 & 3.484630 & 1.990893 & 1.990893 & ⋯ & NA & NA          & NA       & NA & 40 & NA & 3 & 3 & UCLA & plasma-1976-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 1.253560 & 4.152138 & 4.565452 & 4.331834 & 2.141051 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA &  9 & 3 & 3 & UCLA & plasma-2742-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.431948 & 2.989978 & 3.163883 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2800-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.532344 & 5.506428 & 5.373476 & 3.455014 & 3.065219 & ⋯ & NA & NA          & NA       & NA & NA & NA & 6 & A & UCLA & plasma-3931-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 3.025081 & 3.844242 & 3.975094 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2518-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 3.897022 & 2.569852 & 2.569852 & 2.569852 & 1.950463 & ⋯ & NA & NA          & NA       & NA &  8 &  8 & 3 & 3 & UCLA & plasma-2357-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.157183 & 2.898790 & 3.564006 & 1.904700 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2873-P9-N \\\\\n",
       "\t 1.740319 & 0 & 0 & 0 & 2.985512 & 4.670313 & 2.714528 & 1.208835 & 2.859171 & 2.341644 & ⋯ & NA & NA          & NA       & NA &  9 & NA & 4 & 4 & UCLA & plasma-1959-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.279820 & 1.868571 & 1.868571 & 0.000000 & 0.000000 & ⋯ & NA & Nonreactive & Positive & No & 10 & 10 & 3 & 3 & UCLA & plasma-3038-P9-LV\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.204385 & 3.073552 & 2.593098 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2091-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.306454 & 2.579926 & 2.579926 & 1.959825 & 1.959825 & ⋯ & NA & NA          & NA       & NA &  8 &  8 & 3 & 3 & UCLA & plasma-3390-P9-LV\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.819086 & 3.792327 & 2.737814 & 3.398070 & 2.107371 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2741-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.534692 & 3.180081 & 1.724504 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA &  8 &  8 & 4 & 4 & UCLA & plasma-2104-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.296123 & 4.126823 & 4.242812 & 3.320676 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2188-P9-BD\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.625183 & 3.302877 & 3.154837 & 3.431800 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-3627-P9-LV\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 3.551991 & 3.729523 & 3.729523 & 3.880238 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-3134-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.419081 & 1.270637 & 1.812582 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & 22 & 17 & 7 & B & UCLA & plasma-2733-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 1.329632 & 3.985523 & 2.701245 & 1.880885 & 2.234195 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 5 & A & UCLA & plasma-2743-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.607908 & 3.584572 & 3.407785 & 2.539971 & 1.922737 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2778-P9-PR\\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 2.050633 & 4.489160 & 4.410053 & 4.324146 & 3.059573 & 2.050633 & ⋯ & NA & NA          & NA       & NA & 14 & 14 & 3 & 3 & UCLA & plasma-2370-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 1.721745 & 4.427203 & 1.721745 & 0.000000 & 0.000000 & 0.000000 & ⋯ & NA & NA          & NA       & NA & 11 & 11 & 3 & 3 & UCLA & plasma-2248-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.737773 & 4.443548 & 4.364485 & 2.634390 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-3427-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.237773 & 2.658265 & 2.149867 & 1.063306 & 0.000000 & ⋯ & NA & NA          & NA       & NA &  7 & 13 & 5 & A & UCLA & plasma-2549-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.289419 & 1.451445 & 1.451445 & 1.451445 & 2.380271 & ⋯ & NA & NA          & NA       & NA & 14 & 12 & 6 & A & UCLA & plasma-2090-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 0.000000 & 4.439955 & 2.941854 & 2.562431 & 1.943572 & 0.000000 & ⋯ & NA & NA          & NA       & NA & NA & NA & 3 & 3 & UCLA & plasma-2441-P9-N \\\\\n",
       "\t 0.000000 & 0 & 0 & 0 & 2.076831 & 4.155946 & 5.458740 & 5.443767 & 4.259738 & 2.439623 & ⋯ & NA & NA          & NA       & NA & NA & NA & 5 & A & UCLA & plasma-2349-P9-N \\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋱ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 3.905578 & 3.057900 & 2.577918 & 1.620687 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & 19 & 21 &  8 & B & UCLA & plasma-2250-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.318940 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & 22 & 26 &  6 & A & UCLA & plasma-2428-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 2.688058 & 3.958650 & 2.422793 & 1.487445 & 0.000000 & 0.000000 & ⋯ & 199568.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2297-P9-CH           \\\\\n",
       "\t 2.363638 & 1.437427 & 0 & 1.437427 & 1.437427 & 4.397485 & 2.363638 & 2.004139 & 2.363638 & 1.437427 & ⋯ & 152953.00 & Reactive & Unknown  & No  & NA & NA &  3 & 3 & UCLA & plasma-3409-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 3.997759 & 3.499097 & 3.499097 & 0.000000 & 0.000000 & ⋯ &     19.99 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2371-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 2.377990 & 4.372805 & 1.773528 & 0.000000 & 0.000000 & 0.000000 & ⋯ &   1302.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-3331-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.590949 & 0.000000 & 0.000000 & 2.580103 & 0.000000 & ⋯ &     19.99 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2010-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.777729 & 0.000000 & 2.124013 & 3.632922 & 0.000000 & ⋯ & 124864.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2867-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 2.313131 & 3.737124 & 2.955537 & 2.955537 & 1.714340 & 0.000000 & ⋯ &   7443.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2502-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 2.437047 & 4.037347 & 3.361690 & 3.085501 & 2.074363 & 2.074363 & ⋯ & 404758.00 & Reactive & Negative & No  & NA & NA &  5 & A & UCLA & plasma-2094-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 2.352478 & 4.659918 & 3.385567 & 2.996890 & 2.352478 & 0.000000 & ⋯ & 213939.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2738-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.396099 & 1.535241 & 1.535241 & 0.000000 & 0.000000 & ⋯ &    253.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2573-t2-5day-P9-CH   \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 1.937298 & 4.681385 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & ⋯ &    589.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2536-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 2.818081 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  &  8 &  8 &  5 & A & UCLA & plasma-2304-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.324752 & 2.766907 & 2.392386 & 2.392386 & 0.000000 & ⋯ &        NA & NA       & Negative & Yes & NA & NA &  3 & 3 & UCLA & plasma-2368-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.087463 & 1.332347 & 1.884013 & 0.000000 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & 22 & 22 &  7 & B & UCLA & plasma-2567-r1-t1-4day-P9-CH\\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 2.049996 & 4.409331 & 2.049996 & 2.676610 & 0.000000 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  &  7 &  9 &  5 & A & UCLA & plasma-2560-r1-t1-4day-P9-CH\\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 1.384675 & 4.556989 & 4.308668 & 4.386124 & 3.431897 & 2.138161 & ⋯ &        NA & NA       & NA       & NA  & 15 & 15 &  7 & B & UCLA & plasma-3087-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 1.206613 & 3.867923 & 2.542446 & 2.856184 & 2.082741 & 1.206613 & ⋯ &        NA & NA       & NA       & NA  & 19 & 19 &  9 & B & UCLA & plasma-3896-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 1.716082 & 3.792012 & 2.059753 & 1.716082 & 2.518238 & 1.188253 & ⋯ &        NA & NA       & NA       & NA  & 11 & 15 &  9 & B & UCLA & plasma-3944-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 2.216250 & 4.274324 & 1.863860 & 1.314876 & 1.314876 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & 14 & 17 &  6 & A & UCLA & plasma-3366-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.166403 & 0.000000 & 2.086129 & 2.715182 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & 15 & 15 &  8 & B & UCLA & plasma-2574-5day-P9-CH      \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 3.999747 & 0.000000 & 0.000000 & 3.501061 & 0.000000 & ⋯ &     19.99 & Reactive & Negative & Yes & NA & NA &  3 & 3 & UCLA & plasma-3964-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 2.011834 & 0.000000 & 3.966817 & 3.017056 & 2.011834 & 0.000000 & 0.000000 & ⋯ &     19.99 & Reactive & Positive & Yes & NA & NA &  3 & 3 & UCLA & plasma-2575-5day-P9-CH      \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.488949 & 2.440563 & 2.608259 & 1.647080 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & 17 & 17 &  8 & B & UCLA & plasma-3448-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.717095 & 0.000000 & 0.000000 & 0.000000 & 0.000000 & ⋯ &        NA & Reactive & Negative & Yes & NA & NA &  3 & 3 & UCLA & plasma-2576-5day-P9-CH      \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.662506 & 0.000000 & 0.000000 & 2.139235 & 0.000000 & ⋯ &   6932.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2577-5day-P9-CH      \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.341140 & 3.493889 & 3.608291 & 0.000000 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & 17 & 19 & 11 & C & UCLA & plasma-3915-P9-CH           \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 3.542540 & 1.398820 & 0.000000 & 0.000000 & 0.000000 & ⋯ &   1680.00 & Reactive & Negative & No  & NA & NA &  3 & 3 & UCLA & plasma-2568-r1-4day-P9-CH   \\\\\n",
       "\t 0.000000 & 0.000000 & 0 & 0.000000 & 0.000000 & 4.415264 & 3.734135 & 3.583959 & 2.918199 & 0.000000 & ⋯ &        NA & NA       & NA       & NA  & NA & NA &  3 & 3 & UCLA & plasma-3917-P9-N            \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 407 × 145035\n",
       "\n",
       "| chr10_100027865_100027984 &lt;dbl&gt; | chr10_100028045_100028164 &lt;dbl&gt; | chr10_100028161_100028280 &lt;dbl&gt; | chr10_100028371_100028490 &lt;dbl&gt; | chr10_100069285_100069404 &lt;dbl&gt; | chr10_100108994_100109113 &lt;dbl&gt; | chr10_100174700_100174819 &lt;dbl&gt; | chr10_100174780_100174899 &lt;dbl&gt; | chr10_100206002_100206121 &lt;dbl&gt; | chr10_100206118_100206237 &lt;dbl&gt; | ⋯ ⋯ | hbv_load_IU_L &lt;dbl&gt; | hbv_antigen &lt;chr&gt; | hbv_hepatitis_delta_antibody &lt;chr&gt; | hbv_treatment &lt;chr&gt; | meld_score &lt;int&gt; | meld_na_score &lt;int&gt; | child_pugh_score &lt;int&gt; | child_pugh_class &lt;chr&gt; | site_id_cc &lt;chr&gt; | rowname &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.000000 | 0 | 0 | 0 | 1.379512 | 4.468386 | 2.764103 | 2.556595 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | 11 | NA | 3 | 3 | UCLA | plasma-2333-P9-LV |\n",
       "| 0.000000 | 0 | 0 | 0 | 1.613601 | 3.718647 | 2.201849 | 2.201849 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2300-P9-LV |\n",
       "| 1.515843 | 0 | 0 | 0 | 1.515843 | 4.497935 | 3.994503 | 3.495880 | 2.092668 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2270-P9-LV |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.102887 | 2.655036 | 2.390516 | 3.036790 | 0.000000 | ⋯ | NA | NA          | NA       | NA | 19 | NA | 6 | A | UCLA | plasma-1969-P9-LV |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 3.661828 | 3.661828 | 3.484630 | 1.990893 | 1.990893 | ⋯ | NA | NA          | NA       | NA | 40 | NA | 3 | 3 | UCLA | plasma-1976-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 1.253560 | 4.152138 | 4.565452 | 4.331834 | 2.141051 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA |  9 | 3 | 3 | UCLA | plasma-2742-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.431948 | 2.989978 | 3.163883 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2800-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.532344 | 5.506428 | 5.373476 | 3.455014 | 3.065219 | ⋯ | NA | NA          | NA       | NA | NA | NA | 6 | A | UCLA | plasma-3931-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 3.025081 | 3.844242 | 3.975094 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2518-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 3.897022 | 2.569852 | 2.569852 | 2.569852 | 1.950463 | ⋯ | NA | NA          | NA       | NA |  8 |  8 | 3 | 3 | UCLA | plasma-2357-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.157183 | 2.898790 | 3.564006 | 1.904700 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2873-P9-N  |\n",
       "| 1.740319 | 0 | 0 | 0 | 2.985512 | 4.670313 | 2.714528 | 1.208835 | 2.859171 | 2.341644 | ⋯ | NA | NA          | NA       | NA |  9 | NA | 4 | 4 | UCLA | plasma-1959-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.279820 | 1.868571 | 1.868571 | 0.000000 | 0.000000 | ⋯ | NA | Nonreactive | Positive | No | 10 | 10 | 3 | 3 | UCLA | plasma-3038-P9-LV |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.204385 | 3.073552 | 2.593098 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2091-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.306454 | 2.579926 | 2.579926 | 1.959825 | 1.959825 | ⋯ | NA | NA          | NA       | NA |  8 |  8 | 3 | 3 | UCLA | plasma-3390-P9-LV |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.819086 | 3.792327 | 2.737814 | 3.398070 | 2.107371 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2741-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.534692 | 3.180081 | 1.724504 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA |  8 |  8 | 4 | 4 | UCLA | plasma-2104-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.296123 | 4.126823 | 4.242812 | 3.320676 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2188-P9-BD |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.625183 | 3.302877 | 3.154837 | 3.431800 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-3627-P9-LV |\n",
       "| 0.000000 | 0 | 0 | 0 | 3.551991 | 3.729523 | 3.729523 | 3.880238 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-3134-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.419081 | 1.270637 | 1.812582 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | 22 | 17 | 7 | B | UCLA | plasma-2733-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 1.329632 | 3.985523 | 2.701245 | 1.880885 | 2.234195 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 5 | A | UCLA | plasma-2743-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.607908 | 3.584572 | 3.407785 | 2.539971 | 1.922737 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2778-P9-PR |\n",
       "| 0.000000 | 0 | 0 | 0 | 2.050633 | 4.489160 | 4.410053 | 4.324146 | 3.059573 | 2.050633 | ⋯ | NA | NA          | NA       | NA | 14 | 14 | 3 | 3 | UCLA | plasma-2370-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 1.721745 | 4.427203 | 1.721745 | 0.000000 | 0.000000 | 0.000000 | ⋯ | NA | NA          | NA       | NA | 11 | 11 | 3 | 3 | UCLA | plasma-2248-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.737773 | 4.443548 | 4.364485 | 2.634390 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-3427-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.237773 | 2.658265 | 2.149867 | 1.063306 | 0.000000 | ⋯ | NA | NA          | NA       | NA |  7 | 13 | 5 | A | UCLA | plasma-2549-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.289419 | 1.451445 | 1.451445 | 1.451445 | 2.380271 | ⋯ | NA | NA          | NA       | NA | 14 | 12 | 6 | A | UCLA | plasma-2090-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 0.000000 | 4.439955 | 2.941854 | 2.562431 | 1.943572 | 0.000000 | ⋯ | NA | NA          | NA       | NA | NA | NA | 3 | 3 | UCLA | plasma-2441-P9-N  |\n",
       "| 0.000000 | 0 | 0 | 0 | 2.076831 | 4.155946 | 5.458740 | 5.443767 | 4.259738 | 2.439623 | ⋯ | NA | NA          | NA       | NA | NA | NA | 5 | A | UCLA | plasma-2349-P9-N  |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋱ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 3.905578 | 3.057900 | 2.577918 | 1.620687 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | 19 | 21 |  8 | B | UCLA | plasma-2250-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.318940 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | 22 | 26 |  6 | A | UCLA | plasma-2428-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 2.688058 | 3.958650 | 2.422793 | 1.487445 | 0.000000 | 0.000000 | ⋯ | 199568.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2297-P9-CH            |\n",
       "| 2.363638 | 1.437427 | 0 | 1.437427 | 1.437427 | 4.397485 | 2.363638 | 2.004139 | 2.363638 | 1.437427 | ⋯ | 152953.00 | Reactive | Unknown  | No  | NA | NA |  3 | 3 | UCLA | plasma-3409-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 3.997759 | 3.499097 | 3.499097 | 0.000000 | 0.000000 | ⋯ |     19.99 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2371-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 2.377990 | 4.372805 | 1.773528 | 0.000000 | 0.000000 | 0.000000 | ⋯ |   1302.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-3331-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.590949 | 0.000000 | 0.000000 | 2.580103 | 0.000000 | ⋯ |     19.99 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2010-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.777729 | 0.000000 | 2.124013 | 3.632922 | 0.000000 | ⋯ | 124864.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2867-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 2.313131 | 3.737124 | 2.955537 | 2.955537 | 1.714340 | 0.000000 | ⋯ |   7443.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2502-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 2.437047 | 4.037347 | 3.361690 | 3.085501 | 2.074363 | 2.074363 | ⋯ | 404758.00 | Reactive | Negative | No  | NA | NA |  5 | A | UCLA | plasma-2094-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 2.352478 | 4.659918 | 3.385567 | 2.996890 | 2.352478 | 0.000000 | ⋯ | 213939.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2738-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.396099 | 1.535241 | 1.535241 | 0.000000 | 0.000000 | ⋯ |    253.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2573-t2-5day-P9-CH    |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 1.937298 | 4.681385 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ⋯ |    589.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2536-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 2.818081 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  |  8 |  8 |  5 | A | UCLA | plasma-2304-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.324752 | 2.766907 | 2.392386 | 2.392386 | 0.000000 | ⋯ |        NA | NA       | Negative | Yes | NA | NA |  3 | 3 | UCLA | plasma-2368-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.087463 | 1.332347 | 1.884013 | 0.000000 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | 22 | 22 |  7 | B | UCLA | plasma-2567-r1-t1-4day-P9-CH |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 2.049996 | 4.409331 | 2.049996 | 2.676610 | 0.000000 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  |  7 |  9 |  5 | A | UCLA | plasma-2560-r1-t1-4day-P9-CH |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 1.384675 | 4.556989 | 4.308668 | 4.386124 | 3.431897 | 2.138161 | ⋯ |        NA | NA       | NA       | NA  | 15 | 15 |  7 | B | UCLA | plasma-3087-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 1.206613 | 3.867923 | 2.542446 | 2.856184 | 2.082741 | 1.206613 | ⋯ |        NA | NA       | NA       | NA  | 19 | 19 |  9 | B | UCLA | plasma-3896-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 1.716082 | 3.792012 | 2.059753 | 1.716082 | 2.518238 | 1.188253 | ⋯ |        NA | NA       | NA       | NA  | 11 | 15 |  9 | B | UCLA | plasma-3944-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 2.216250 | 4.274324 | 1.863860 | 1.314876 | 1.314876 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | 14 | 17 |  6 | A | UCLA | plasma-3366-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.166403 | 0.000000 | 2.086129 | 2.715182 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | 15 | 15 |  8 | B | UCLA | plasma-2574-5day-P9-CH       |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 3.999747 | 0.000000 | 0.000000 | 3.501061 | 0.000000 | ⋯ |     19.99 | Reactive | Negative | Yes | NA | NA |  3 | 3 | UCLA | plasma-3964-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 2.011834 | 0.000000 | 3.966817 | 3.017056 | 2.011834 | 0.000000 | 0.000000 | ⋯ |     19.99 | Reactive | Positive | Yes | NA | NA |  3 | 3 | UCLA | plasma-2575-5day-P9-CH       |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.488949 | 2.440563 | 2.608259 | 1.647080 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | 17 | 17 |  8 | B | UCLA | plasma-3448-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.717095 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ⋯ |        NA | Reactive | Negative | Yes | NA | NA |  3 | 3 | UCLA | plasma-2576-5day-P9-CH       |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.662506 | 0.000000 | 0.000000 | 2.139235 | 0.000000 | ⋯ |   6932.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2577-5day-P9-CH       |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.341140 | 3.493889 | 3.608291 | 0.000000 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | 17 | 19 | 11 | C | UCLA | plasma-3915-P9-CH            |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 3.542540 | 1.398820 | 0.000000 | 0.000000 | 0.000000 | ⋯ |   1680.00 | Reactive | Negative | No  | NA | NA |  3 | 3 | UCLA | plasma-2568-r1-4day-P9-CH    |\n",
       "| 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 4.415264 | 3.734135 | 3.583959 | 2.918199 | 0.000000 | ⋯ |        NA | NA       | NA       | NA  | NA | NA |  3 | 3 | UCLA | plasma-3917-P9-N             |\n",
       "\n"
      ],
      "text/plain": [
       "    chr10_100027865_100027984 chr10_100028045_100028164\n",
       "1   0.000000                  0                        \n",
       "2   0.000000                  0                        \n",
       "3   1.515843                  0                        \n",
       "4   0.000000                  0                        \n",
       "5   0.000000                  0                        \n",
       "6   0.000000                  0                        \n",
       "7   0.000000                  0                        \n",
       "8   0.000000                  0                        \n",
       "9   0.000000                  0                        \n",
       "10  0.000000                  0                        \n",
       "11  0.000000                  0                        \n",
       "12  1.740319                  0                        \n",
       "13  0.000000                  0                        \n",
       "14  0.000000                  0                        \n",
       "15  0.000000                  0                        \n",
       "16  0.000000                  0                        \n",
       "17  0.000000                  0                        \n",
       "18  0.000000                  0                        \n",
       "19  0.000000                  0                        \n",
       "20  0.000000                  0                        \n",
       "21  0.000000                  0                        \n",
       "22  0.000000                  0                        \n",
       "23  0.000000                  0                        \n",
       "24  0.000000                  0                        \n",
       "25  0.000000                  0                        \n",
       "26  0.000000                  0                        \n",
       "27  0.000000                  0                        \n",
       "28  0.000000                  0                        \n",
       "29  0.000000                  0                        \n",
       "30  0.000000                  0                        \n",
       "⋮   ⋮                         ⋮                        \n",
       "378 0.000000                  0.000000                 \n",
       "379 0.000000                  0.000000                 \n",
       "380 0.000000                  0.000000                 \n",
       "381 2.363638                  1.437427                 \n",
       "382 0.000000                  0.000000                 \n",
       "383 0.000000                  0.000000                 \n",
       "384 0.000000                  0.000000                 \n",
       "385 0.000000                  0.000000                 \n",
       "386 0.000000                  0.000000                 \n",
       "387 0.000000                  0.000000                 \n",
       "388 0.000000                  0.000000                 \n",
       "389 0.000000                  0.000000                 \n",
       "390 0.000000                  0.000000                 \n",
       "391 0.000000                  0.000000                 \n",
       "392 0.000000                  0.000000                 \n",
       "393 0.000000                  0.000000                 \n",
       "394 0.000000                  0.000000                 \n",
       "395 0.000000                  0.000000                 \n",
       "396 0.000000                  0.000000                 \n",
       "397 0.000000                  0.000000                 \n",
       "398 0.000000                  0.000000                 \n",
       "399 0.000000                  0.000000                 \n",
       "400 0.000000                  0.000000                 \n",
       "401 0.000000                  0.000000                 \n",
       "402 0.000000                  0.000000                 \n",
       "403 0.000000                  0.000000                 \n",
       "404 0.000000                  0.000000                 \n",
       "405 0.000000                  0.000000                 \n",
       "406 0.000000                  0.000000                 \n",
       "407 0.000000                  0.000000                 \n",
       "    chr10_100028161_100028280 chr10_100028371_100028490\n",
       "1   0                         0                        \n",
       "2   0                         0                        \n",
       "3   0                         0                        \n",
       "4   0                         0                        \n",
       "5   0                         0                        \n",
       "6   0                         0                        \n",
       "7   0                         0                        \n",
       "8   0                         0                        \n",
       "9   0                         0                        \n",
       "10  0                         0                        \n",
       "11  0                         0                        \n",
       "12  0                         0                        \n",
       "13  0                         0                        \n",
       "14  0                         0                        \n",
       "15  0                         0                        \n",
       "16  0                         0                        \n",
       "17  0                         0                        \n",
       "18  0                         0                        \n",
       "19  0                         0                        \n",
       "20  0                         0                        \n",
       "21  0                         0                        \n",
       "22  0                         0                        \n",
       "23  0                         0                        \n",
       "24  0                         0                        \n",
       "25  0                         0                        \n",
       "26  0                         0                        \n",
       "27  0                         0                        \n",
       "28  0                         0                        \n",
       "29  0                         0                        \n",
       "30  0                         0                        \n",
       "⋮   ⋮                         ⋮                        \n",
       "378 0                         0.000000                 \n",
       "379 0                         0.000000                 \n",
       "380 0                         0.000000                 \n",
       "381 0                         1.437427                 \n",
       "382 0                         0.000000                 \n",
       "383 0                         0.000000                 \n",
       "384 0                         0.000000                 \n",
       "385 0                         0.000000                 \n",
       "386 0                         0.000000                 \n",
       "387 0                         0.000000                 \n",
       "388 0                         0.000000                 \n",
       "389 0                         0.000000                 \n",
       "390 0                         0.000000                 \n",
       "391 0                         0.000000                 \n",
       "392 0                         0.000000                 \n",
       "393 0                         0.000000                 \n",
       "394 0                         0.000000                 \n",
       "395 0                         0.000000                 \n",
       "396 0                         0.000000                 \n",
       "397 0                         0.000000                 \n",
       "398 0                         0.000000                 \n",
       "399 0                         0.000000                 \n",
       "400 0                         0.000000                 \n",
       "401 0                         2.011834                 \n",
       "402 0                         0.000000                 \n",
       "403 0                         0.000000                 \n",
       "404 0                         0.000000                 \n",
       "405 0                         0.000000                 \n",
       "406 0                         0.000000                 \n",
       "407 0                         0.000000                 \n",
       "    chr10_100069285_100069404 chr10_100108994_100109113\n",
       "1   1.379512                  4.468386                 \n",
       "2   1.613601                  3.718647                 \n",
       "3   1.515843                  4.497935                 \n",
       "4   0.000000                  4.102887                 \n",
       "5   0.000000                  3.661828                 \n",
       "6   1.253560                  4.152138                 \n",
       "7   0.000000                  4.431948                 \n",
       "8   0.000000                  4.532344                 \n",
       "9   0.000000                  3.025081                 \n",
       "10  0.000000                  3.897022                 \n",
       "11  0.000000                  4.157183                 \n",
       "12  2.985512                  4.670313                 \n",
       "13  0.000000                  4.279820                 \n",
       "14  0.000000                  4.204385                 \n",
       "15  0.000000                  4.306454                 \n",
       "16  0.000000                  4.819086                 \n",
       "17  0.000000                  4.534692                 \n",
       "18  0.000000                  4.296123                 \n",
       "19  0.000000                  4.625183                 \n",
       "20  3.551991                  3.729523                 \n",
       "21  0.000000                  4.419081                 \n",
       "22  1.329632                  3.985523                 \n",
       "23  0.000000                  4.607908                 \n",
       "24  2.050633                  4.489160                 \n",
       "25  1.721745                  4.427203                 \n",
       "26  0.000000                  4.737773                 \n",
       "27  0.000000                  4.237773                 \n",
       "28  0.000000                  4.289419                 \n",
       "29  0.000000                  4.439955                 \n",
       "30  2.076831                  4.155946                 \n",
       "⋮   ⋮                         ⋮                        \n",
       "378 0.000000                  3.905578                 \n",
       "379 0.000000                  4.318940                 \n",
       "380 2.688058                  3.958650                 \n",
       "381 1.437427                  4.397485                 \n",
       "382 0.000000                  3.997759                 \n",
       "383 2.377990                  4.372805                 \n",
       "384 0.000000                  4.590949                 \n",
       "385 0.000000                  4.777729                 \n",
       "386 2.313131                  3.737124                 \n",
       "387 2.437047                  4.037347                 \n",
       "388 2.352478                  4.659918                 \n",
       "389 0.000000                  4.396099                 \n",
       "390 1.937298                  4.681385                 \n",
       "391 0.000000                  2.818081                 \n",
       "392 0.000000                  4.324752                 \n",
       "393 0.000000                  4.087463                 \n",
       "394 2.049996                  4.409331                 \n",
       "395 1.384675                  4.556989                 \n",
       "396 1.206613                  3.867923                 \n",
       "397 1.716082                  3.792012                 \n",
       "398 2.216250                  4.274324                 \n",
       "399 0.000000                  4.166403                 \n",
       "400 0.000000                  3.999747                 \n",
       "401 0.000000                  3.966817                 \n",
       "402 0.000000                  4.488949                 \n",
       "403 0.000000                  4.717095                 \n",
       "404 0.000000                  4.662506                 \n",
       "405 0.000000                  4.341140                 \n",
       "406 0.000000                  3.542540                 \n",
       "407 0.000000                  4.415264                 \n",
       "    chr10_100174700_100174819 chr10_100174780_100174899\n",
       "1   2.764103                  2.556595                 \n",
       "2   2.201849                  2.201849                 \n",
       "3   3.994503                  3.495880                 \n",
       "4   2.655036                  2.390516                 \n",
       "5   3.661828                  3.484630                 \n",
       "6   4.565452                  4.331834                 \n",
       "7   2.989978                  3.163883                 \n",
       "8   5.506428                  5.373476                 \n",
       "9   3.844242                  3.975094                 \n",
       "10  2.569852                  2.569852                 \n",
       "11  2.898790                  3.564006                 \n",
       "12  2.714528                  1.208835                 \n",
       "13  1.868571                  1.868571                 \n",
       "14  3.073552                  2.593098                 \n",
       "15  2.579926                  2.579926                 \n",
       "16  3.792327                  2.737814                 \n",
       "17  3.180081                  1.724504                 \n",
       "18  4.126823                  4.242812                 \n",
       "19  3.302877                  3.154837                 \n",
       "20  3.729523                  3.880238                 \n",
       "21  1.270637                  1.812582                 \n",
       "22  2.701245                  1.880885                 \n",
       "23  3.584572                  3.407785                 \n",
       "24  4.410053                  4.324146                 \n",
       "25  1.721745                  0.000000                 \n",
       "26  4.443548                  4.364485                 \n",
       "27  2.658265                  2.149867                 \n",
       "28  1.451445                  1.451445                 \n",
       "29  2.941854                  2.562431                 \n",
       "30  5.458740                  5.443767                 \n",
       "⋮   ⋮                         ⋮                        \n",
       "378 3.057900                  2.577918                 \n",
       "379 0.000000                  0.000000                 \n",
       "380 2.422793                  1.487445                 \n",
       "381 2.363638                  2.004139                 \n",
       "382 3.499097                  3.499097                 \n",
       "383 1.773528                  0.000000                 \n",
       "384 0.000000                  0.000000                 \n",
       "385 0.000000                  2.124013                 \n",
       "386 2.955537                  2.955537                 \n",
       "387 3.361690                  3.085501                 \n",
       "388 3.385567                  2.996890                 \n",
       "389 1.535241                  1.535241                 \n",
       "390 0.000000                  0.000000                 \n",
       "391 0.000000                  0.000000                 \n",
       "392 2.766907                  2.392386                 \n",
       "393 1.332347                  1.884013                 \n",
       "394 2.049996                  2.676610                 \n",
       "395 4.308668                  4.386124                 \n",
       "396 2.542446                  2.856184                 \n",
       "397 2.059753                  1.716082                 \n",
       "398 1.863860                  1.314876                 \n",
       "399 0.000000                  2.086129                 \n",
       "400 0.000000                  0.000000                 \n",
       "401 3.017056                  2.011834                 \n",
       "402 2.440563                  2.608259                 \n",
       "403 0.000000                  0.000000                 \n",
       "404 0.000000                  0.000000                 \n",
       "405 3.493889                  3.608291                 \n",
       "406 1.398820                  0.000000                 \n",
       "407 3.734135                  3.583959                 \n",
       "    chr10_100206002_100206121 chr10_100206118_100206237 ⋯ hbv_load_IU_L\n",
       "1   0.000000                  0.000000                  ⋯ NA           \n",
       "2   0.000000                  0.000000                  ⋯ NA           \n",
       "3   2.092668                  0.000000                  ⋯ NA           \n",
       "4   3.036790                  0.000000                  ⋯ NA           \n",
       "5   1.990893                  1.990893                  ⋯ NA           \n",
       "6   2.141051                  0.000000                  ⋯ NA           \n",
       "7   0.000000                  0.000000                  ⋯ NA           \n",
       "8   3.455014                  3.065219                  ⋯ NA           \n",
       "9   0.000000                  0.000000                  ⋯ NA           \n",
       "10  2.569852                  1.950463                  ⋯ NA           \n",
       "11  1.904700                  0.000000                  ⋯ NA           \n",
       "12  2.859171                  2.341644                  ⋯ NA           \n",
       "13  0.000000                  0.000000                  ⋯ NA           \n",
       "14  0.000000                  0.000000                  ⋯ NA           \n",
       "15  1.959825                  1.959825                  ⋯ NA           \n",
       "16  3.398070                  2.107371                  ⋯ NA           \n",
       "17  0.000000                  0.000000                  ⋯ NA           \n",
       "18  3.320676                  0.000000                  ⋯ NA           \n",
       "19  3.431800                  0.000000                  ⋯ NA           \n",
       "20  0.000000                  0.000000                  ⋯ NA           \n",
       "21  0.000000                  0.000000                  ⋯ NA           \n",
       "22  2.234195                  0.000000                  ⋯ NA           \n",
       "23  2.539971                  1.922737                  ⋯ NA           \n",
       "24  3.059573                  2.050633                  ⋯ NA           \n",
       "25  0.000000                  0.000000                  ⋯ NA           \n",
       "26  2.634390                  0.000000                  ⋯ NA           \n",
       "27  1.063306                  0.000000                  ⋯ NA           \n",
       "28  1.451445                  2.380271                  ⋯ NA           \n",
       "29  1.943572                  0.000000                  ⋯ NA           \n",
       "30  4.259738                  2.439623                  ⋯ NA           \n",
       "⋮   ⋮                         ⋮                         ⋱ ⋮            \n",
       "378 1.620687                  0.000000                  ⋯        NA    \n",
       "379 0.000000                  0.000000                  ⋯        NA    \n",
       "380 0.000000                  0.000000                  ⋯ 199568.00    \n",
       "381 2.363638                  1.437427                  ⋯ 152953.00    \n",
       "382 0.000000                  0.000000                  ⋯     19.99    \n",
       "383 0.000000                  0.000000                  ⋯   1302.00    \n",
       "384 2.580103                  0.000000                  ⋯     19.99    \n",
       "385 3.632922                  0.000000                  ⋯ 124864.00    \n",
       "386 1.714340                  0.000000                  ⋯   7443.00    \n",
       "387 2.074363                  2.074363                  ⋯ 404758.00    \n",
       "388 2.352478                  0.000000                  ⋯ 213939.00    \n",
       "389 0.000000                  0.000000                  ⋯    253.00    \n",
       "390 0.000000                  0.000000                  ⋯    589.00    \n",
       "391 0.000000                  0.000000                  ⋯        NA    \n",
       "392 2.392386                  0.000000                  ⋯        NA    \n",
       "393 0.000000                  0.000000                  ⋯        NA    \n",
       "394 0.000000                  0.000000                  ⋯        NA    \n",
       "395 3.431897                  2.138161                  ⋯        NA    \n",
       "396 2.082741                  1.206613                  ⋯        NA    \n",
       "397 2.518238                  1.188253                  ⋯        NA    \n",
       "398 1.314876                  0.000000                  ⋯        NA    \n",
       "399 2.715182                  0.000000                  ⋯        NA    \n",
       "400 3.501061                  0.000000                  ⋯     19.99    \n",
       "401 0.000000                  0.000000                  ⋯     19.99    \n",
       "402 1.647080                  0.000000                  ⋯        NA    \n",
       "403 0.000000                  0.000000                  ⋯        NA    \n",
       "404 2.139235                  0.000000                  ⋯   6932.00    \n",
       "405 0.000000                  0.000000                  ⋯        NA    \n",
       "406 0.000000                  0.000000                  ⋯   1680.00    \n",
       "407 2.918199                  0.000000                  ⋯        NA    \n",
       "    hbv_antigen hbv_hepatitis_delta_antibody hbv_treatment meld_score\n",
       "1   NA          NA                           NA            11        \n",
       "2   NA          NA                           NA            NA        \n",
       "3   NA          NA                           NA            NA        \n",
       "4   NA          NA                           NA            19        \n",
       "5   NA          NA                           NA            40        \n",
       "6   NA          NA                           NA            NA        \n",
       "7   NA          NA                           NA            NA        \n",
       "8   NA          NA                           NA            NA        \n",
       "9   NA          NA                           NA            NA        \n",
       "10  NA          NA                           NA             8        \n",
       "11  NA          NA                           NA            NA        \n",
       "12  NA          NA                           NA             9        \n",
       "13  Nonreactive Positive                     No            10        \n",
       "14  NA          NA                           NA            NA        \n",
       "15  NA          NA                           NA             8        \n",
       "16  NA          NA                           NA            NA        \n",
       "17  NA          NA                           NA             8        \n",
       "18  NA          NA                           NA            NA        \n",
       "19  NA          NA                           NA            NA        \n",
       "20  NA          NA                           NA            NA        \n",
       "21  NA          NA                           NA            22        \n",
       "22  NA          NA                           NA            NA        \n",
       "23  NA          NA                           NA            NA        \n",
       "24  NA          NA                           NA            14        \n",
       "25  NA          NA                           NA            11        \n",
       "26  NA          NA                           NA            NA        \n",
       "27  NA          NA                           NA             7        \n",
       "28  NA          NA                           NA            14        \n",
       "29  NA          NA                           NA            NA        \n",
       "30  NA          NA                           NA            NA        \n",
       "⋮   ⋮           ⋮                            ⋮             ⋮         \n",
       "378 NA          NA                           NA            19        \n",
       "379 NA          NA                           NA            22        \n",
       "380 Reactive    Negative                     No            NA        \n",
       "381 Reactive    Unknown                      No            NA        \n",
       "382 Reactive    Negative                     No            NA        \n",
       "383 Reactive    Negative                     No            NA        \n",
       "384 Reactive    Negative                     No            NA        \n",
       "385 Reactive    Negative                     No            NA        \n",
       "386 Reactive    Negative                     No            NA        \n",
       "387 Reactive    Negative                     No            NA        \n",
       "388 Reactive    Negative                     No            NA        \n",
       "389 Reactive    Negative                     No            NA        \n",
       "390 Reactive    Negative                     No            NA        \n",
       "391 NA          NA                           NA             8        \n",
       "392 NA          Negative                     Yes           NA        \n",
       "393 NA          NA                           NA            22        \n",
       "394 NA          NA                           NA             7        \n",
       "395 NA          NA                           NA            15        \n",
       "396 NA          NA                           NA            19        \n",
       "397 NA          NA                           NA            11        \n",
       "398 NA          NA                           NA            14        \n",
       "399 NA          NA                           NA            15        \n",
       "400 Reactive    Negative                     Yes           NA        \n",
       "401 Reactive    Positive                     Yes           NA        \n",
       "402 NA          NA                           NA            17        \n",
       "403 Reactive    Negative                     Yes           NA        \n",
       "404 Reactive    Negative                     No            NA        \n",
       "405 NA          NA                           NA            17        \n",
       "406 Reactive    Negative                     No            NA        \n",
       "407 NA          NA                           NA            NA        \n",
       "    meld_na_score child_pugh_score child_pugh_class site_id_cc\n",
       "1   NA            3                3                UCLA      \n",
       "2   NA            3                3                UCLA      \n",
       "3   NA            3                3                UCLA      \n",
       "4   NA            6                A                UCLA      \n",
       "5   NA            3                3                UCLA      \n",
       "6    9            3                3                UCLA      \n",
       "7   NA            3                3                UCLA      \n",
       "8   NA            6                A                UCLA      \n",
       "9   NA            3                3                UCLA      \n",
       "10   8            3                3                UCLA      \n",
       "11  NA            3                3                UCLA      \n",
       "12  NA            4                4                UCLA      \n",
       "13  10            3                3                UCLA      \n",
       "14  NA            3                3                UCLA      \n",
       "15   8            3                3                UCLA      \n",
       "16  NA            3                3                UCLA      \n",
       "17   8            4                4                UCLA      \n",
       "18  NA            3                3                UCLA      \n",
       "19  NA            3                3                UCLA      \n",
       "20  NA            3                3                UCLA      \n",
       "21  17            7                B                UCLA      \n",
       "22  NA            5                A                UCLA      \n",
       "23  NA            3                3                UCLA      \n",
       "24  14            3                3                UCLA      \n",
       "25  11            3                3                UCLA      \n",
       "26  NA            3                3                UCLA      \n",
       "27  13            5                A                UCLA      \n",
       "28  12            6                A                UCLA      \n",
       "29  NA            3                3                UCLA      \n",
       "30  NA            5                A                UCLA      \n",
       "⋮   ⋮             ⋮                ⋮                ⋮         \n",
       "378 21             8               B                UCLA      \n",
       "379 26             6               A                UCLA      \n",
       "380 NA             3               3                UCLA      \n",
       "381 NA             3               3                UCLA      \n",
       "382 NA             3               3                UCLA      \n",
       "383 NA             3               3                UCLA      \n",
       "384 NA             3               3                UCLA      \n",
       "385 NA             3               3                UCLA      \n",
       "386 NA             3               3                UCLA      \n",
       "387 NA             5               A                UCLA      \n",
       "388 NA             3               3                UCLA      \n",
       "389 NA             3               3                UCLA      \n",
       "390 NA             3               3                UCLA      \n",
       "391  8             5               A                UCLA      \n",
       "392 NA             3               3                UCLA      \n",
       "393 22             7               B                UCLA      \n",
       "394  9             5               A                UCLA      \n",
       "395 15             7               B                UCLA      \n",
       "396 19             9               B                UCLA      \n",
       "397 15             9               B                UCLA      \n",
       "398 17             6               A                UCLA      \n",
       "399 15             8               B                UCLA      \n",
       "400 NA             3               3                UCLA      \n",
       "401 NA             3               3                UCLA      \n",
       "402 17             8               B                UCLA      \n",
       "403 NA             3               3                UCLA      \n",
       "404 NA             3               3                UCLA      \n",
       "405 19            11               C                UCLA      \n",
       "406 NA             3               3                UCLA      \n",
       "407 NA             3               3                UCLA      \n",
       "    rowname                     \n",
       "1   plasma-2333-P9-LV           \n",
       "2   plasma-2300-P9-LV           \n",
       "3   plasma-2270-P9-LV           \n",
       "4   plasma-1969-P9-LV           \n",
       "5   plasma-1976-P9-N            \n",
       "6   plasma-2742-P9-N            \n",
       "7   plasma-2800-P9-N            \n",
       "8   plasma-3931-P9-N            \n",
       "9   plasma-2518-P9-N            \n",
       "10  plasma-2357-P9-N            \n",
       "11  plasma-2873-P9-N            \n",
       "12  plasma-1959-P9-N            \n",
       "13  plasma-3038-P9-LV           \n",
       "14  plasma-2091-P9-N            \n",
       "15  plasma-3390-P9-LV           \n",
       "16  plasma-2741-P9-N            \n",
       "17  plasma-2104-P9-N            \n",
       "18  plasma-2188-P9-BD           \n",
       "19  plasma-3627-P9-LV           \n",
       "20  plasma-3134-P9-N            \n",
       "21  plasma-2733-P9-N            \n",
       "22  plasma-2743-P9-N            \n",
       "23  plasma-2778-P9-PR           \n",
       "24  plasma-2370-P9-N            \n",
       "25  plasma-2248-P9-N            \n",
       "26  plasma-3427-P9-N            \n",
       "27  plasma-2549-P9-N            \n",
       "28  plasma-2090-P9-N            \n",
       "29  plasma-2441-P9-N            \n",
       "30  plasma-2349-P9-N            \n",
       "⋮   ⋮                           \n",
       "378 plasma-2250-P9-CH           \n",
       "379 plasma-2428-P9-CH           \n",
       "380 plasma-2297-P9-CH           \n",
       "381 plasma-3409-P9-CH           \n",
       "382 plasma-2371-P9-CH           \n",
       "383 plasma-3331-P9-CH           \n",
       "384 plasma-2010-P9-CH           \n",
       "385 plasma-2867-P9-CH           \n",
       "386 plasma-2502-P9-CH           \n",
       "387 plasma-2094-P9-CH           \n",
       "388 plasma-2738-P9-CH           \n",
       "389 plasma-2573-t2-5day-P9-CH   \n",
       "390 plasma-2536-P9-CH           \n",
       "391 plasma-2304-P9-CH           \n",
       "392 plasma-2368-P9-CH           \n",
       "393 plasma-2567-r1-t1-4day-P9-CH\n",
       "394 plasma-2560-r1-t1-4day-P9-CH\n",
       "395 plasma-3087-P9-CH           \n",
       "396 plasma-3896-P9-CH           \n",
       "397 plasma-3944-P9-CH           \n",
       "398 plasma-3366-P9-CH           \n",
       "399 plasma-2574-5day-P9-CH      \n",
       "400 plasma-3964-P9-CH           \n",
       "401 plasma-2575-5day-P9-CH      \n",
       "402 plasma-3448-P9-CH           \n",
       "403 plasma-2576-5day-P9-CH      \n",
       "404 plasma-2577-5day-P9-CH      \n",
       "405 plasma-3915-P9-CH           \n",
       "406 plasma-2568-r1-4day-P9-CH   \n",
       "407 plasma-3917-P9-N            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Setting row names on a tibble is deprecated.”\n"
     ]
    }
   ],
   "source": [
    "as.data.frame(combined_data_clean)\n",
    "rownames(combined_data_clean) <- combined_data_clean$plasma_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b8e51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the resulting dataframe\n",
    "#print(rownames(combined_data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38859424",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if 'plasma_alias' is a column in the dataframe\n",
    "\"plasma_alias\" %in% colnames(combined_data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446582fd",
   "metadata": {},
   "source": [
    "# Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68fdba05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]    407 144966\n"
     ]
    }
   ],
   "source": [
    "columns_to_remove <- colnames(filtered_pheno_data)\n",
    "\n",
    "# Remove columns in `combined_data_clean` that are present in `columns_to_remove`\n",
    "combined_data_clean_filtered <- combined_data_clean[, !colnames(combined_data_clean) %in% columns_to_remove]\n",
    "\n",
    "# Verify the dimensions of the cleaned dataset\n",
    "print(dim(combined_data_clean_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3f1485",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"donor_id\"                     \"plasma_alias\"                \n",
      " [3] \"tumor_tissue_alias\"           \"normal_tissue_alias\"         \n",
      " [5] \"sample_source\"                \"info_source\"                 \n",
      " [7] \"phase_HBV\"                    \"gender\"                      \n",
      " [9] \"race\"                         \"hispanic\"                    \n",
      "[11] \"specimen_collection_date\"     \"dx_cirrhosis\"                \n",
      "[13] \"dx_hcc\"                       \"bclc_stage\"                  \n",
      "[15] \"prime_etiology\"               \"non_prime_liver_disease_name\"\n",
      "[17] \"prime_liver_disease_name\"     \"ascites\"                     \n",
      "[19] \"ascietes_severity\"            \"encephalopathy\"              \n",
      "[21] \"encephalopathy_severity\"      \"afp_ng_mL\"                   \n",
      "[23] \"hbv_antigen\"                  \"hbv_hepatitis_delta_antibody\"\n",
      "[25] \"hbv_treatment\"                \"child_pugh_class\"            \n",
      "[27] \"site_id_cc\"                   NA                            \n"
     ]
    }
   ],
   "source": [
    "non_numeric_cols <- sapply(combined_data_clean, function(x) !is.numeric(x) && !is.integer(x))\n",
    "print(names(combined_data_clean_dt)[non_numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0c6ee2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>578</li><li>145034</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 578\n",
       "\\item 145034\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 578\n",
       "2. 145034\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]    578 145034"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>407</li><li>145035</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 407\n",
       "\\item 145035\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 407\n",
       "2. 145035\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]    407 145035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(combined_data)\n",
    "dim(combined_data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5a604",
   "metadata": {},
   "source": [
    "# Creating and Storing the Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a23a29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# library(caret)\n",
    "\n",
    "# # Set seed for reproducibility\n",
    "# set.seed(123)\n",
    "\n",
    "# # Number of folds\n",
    "# k <- 5\n",
    "\n",
    "# # Define the classification function for Child-Pugh scores\n",
    "# classify_child_pugh <- function(score) {\n",
    "#   if (score < 5) {\n",
    "#     return(1)\n",
    "#   } else if (score >= 5 && score <= 6) {\n",
    "#     return(2)\n",
    "#   } else if (score >= 7 && score <= 9) {\n",
    "#     return(3)\n",
    "#   } else if (score >= 10 && score <= 15) {\n",
    "#     return(4)\n",
    "#   } else {\n",
    "#     return(4) # If there are any values above 15\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# # Apply the classification function to the Child-Pugh scores\n",
    "# combined_data_clean$child_pugh_class <- sapply(combined_data_clean$child_pugh_score, classify_child_pugh)\n",
    "\n",
    "# # Ensure that the Child-Pugh class column is a factor\n",
    "# combined_data_clean$child_pugh_class <- factor(combined_data_clean$child_pugh_class)\n",
    "\n",
    "# # Perform stratified k-fold cross-validation based on the newly classified Child-Pugh class\n",
    "# folds <- createFolds(combined_data_clean$child_pugh_class, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# # Perform k-fold cross-validation manually and save each fold's data\n",
    "# for (i in 1:k) {\n",
    "#     # Get the training and testing indices for stratified splits\n",
    "#     train_indices <- folds[[i]]\n",
    "#     test_indices <- setdiff(1:nrow(combined_data_clean), train_indices)\n",
    "  \n",
    "#     # Subset the training and testing data\n",
    "#     training_set <- combined_data_clean[train_indices, ]\n",
    "#     testing_set <- combined_data_clean[test_indices, ]\n",
    "\n",
    "#     # Print class distribution in each fold\n",
    "#     cat(paste(\"Fold\", i, \"Class Distribution in Testing Set:\\n\"))\n",
    "#     print(table(testing_set$child_pugh_class))\n",
    "\n",
    "#     # Save the data as .rds\n",
    "#     saveRDS(training_set, file = paste0(\"/u/home/c/ctang04/Liver Project/data/childpugh_training_set_fold_\", i, \".rds\"))\n",
    "#     saveRDS(testing_set, file = paste0(\"/u/home/c/ctang04/Liver Project/data/childpugh_testing_set_fold_\", i, \".rds\"))\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e049f",
   "metadata": {},
   "source": [
    "# Read in the Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f22b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read all training and testing sets into memory before the loop\n",
    "#training_sets <- lapply(1:k, function(i) read.csv(paste0(\"/u/home/c/ctang04/Liver Project/data/training_set_fold_\", i, \".csv\")))\n",
    "#testing_sets <- lapply(1:k, function(i) read.csv(paste0(\"/u/home/c/ctang04/Liver Project/data/testing_set_fold_\", i, \".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "482d94b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read .rds files\n",
    "k<-5\n",
    "training_sets <- lapply(1:k, function(i) readRDS(paste0(\"/u/home/c/ctang04/Liver Project/data/childpugh_training_set_fold_\", i, \".rds\")))\n",
    "testing_sets <- lapply(1:k, function(i) readRDS(paste0(\"/u/home/c/ctang04/Liver Project/data/childpugh_testing_set_fold_\", i, \".rds\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0597767d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Child-Pugh Classes:\n",
      "\n",
      "  1   2   3   4 \n",
      "210 118  61  18 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "\n",
    "# Define the classification function for Child-Pugh scores\n",
    "classify_child_pugh <- function(score) {\n",
    "  if (score < 5) {\n",
    "    return(1)\n",
    "  } else if (score >= 5 && score <= 6) {\n",
    "    return(2)\n",
    "  } else if (score >= 7 && score <= 9) {\n",
    "    return(3)\n",
    "  } else if (score >= 10 && score <= 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(4) # If there are any values above 15\n",
    "  }\n",
    "}\n",
    "\n",
    "# Apply the classification function to the Child-Pugh scores\n",
    "combined_data_clean$child_pugh_class <- sapply(combined_data_clean$child_pugh_score, classify_child_pugh)\n",
    "\n",
    "# Ensure that the Child-Pugh class column is a factor\n",
    "combined_data_clean$child_pugh_class <- factor(combined_data_clean$child_pugh_class)\n",
    "\n",
    "# Calculate and print the distribution of actual classes\n",
    "class_distribution <- table(combined_data_clean$child_pugh_class)\n",
    "cat(\"Distribution of Child-Pugh Classes:\\n\")\n",
    "print(class_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75537fbd",
   "metadata": {},
   "source": [
    "# Predicting Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25355db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:IRanges’:\n",
      "\n",
      "    cov, var\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:S4Vectors’:\n",
      "\n",
      "    cov, var\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:BiocGenerics’:\n",
      "\n",
      "    var\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 325   2\n",
      "[1]    325 144965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.975609756097561 \n",
      "[1] 325   2\n",
      "[1]    325 144965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.752136752136752 \n",
      "[1] 325   2\n",
      "[1]    325 144965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 AUC: 0.634146341463415 \n",
      "[1] 326   2\n",
      "[1]    326 144965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nBwAAAMARKOwAAAAAOOIn4+r/7ZTscRgAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 AUC: 0.95 \n",
      "[1] 327   2\n",
      "[1]    327 144965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94\nAAAgAElEQVR4nOzdd1wT9/8H8HcIGwOoLFFUVBTFLbgKiAOFoqIi4ECtX3HvWn9Fa6tW/bpX\nW62rTkSr4CiI4qgDq3UjVdGKggMRBQQlQCDJ/f64fimGgKzkkuP1/KOP3ucul1cuAV7eioBh\nGAIAAAAA7afDdQAAAAAAqB4odgAAAAA8gWIHAAAAwBModgAAAAA8gWIHAAAAwBModgAAAAA8\ngWIHAAAAwBModgAAAAA8ocXFTiKRCMrk6elZzlV9++23AoHg119/LXux2rVr29nZlbGAVCpd\nu3Zt9+7da9eubWNj4+HhceDAgfK+HqLU1FRdXV2BQFC/fn25XF7+BxYpLCx89OhRSkpKJR5b\ntnJuompX8m0VCoVNmzYdMGDA5cuXVfrUxd/u2bNnCwSCEydOlPOx1ftGfPKDBwAAwNLlOkA1\naNeunUAgKDnetGlTdcaQy+W9evWKjY0VCoXt2rXT0dG5cuXKxYsXz549+8svv5RnDQcPHpTJ\nZET06tWrixcv9uzZs6IZnj9/7ujoOGjQoKNHj1b4BWiwNm3aCIVC9v/z8vKSkpKePn0aFRW1\nbt262bNnc5tNKb6+EQAAoOH4UOxu3bpV9FefQzt27IiNjW3Tps3Zs2etrKyI6OHDhz169Ni5\nc2dQUFB5WlpoaCgReXh4XLhwISwsrBLFjq+uXr1qYmJSNJmbm7to0aLVq1fPnz9/8ODBjRs3\nVnWAcePGeXh4ODs7q/qJAAAAqkKLD8VqmoiICCL66aef2FZHRI6OjrNmzSKiCxcufPLhCQkJ\nt2/ftrOz+/nnn4koPDy8oKBAhXG1mbGx8apVq5ydnfPz8y9duqR0mYyMDHb3Z7Vo3bq1r6+v\ntbV1da0QAABAFWpKsdu/f7+3t7eNjY2tra23t/e+ffvKXl4ikXz77bddu3Y1MzPr1q3bggUL\nxGJx2Q9JSEggorZt2xYftLW1JaKXL19+MiG7u27UqFGOjo4dO3bMyso6efKk0iV37NjRt2/f\nunXrtmjRIigoKDY2lh0fMGBAs2bNiOjYsWMCgWD69OlENH36dIFAcPHixeJr+OOPPwQCweTJ\nk4tGpFLpmjVrevToYW1tbWpq6uTkFBIS8vbt20/GZo0bN04gEGzcuFFhfO7cuQKBYPHixexk\nfHz8sGHDmjZtamxs7ODgMH78+OfPn5fzKUpq164dEf3999/s5LJlywQCwa1bt65fv96hQwcr\nK6ucnJyi1xsQENC0aVNTU1NnZ+dNmzYplOZPvt3z5s0reY5dhd6I6koCAABQFkZr5efnsy9B\nKpWWveQXX3xBRLq6uu3bt2/fvr2uri4RBQUFFS2wYMECIjp48CA7mZGRwR5009XV7dixI3ve\neteuXU1MTBo0aFBGnvz8fIXBMWPGENH69evLTiiXy9njiQ8fPmQYZtWqVUQUEBCgsJhMJhs2\nbBgRGRgYdOvWrU2bNkQkEAj279/PMExYWNiMGTOIyNHRcdGiRdHR0QzDTJs2jYguXLhQfD3s\nZQeTJk1iJyUSiYuLCxGZmZm5u7u7u7ubmZkRUfv27fPy8pRuIgUxMTFE1KNHD4Vx9kUlJiYy\nDBMbG6uvr09ETk5OvXv3rl+/PhHZ2dllZGSUsWXYtzgnJ6fkrC5duhDR1q1b2cmlS5cSUXh4\nuKWlZYMGDTw9PcViMcMw69atEwqFQqGwbdu2Xbt2NTIyIqLevXuzc5nyvd0hISFEFBUVVek3\norqSAAAAlIH/xY49Qtq0aVO2MzEMk5CQwF5XER4ezo4otBb2fHxnZ+eUlBR2ZP/+/WwpKc/f\n1/z8/JMnT27evJn929+mTZv379+X/RB2Z0+XLl3YyeTkZIFAYGRkpPDAXbt2EVHnzp3T0tLY\nkePHjwuFwrp167LlIDExkYgGDRpU9JDyFLs9e/YQkbu7e1F/ysnJ6datGxFdvHhR6SZSUFhY\nWLduXaFQ+Pbt26LBa9euEZGrqys76ebmVnybS6XSoUOHEtHGjRvL2DJKi11eXt4333xDRHp6\negkJCewgW+zq1KmzaNGioo/EvXv3hEJho0aN7ty5w468fv2aPXlx3rx57Eh53m6FYleJN6K6\nkgAAAJSBD8VOqaFDh7KLsQfszpw5U/yxp06dYvdIsZPFW0tGRoaBgYGenl5ycnLxh7AH1Mrz\n95X9o85q0qTJixcvPvmQiRMnEtHmzZuLRthetXfv3uKLNWrUSCAQFFUZ1uDBg4no0qVLTGWL\n3f79+4cOHcquocjy5cuJaPfu3exk2cWOYZjx48cT0S+//FI0MmfOHCLavn07O1m3bl1dXd3C\nwsKiBe7du7dixYrTp0+XsWXYzdihQwfn/3FycjI2NmbH161bV7QkW+ycnZ2LP3zIkCFEpPAU\nKSkphoaG5ubmMpmsnG+3QrGrxBtRXUkAAADKwIdz7Nq1a9e+BPYgYGFh4b179+rVq9enT5/i\nD+nXr5+Njc29e/ekUqnC2h48eCCRSLy8vBo1alR8PDg4uJx5rKysdu3atXr16sGDByclJbVv\n3/7GjRtlLF9QUHD48GF9ff3AwMCiQXZvX1hYWNFIamrqs2fPnJ2dHR0diz989+7dL1++rMoF\nmyNGjDh8+DC7R42IGIaJi4tju2/5seGL390jPDzc0NDQ39+fnXRycpJKpb6+vufOnWPv0ufk\n5PT111+X53aDd+7cufk/9+/fNzAwcHNzO3nyZMl7nXh7exefvHbtmpmZWe/evYsP2tratmnT\nJisr6++//67E2125N0IVSQAAABTw/HYnycnJMplM6Q3t7O3tX79+/fz58yZNmhQfZ/e1NG/e\nXGF5BweHcuYRiUTsWX1EtG3btokTJ06cOPH27dulLX/y5MnMzExTU1P2hDxWVlYWEZ05c+bN\nmzfsZbZsMHt7e4WHm5qampqaljNbad68eXPixIm4uLi4uLi7d+9mZ2dXdA0eHh6WlpZnz57N\nycmpVavWtWvXnj17FhgYyJ6uR0RbtmwZMmRIdHR0dHQ0e2WAj49PQEBA0UXEZcjJySl+u5My\nFL+Rb05ODnuL4NI+Hu/evavE212JN0JFSQAAABTwodiVgfnfsbyS2EsoSt5SRE9PT+nyhoaG\n7NlOpT2RXC4XCAQ6Oh/tBB03btzMmTPv3LnD1h2lj2Wvh33//n1UVJTCLJlMdvjw4alTpxIR\ne+i5tHgVorBZzp8/7+vr++HDh9q1a/v4+AwbNszFxeX06dPseWzlJBQK/fz8tmzZcurUqaFD\nhx46dIiIilfVli1bxsfHnzlzJioq6sKFCzExMadOnVqwYEFYWNjnn39e9RfFKjpKS0Ts7U5s\nbGwmTZqkdGFra+unT58qnVXG212JN0JFSQAAABTwvNg1btxYR0cnKSmp5KwnT54IhUKF3XVE\nxI4U3USjyMuXL8u4sdyff/7ZvXt3Ly8vhXuUCIXCWrVq5efnl3ZPtezs7KioKF1d3dTUVAsL\ni+KzNm/ePHXq1LCwMLbYsXtukpOTFdaQmJh469atDh06lNzZUxqFr7qaOHFiTk7Onj17RowY\nwfZdKt+99xQEBgZu2bLl6NGjfn5+hw8ftra2VjjMqqen9/nnn7M17uXLl2vXrt2wYUNwcPCr\nV68q+lzlYWZmZmFhIRAIFi5cWNoyaWlpVMG3uxJvhIqSAAAAKODDOXZl0NfXb9myZUpKyvnz\n54uPnzt37tWrV61atSq5L6Rly5ZGRkYxMTEKt1jbu3dvGU/UokULHR2dGzdu5ObmFh9//Phx\nenq6vb190RFJBeHh4fn5+b169VJodUTk5+fHfi8ZW0wbNmxYp06da9euKVSKZcuWDRs2rGTP\nKO7du3fFJ0+fPl30/7m5uY8fP7azsxs9enRRqyMipW24bO7u7jY2NidOnIiNjX3x4sXIkSOL\nVvjo0aOWLVsOHDiwaOEGDRqsW7fO0tIyNTVVdbdq69ChQ2pqatEd5ljv3r1r0qRJ165dqVJv\nd+XeCFUkAQAAUMDzYkdE3333HRFNmjSp6GLVv//+mz0ipnT3ibm5+ZQpUwoKCgIDA9mdKER0\n8uTJFStWlPEsderU8fHxycjImDJlSlG3e/Xq1dixY+njI5IK9u/fT0QBAQElZ1lbW7MXNBw8\neJCIdHR0FixYIJVKR48enZGRwS5z9uzZ0NDQunXrfvbZZ0UP/PDhQ9H/sxeR7Ny5s2iX4bFj\nx9j7m7CMjY0tLCxSU1OLto9cLt+6deu2bduIKC8vr4xXrUBHR8fPzy87O5u9kLP4q7a3t09K\nSoqMjCx+j9+TJ0+mp6e3aNGinOfPVQL7FgcEBNy5c4cd+fDhw5gxY5KSkvr160eVersr90ao\nIgkAAIAibi/KrYpy3sdOLpePHDmSiPT19V1cXJydndmzo8aMGVO0TGk3KDYwMOjSpQt7ZK1z\n586dO3cu464Tz549s7GxIaI6deq4urp27tyZvQNtr169it/jo7gXL17o6Ojo6emVdpPen376\niYicnJzYSYlEwpYAExMTNzc3Z2dn9qy+I0eOsAuw3xWhr68fEBCwc+dOhmGSk5PZnYXstyOw\nd1FhX1HR7U6+//57IqpVq9bQoUPZ70UQiUQjRowgooYNG27YsKHkJipN0Rd8tW3bVmEWe9dl\nImrVqpWXl1fr1q2JSFdXV+FONArYhyi9QbEC9nYnoaGhCuPsnUoEAoGjo2Pv3r3Nzc2JyN3d\nXSKRsAuU5+1WuN1JJd6I6koCAABQBv4XO9aePXv69u1rbW1tY2Pj5eXFfkNAkZKtJT8/f/78\n+Z07dzY2Nq5fv/7s2bNzcnI8PDzK/vualpY2c+bMZs2aGRoa2tra9urVa9euXXK5vLTlV65c\nSUTe3t6lLZCamspejXH37l12RC6Xb9iwwcPDw9zcvEGDBgMGDLh582bxhyxatKhOnTrGxsYL\nFixgR+7cuePj42Npaclurs6dOz948KB4sZPJZJs3b27btq2JiYmjo+OYMWOSk5NzcnIGDx5s\namrq7++vdBMpJZPJ2G9RW7t2rcIsuVx++PBhd3f3evXqGRoaOjg4jBw5Mj4+vuwVVr3YMQxz\n4sSJAQMG2NnZsV/ktXHjxqIuxfrk261Q7JhKvRHVkgQAAKAMAqb060aBZ7Kyst6/f9+wYUOu\ngwAAAIBKoNgBAAAA8AT/L54AAAAAqCFQ7AAAAAB4AsUOAAAAgCdQ7AAAAAB4AsUOAAAAgCdQ\n7AAAAAB4AsUOAAAAgCdQ7AAAAAB4AsUOAAAAgCdQ7AAAAAB4AsUOAAAAgCdQ7AAAAAB4QouL\n3YkTJwQl2NjY9O7d+9y5c1yn+zSpVCqVSkube//+fUtLy9Lmavtrr6iyt8avv/6qsClMTU1d\nXFx++eUXhmHUmZOXyv6gAgCARtHiYsfy9vZe8D8hISG9e/f+448/PD09L168WF1PkZWVNX78\neHt7+wYNGgQFBaWlpZW25Pv370v2LW9v75JLZmRkNGrUaPr06VUJVvZrl8vlc+bMqVevXu3a\ntUeMGJGdnV2V5ypS/q1BRNevXx8+fHjz5s3NzMy6dOly8ODB4nOvXbvm7e1tZWVlb28/cuTI\nly9fViWYr68vuym++eabwMDAZ8+eBQcHL1q0qCrr1C4a+0EFAAC10eU6QFX5+vpOnDix+MiY\nMWP69eu3cuXKHj16VH39YrG4S5cur1+/DgoK0tXVDQ0NjY2NvX37dt26dUsu/OTJEyLq16+f\nnZ1d0aCTk1PJJcePH//q1asqZiv7tYeFhYWHh//+++/6+vrDhw//9ttvf/jhhyo+Y4W2xrVr\n19zd3UUiUUBAgI2NzdGjR4cPH56UlDRv3jwiOnbsmJ+fX/PmzadMmZKXl7dnz55u3bpdunTJ\n3t6+ctn8/f1HjhxZNLl8+fK2bduuWLFi7ty5tWrVqtw6tYgmf1ABAEB9GK0VFRVFRFu2bCk5\ny9zc3N7evlqeZePGjUR0+vRpdvLq1as6OjpLlixRuvDhw4eJ6M6dO2Wvc8eOHQYGBkQ0adKk\n0pa5d++ehYVFaXPL89oXL1783XffsYOrV6/28vIqO1V5VGhr9O7d28TE5K+//mInpVKpu7u7\nrq7umzdvJBKJlZVVmzZt8vLy2LmvX7+2tLQcN26c0lWVvTXYHYGhoaEK4+x+pps3b1boNZbh\n3bt31bIemUxWWFhYLasqwtUHFQAANIrWH4pVSiAQsH+Qqm7Tpk2tW7f29PRkJ7t27dqtW7ef\nfvpJ6cKJiYlE1LRp0zJWmJiYOHPmzGXLllVLvJKKXvt33323ePFiIkpKStq+ffvgwYOrvvLy\nbw2GYS5fvty9e/fWrVuzI0KhcOzYsVKp9ObNm/fv33/z5k1wcLChoSE719raetSoUfv27ROL\nxVXPWZxEIiGi5OTk4cOH29vbm5mZ9ejR48SJE8WXiY6O9vDwsLa2NjU17dix4/bt24tm9ezZ\n09/f//Hjx15eXs7OzkT04cOHefPmOTg4GBsbN23adO7cucUz375928fHx8bGxtbW1sfH5/bt\n20WzvL29Bw8e/Ouvv9arV09PT69evXoTJ058//59tbxMrfugAgCAKvCw2F24cOHdu3f9+/ev\n+qqkUmliYqKHh0fxQQ8Pj7S0NKWnrD158sTKykoqlR47diw0NDQ+Pr7kCkeOHNmlS5cZM2ZU\nPV5JCq+9oKBg6dKlnTp1mjx58oQJE6q48gptDYlEMnfu3JkzZxYffPbsGREZGRnl5OQQkVAo\nLD7X0NCwoKDgxYsXVczJyszMjIiIEAqFLVu2/Ouvv9q3bx8bGzts2LA5c+ZkZ2cPGDBg27Zt\n7JJ79uzx8fHJzMwcM2bM5MmT5XL5hAkTDh06VLSq7OzswYMHW1tbh4SEEFFQUNCaNWvat28/\nf/58JyenNWvWFL2b586d69at271798aOHTtmzJh79+5169bt7NmzRau6e/fuF1984e/vv337\ndjc3t23bts2ZM6fqL1brPqgAAKAiWn+OXVRU1OvXr9n/l0qlycnJ4eHh3t7e33//fdVX/vr1\na7lcrnA9JjuZkpJiZmamsHxiYmJeXp69vX3RX9O+ffvu3r27Xr167OSiRYv+/vvv+Ph4HZ1q\nqNRlv/asrCxPT09ra+vbt283bty46k9Xoa1haGi4ZMmS4iNJSUk7d+60s7Pr3r17fn6+oaFh\nWFjYpEmT2Hr3/v179ohqSkqKo6NjJeIdOXKE3RHFMMybN2+OHj36+vXrefPm1a5d28/Pz9zc\nPC4uztzcnIjmz5/fp0+fL7/8cvjw4SKRKCwsrGHDhjdv3tTX1yeiJUuWWFpanj17NiAggF3z\nmTNntm3bNn78eCLKzs6OjIycOXPm+vXr2bmjRo26ceMGEcnl8i+//NLCwuLWrVsWFhZE9OWX\nX7Zr127u3Lm3b98WCAT0v72nwcHBRBQcHOzi4nL69OlKvFgFGv5BBQAAteFDsWNPOCuip6fX\nv39/IyOjkgsXFBScP39e6XrMzMy6du2qMPj27VsiMjU1VViSiN68edOqVSuF5Z88eSKXyzds\n2DBgwACpVLpv37758+ePGTOG/eN9+fLlFStW7N27187OTiaTVex1KlP2a1+xYoVMJgsNDRUI\nBNnZ2bq6uiYmJsUXVvXWKC4yMjI4ODgnJycmJkZfX19fX/+///3vl19+2bVr10GDBonF4tDQ\nULlcTkRsAaqEI0eOHDlyhP1/oVDYpEmTOXPmzJo16927d+fPn1+6dCnb6ohIV1d30qRJw4cP\n//PPPz09PSMiIgQCAdvqiCg9PV0qlebl5RWt2cTE5D//+U/RY3V0dM6dO/fo0aMWLVoQ0b59\n+9hZSUlJ8fHxS5cuZVsdEVlaWk6aNGnhwoXPnj1ju3WtWrXGjh1btOa2bdv+9ttvJV8Lzz6o\nAACgPlyf5Fd5Si8gSE5O9vHxoWJnkRdXtH+rpK5du5ZcPjk5mYgUzkBnz1KPi4sruXxKSorC\n+fVTp04lonv37mVlZTVq1CggIIAdZ28MVr0XTyi8dldX1+Iv0MPDQ81bg/XkyZO+ffsS0Wef\nfZaQkFB81qFDh7p06SISiZycnJYtW8buA7t//35Ft0ZpF0+wrl69WtrLDAsLY5e5ffv2jz/+\nGBwc3KlTJ3YPYlBQEDvLw8OjZcuWxVe4fv16tgW2b99++vTpZ86ckcvlDMPExMQQ0bFjx4ov\nzHbNs2fPMgzj5eXl5ORUfG5wcLDS16VFH1QAANAoWr/HTkGjRo3WrVt34sSJc+fOFZ1IXsTa\n2pqpyB1rbWxsiCg9Pb34IDtpa2tbcvmSg3369Nm0adP9+/cjIyOfP38+evRo9s8tuyMkPj5+\n48aNnTt37tatW/lTlUbhtcfGxpa9vKq3BhHt3r176tSpFhYWhw4d8vf3V5jr7+9ffPDrr78u\nepZqxJawBQsW9OnTR2EWu8tt6dKl3333XcOGDQcNGhQSEuLs7KxwsprCns5Zs2YFBAQcP378\n7Nmz+/fv//HHH3v37n3y5El2rsIeR/ZQZtENfvX09MqTmd8fVAAAUB2+FTsiatKkCRFlZGSU\nnFXRI1wGBgbNmjW7fPly8cE//vjDysqq6HBbkWfPnp06dcrT05MNwPrw4QMR2dnZsTurFE47\nu3LlypUrVxYsWFBdfy/LeO0lqXRrEFFERMS4ceM+//zzPXv21KlTR2Hurl277O3ti1eo6Ojo\njh07llyyithrP3V1dYvf1/DRo0c3b950dnb+8OHD999/P2HChC1bthTNLeP4Y0ZGRnJysoOD\nw+TJkydPniyRSEJCQjZs2BAdHc1e/3vv3r2BAwcWLX/v3j0iat68eYUy8/6DCgAAqsLp/sIq\nKe1ebuxf5YEDB5Z8SEWPcDEMs3z5coFAcOPGDXYyISFBT08vJCSk5JIpKSlCobB///7sgTmG\nYQoKCrp162ZtbZ2bm6uwsCoOxTJlvvaSVLo15HJ5s2bNWrRoIZPJlK7qs88+a9Cgwfv379lJ\n9s5q27dvV7pwVQ7FMgzTp08fGxubJ0+esJMSiaRLly52dnZyufyvv/4iovXr1xctHBsbKxAI\nih+KdXZ2LprL9q3FixcXjbAHW6OiomQyWevWrevXr5+RkcHOSk9Pr1+/fuvWrdmN4OXl1b59\n++LBqutQLMPdBxUAADQKD/fY6ejo1K9fnz3rSEFFj3AR0fjx43fv3j1w4MCZM2fq6Oj89NNP\n9evXL/qGpdDQ0Hnz5n355ZezZ8+2tbVdtmzZvHnz3NzcfHx88vLywsPDHz9+HB4ervRKDlUo\n47WXpNKtER8fn5iY6OLiUvJ2HpMnT27evPmiRYv69ev32WefDR8+/PHjx/v37+/cuXNQUFCF\n8pTT6tWr3d3d2eeysbE5fPjwrVu3Dh48KBAIWrRo0bhx46VLl75+/bp58+bXr1+PiIiwtra+\nevXquXPnevfurbCqLl26NG/efNmyZSkpKc2bN4+Pjz9+/HiLFi169Oiho6Ozbt06Hx8fZ2fn\nUaNGMQyzb9++t2/f7tmzp6LXlvL+gwoAAKrCba+sijK+fYE9W//IkSPV8kRv3rwJCgpq2LBh\n/fr1AwMDX758WTSLPX63aNGi4qm8vLysrKzq1avn4+Nz69YtpetU0R47prpfe0nl3BoRERGl\nfeTYKwkYhomOjnZxcalVq5aLi8s333zz4cOH0p60invsGIZ5/PjxkCFDGjRoYGZm5ubmdvLk\nyaJZDx488PLyMjMzs7OzGz58+IsXL/bu3WtlZdWvXz+mxB47hmGePn06cuTIBg0aGBgYNGnS\nZPLkySkpKUVzb9y44eXlZW1tbW1t7e3tffv27aJZ5d9jVzmcfFABAECjCJgK7hgA9bh//76H\nhwd7GwvA1gAAACgP3H0UAAAAgCdQ7AAAAAB4AsVOQ9nb2x8/fpzrFJoCWwMAAKA8cI4dAAAA\nAE9gjx0AAAAAT6DYAQAAAPAEih0AAAAAT6DYAQAAAPAEih0AAAAAT6DYAQAAAPAEih0AAAAA\nT6DYAQAAAPAEih0AAAAAT6DY8YdcLs/Pz+c6hUYrLCzMycmRSqVcB9FoeXl5+EKasuXk5OTl\n5XGdQqNJpVKJRMJ1igr4+eefJ06cGBISorZ3ViKR5OTkyGQy9TydlhKLxVxH0D66XAeAaiOX\nywsKCgwNDbkOormkUml+fr6enp6uLj75pSooKDAwMBAIBFwH0Vz5+fm6urpGRkZcB9FcMplM\nKpUaGBhwHaRc1q5d+9VXX9na2sbExKjtbWV/HeE3dtkkEomJiQnXKbQM9tgBAECNFhgY2KdP\nn8uXL7du3ZrrLABVhf0WAABQozVo0ODMmTNcpwCoHthjBwAAAMATKHYAAAAAPIFiBwAANciT\nJ0+ePn3KdQoAVUGxAwCAmiIuLs7V1bVfv345OTlcZwFQCRQ7AACoES5duuTh4ZGWljZ+/Pha\ntWpxHQdAJXBVLAAA8F9kZGRgYGBBQcG2bduCg4O5jgOgKih2AADAc/v27Rs3bpyOjs6vv/7q\n5+fHdRwAFUKxAwAAPvvw4UNISIiJicnx48fd3d25jgOgWih2AADAZyKR6OTJkwzDtGvXjuss\nACqHYgcAADzXtm1briMAqAmuigUAAADgCRQ7AAAAAJ5AsQMAAP5IS0sbOXJkZmYm10EAuIFi\nBwAAPJGUlOTq6hoWFrZ161auswBwA8UOAAD44MGDB+7u7omJidOnT//666+5jgPADRQ7AADQ\netevX3d3d09JSVm4cOEPP/ygo4O/blBD4XYnAACg3U6ePDl06FCJRIKvCwNAsQMAAO1mYWFh\nZGS0e/duf39/rrMAcAzFDgAAtJuLi0tSUpJIJOI6CAD3cBYCAABoPbQ6ABaKHQAAAABPoNgB\nAIA2kUgkHz584DoFgIZCsQMAAK0hFosHDhzo6+ubn5/PdRYATYRiBwAA2uHt27ceHh6nT582\nMjKSy+VcxwHQRCh2AACgBZ49e+bm5nbz5s0RI0YcO3bM2NiY60QAmgjFDgC00iHs31gAACAA\nSURBVPv3tGAB9ehB7u60YAG9f891IFClhIQENze3R48eTZ06dd++fXp6elwnAtBQuI8dAGif\nDx/IxYX+/vufydhYOnSIbt0i3PKCl/Ly8jw9PVNSUpYsWbJgwQKu4wBoNBQ7ANA+S5b82+pY\njx/T4sW0Zg1HgUCVjIyMfvrpp9evX0+aNInrLACaDsUOALTPhQtKBs+fV3cMUJtBgwZxHQFA\nO+AcOwAAAACeQLEDAO3j4aFksGdPdccAANA0KHYAoH2+/ZaaN/9oxMGBFi7kKA1UK7lcvm3b\nNqlUynUQAK2Ec+wAQPuIRHTjBq1e/c/Jdj160P/9Hy6J5YOCgoIxY8YcPHjw+fPnS5cu5ToO\ngPZBsQMArWRqSkuWcB0CqpVYLB46dOipU6ecnZ1nzpzJdRwArYRiBwAA3Hv37l3//v2vXLnS\nq1evo0ePmpqacp0IQCvhHDsAAOBYampqjx49rly5Mnjw4BMnTqDVAVQaih0AAHDs8uXL9+/f\nnzJlSnh4uKGhIddxALQYDsUCAADH/P397ezsunbtynUQAK2HPXYAAMA9tDqAaoFiBwAAAMAT\nKHYAAAAAPIFiBwAAarVy5copU6ZwnQKAn1DsAABATRiGmTVrVkhIyG+//ZaWlsZ1HAAewlWx\nAACgDlKpdOLEiTt37mzSpMnp06etra25TgTAQyh2AACgcrm5uf7+/tHR0Z06dYqOjraysuI6\nEQA/odgBAIBqFRYW9u3b948//ujVq9exY8dEIhHXiQB4C8UOAABUS09P7/PPP7ewsDh48CC+\nWAJApVDsAABA5ebPny+Xy3V0cMUegGrhZwwAANQBrQ5ADfBjBgAAAMAT2noolmEYriNoHHab\nYMuUoWgTYSuVDZuoPLCJynD//v0WLVpgE30SftY+CdtHKYFAUOosbdxkDMNkZ2dznULjMAwj\nl8uFQiHXQTSXXC5nN1EZPxIgk8nwKSqbVCoVCATYSqUJDQ2dNWvWokWLpk2bxnUWzYVfR+Uh\nlUp1dbV1D5Tq6OjomJqaljZXK7eXQCAwNzfnOoXGkUqlubm5ZbzZkJeXJxaLjY2NDQwMuM6i\nubKzs0UiEU6HKkN6erpQKMRvIaU2btw4e/ZsMzOzLl26YBOVQSwW5+XliUQiFJcyZGZm4lNU\nUfjdDQAA1YBhmK+//nrWrFnW1tYxMTHdu3fnOhFATYR/KAAAQFXJZLJJkybt2LHD3t4+Jiam\nYcOGUqmU61AANRH22AEAQFXt379/x44d7du3v3LlioODA9dxAGou7LEDAICqGjVq1Nu3b4OD\ng83MzLjOAlCjodgBAEBVCQSCOXPmcJ0CAHAoFgAAAIAvUOwAAAAAeALFDgAAKuby5cu//PIL\n1ykAQAmcYwcAABUQFRUVEBAgk8k8PT0bNmzIdRwA+Aj22AEAQHmFhoYOGTJELpfv378frQ5A\nA6HYAQBAufz4449jxowxMDCIjIwcOnQo13EAQAkUOwAA+LSVK1fOmDHD0tLy0qVLnp6eXMcB\nAOVwjh0AAHwCwzAJCQmNGzeOiYlp3rw513EAoFQodgAA8AkCgWDbtm3v3r2ztrbmOgsAlAWH\nYgEA4NP09fXR6gA0H4odAAAAAE+g2AEAgCKJRMJ1BACoDBQ7AAD4yIMHD1q2bHnixAmugwBA\nhaHYAQDAv65everm5pacnPzgwQOuswBAheGqWAAA+Me5c+cGDx6cm5v7888/T5w4kes4AFBh\n2GMHAABERGFhYd7e3hKJJCwsDK0OQEthjx0AAFBkZOSoUaNMTEyOHj3au3dvruMAQCVhjx0A\nAFCfPn0GDRr0+++/o9UBaDXssQMAADIyMoqIiOA6BQBUFfbYAQAAAPAEih0AAAAAT6DYAQDU\nOOnp6VevXuU6BQBUPxQ7AICa5cWLF25ubl5eXo8fP+Y6CwBUMxQ7AIAa5OHDh66urg8fPgwK\nCmratCnXcQCgmqHYAQDUFDdv3nR3d3/+/PnXX3+9adMmHR38CQDgG9zuBACgRjh//ryvr29O\nTs66detmz57NdRwAUAkUOwAA/mMY5ttvv5VIJAcOHAgMDOQ6DgCoCoodAAD/CQSCo0eP3rt3\nr2fPnlxnAQAVwgkWAAA1gqWlJVodAO+h2AEAAADwBIodAAAAAE+g2AEA8I1YLB45cuSDBw+4\nDgIA6oaLJwAAeCUzM7N///5Xr14VCoV79+7lOg4AqBWKHQAAf6Smpnp7e9+9e9fX13fbtm1c\nxwEAdcOhWAAAnnj69Kmbm9vdu3e/+OKL8PBwQ0NDrhMBgLqh2AEA8EFcXFy3bt2ePHnyzTff\n7Nq1S1cXB2QAaiL85AMA8IGVlVWtWrXmzp371VdfcZ0FADiDYgcAwAe2trZ//fWXsbEx10EA\ngEs4FAsAwBNodQCAYgcAAADAEyh2AADah2GYN2/ecJ0CADQOih0AgJaRyWTjx4/v1q1bamoq\n11kAQLOg2AEAaBOJRBIQEPDLL78YGxszDMN1HADQLCh2AABaIysry9PT88iRIz169Lh8+bKt\nrS3XiaDCPnygr76i5s2NbW0tOnUSHj7MdSDgFxQ7AADt8Pr16549e8bGxg4cOPDkyZNmZmZc\nJ4IKYxjy96e1a+nVK0FhIcXHCwICaN8+rmMBj6DYQU3x+DEtXKg3ZYpoyRLdly+5TgNQcf7+\n/nFxcePGjTty5IiRkRHXcaAyoqIoJkZxcPZskkq5SAN8hBsUQ40QEUEjR5JEost+5jdsoMhI\n6tmT61gAFfHzzz9HRER89913AoGA6yxQSXFxSgYzMujZM2raVO1pgI9Q7ID/MjMpOJgkkn9H\nxGIaNYqePCEDA+5iAVRQ69atW7duzXUKqBITE+XjtWqpNwfwFw7FAv9dvEhZWYqDKSl04wYX\naQCgBvv8czI0VBzs3p2srblIA3yEYgf8l5dXsXEAABVxdKTVqz8asbGh3bu5CQO8hGIH/Nex\no5JBXV1q107tUQDK7ZdffsnIyOA6BVS/adMoLo6++qpwxIj8NWvkjx6RgwPXmYBHcI4d8J+j\nI82YQT/88NHgokVkZcVRIIAyMQwTEhKyatWq33777fjx41zHgerXrh01a1aQl5dnbm6uq4s9\nLFCdUOygRlizhho3pi1bmGfPqFkzmjVL8J//cJ0JQBmZTDZ58uTt27c3btx4zZo1XMcBAC2D\nYgc1gp4ezZ5Nkybli8VikUhkgKthQSNJJJKgoKDw8HAnJ6eYmJj69etznQgAtAyKHQCARsjJ\nyRkyZMiZM2e6dOly4sSJunXrcp0IALQPDu0DAGiEx48fX716tX///ufPn0erA4DKwR47AACN\n0KFDhz/++KNVq1a6uvjNDACVhF8fAACaom3btlxHAADthkOxAAAAADyBYgcAwA2GYbiOAAB8\ng2IHAMCBsLCwfv365efncx0EAHgFxQ4AQN02btwYFBR07dq1hIQErrMAAK+g2AEAqNXKlStn\nzZpVu3btmJiYDh06cB0HAHgFV8UCAKiJTCabOnXq1q1bGzVqFBMT06JFC64TAQDfoNgBAKjJ\n2LFj9+3bh68LAwDVQbEDAFCTgICAJ0+eHD9+3MLCgussAMBPKHYAAGrSv39/Hx8fgUDAdRAA\n4C1cPAEAoD5odQCgUih2AAAAADyBYgcAoBL37t0rLCzkOgUA1CwodgAA1S8mJqZr167jx4/n\nOggA1CwodgAA1ezo0aO+vr6FhYXe3t5cZwGAmgXFDgCgOm3evHno0KG6urq//fZbYGAg13EA\noGZBsQMAqDYrV66cOnWqubn5mTNn+vXrx3UcAKhxcB87AIDqcePGjfnz59vZ2Z0+fdrR0ZHr\nOABQE6HYAQBUDxcXlz179vTo0cPOzo7rLFopNZV27qTERGrUiMaMIXt7rgMBaCEUOwCAahMU\nFMR1BG0VG0s+PvThwz+Tq1bRgQPk68tpJgAtpI5id+XKlRUrVigM9u7de+bMmcVHjhw5snv3\n7qJJoVB49OhRNcQDAABuFRTQyJH/tjoiysujsWMpMZHq1OEuFoAWUkexa9Wq1aJFi4om5XL5\nhg0b2rVrp7BYWlpax44dBw4cyE7ii3cAAGqIW7foxQvFwXfv6MIFGjKEi0AAWksdxc7c3Lxj\nx45Fk6dPn27atKmHh4fCYmlpaY6OjsWXBADQWI8ePdq/f//333/PdRA+yM2t2DgAlEbdtzvJ\ny8s7dOjQlClTSs5KS0uzsbHJz8//UHx3PACA5rlz546bm9uSJUvOnTvHdRY+aNOGdJXtZ+jQ\nQe1RALScui+eOHz4sIuLi7W1tcI4wzBpaWlRUVHr169nGMbOzm769Oll3C8A38BYkkwmYxgG\nW6YMMpmM/S+2UhkYhpFKpTgXogyXL18ePXp0Tk7OqlWr3N3d8XEqSSaTyeXy8m+Z2rVp3jyd\nJUuExQcnTpQ3by7j69aVy+VEJJVKGYbhOotGw8+XUnp6eqXNEqjzI/XmzZsZM2Zs2rSpbt26\nCrMyMjImTpzo7e3t5+cnlUp37doVFxe3efNmMzOzkuuRy+WZmZlqiQwA8JHo6OgJEybIZLL1\n69cPGzaM6zj8IZPR3r2GW7caPXsmrF9fNnp0/qRJefr6XMcC0DxCobB27dqlzVVrsdu8eXNu\nbu5XX331ySUlEsmoUaMmTZrUq1evknMZhsnFmRclsP8+NjAw4DqI5pJKpYWFhfr6+kKh8NNL\n11QSiURfXx977JQKDQ2dNm2anp7e7t27fXx8uI6judg9dmXsVIDCwkKpVGpgYKCjg6+AKlV+\nfr6hoSHXKTSOjo6OkZFRaXPVdyi2oKDg0qVL5Wl1RGRgYGBpaZmVlaV0rkAgMDExqdZ0fMDu\n0seWKUNeXh7bfVF/yyCVSo2NjfHHRql3796JRKLQ0NDu3bvjZ60MEolEKpViE5VBLBZLpVIj\nIyNdpWcXAhERSSQSfIoqSn2/u2/cuEFEHUo5FfbWrVtTp059//49O5mbm/vmzZuGDRuqLR4A\nwCeFhIQ8ePCgc+fOXAcBAFBOff9QiIuLc3R0VDgEdu7cuYKCAm9v7zZt2uTk5Kxbt27QoEF6\nenoHDx60s7PDrU8AQNPY2Nikp6dznQIAQDn17bGLj49v0aKFwuCFCxfOnDlDRPr6+mvWrDEw\nMFi7du3q1astLS0XL16Mg0EAAAAA5afWiydApaRSaW5urqmpKddBNFdeXp5YLBaJRDjHrgzZ\n2dkikQj/rCKivLw8pWcop6en6+rqmpubqz+StsA5dp8kFovz8vLMzc1xjl0ZMjMz6+BL5SoI\nv7sBAJR4/fp1t27dVq1axXUQAIAKQLEDAFD0+PHj7t273717NzExkessAAAVgGIHAPCRe/fu\neXh4JCUlzZgxY+vWrVzHAQCoABQ7AIB/Xbx40dXVNTU1ddWqVRs3bsSNmgFAu+CcTQCAf9y/\nf9/Ly0sqle7evXv06NFcxwEAqDAUOwCAfzg5OU2YMMHT07N///5cZwEAqAwUOwCAf23cuJHr\nCAAAlYdz7AAAAAB4AsUOAAAAgCdQ7ACghsrLy4uOjuY6BQBAdUKxA4CaKDs728vLa8CAAezX\nVQMA8AMungCAGictLc3b2/vOnTsDBgxwdXXlOg4AQLXBHjsAqFmSk5Pd3Nzu3LkzatSoiIgI\nIyMjrhMBAFQbFDuoKSIjacgQfXd388BAvYsXuU4DHLl//76rq+vjx49nzJixe/duPT09rhMB\nAFQnFDuoEVatooED6dQpYUKC7vHjOh4etHs315mACz/++OOrV6/YrwvT0cEvQADgGwHDMFxn\ngOohlUpzc3NNTU25DqJxXrygZs2ooOCjwVq1KCWFsLVKys7OFolEfC09hYWF586d8/LyqspK\n0tPTdXV1zc3NqysV/0gkEqlUamJiwnUQzSUWi/Py8szNzXV1cbJ7qTIzM+vUqcN1Ci3Dz9/d\nAMVduaLY6ogoJ4du3uQiDXBKT0+viq0OAECTodgB/wkEFRsHAADQUih2wH+ffUYGBoqDpqbk\n4sJFGgAAAJVBsQP+q1+fVq1SHNyyhWrV4iINqItMJps8eXJUVBTXQQAA1AfnbEKNMGMGOTnR\n5s2yZ88YBwedWbN0unThOhOokkQiGTFixJEjR27fvu3j4yPAcXcAqBlQ7KCm6N2buncvEIvF\nIpHIoOShWeCRnJwcPz+/06dPd+7cOTo6Gq0OAGoOFDsA4JXMzEwfH58///yzT58+R48erYUj\n7gBQk+AcOwDgj5SUlG7duv35559BQUHR0dFodQBQ02CPHQDwh6WlZaNGjfr27YsvlgCAmgnF\nDgD4Q19f/7fffjM0NOQ6CAAAN/AvWgDgFbQ6AKjJUOwAAAAAeALFDgC02MuXL7mOAACgQVDs\nAEBbrVy5slWrVjdv3uQ6CACApkCxAwDtwzDM7NmzQ0JC6tSpIxKJuI4DAKApcFUsAGiZgoKC\n0aNH//rrry1btoyJibGzs+M6EQCApkCxAwBtIhaLhw4deurUKRcXl+joaAsLC64TAQBoEByK\nBQBt8uWXX546dapfv37nz59HqwMAUIA9dgCgTZYtW2Zubr5kyRJ9fX2uswAAaBwUOwDQJhYW\nFitXruQ6BQCAhsKhWAAAAACeQLEDAAAA4AkUOwDQXOHh4ffv3+c6BQCA1kCxAwAN9fPPPwcG\nBvr5+clkMq6zAABoB1w8AcA3b9/ShQv07h21b0+dO3OdprJWrlwZEhJSu3btnTt3CoVCruMA\nAGgHFDsAXjl8mCZMoKysfyYHDKBDh8jQkNNMFcQwzFdffbVu3bp69erFxMS0adOG60QAAFoD\nh2IB+OPxYxo79t9WR0SRkRQSwl2gipNKpePGjVu3bp2jo+Off/6JVgcAUCEodgD8sX8/icWK\ngzt2kFzORZpKyczMvHjxoouLy6VLlxo2bMh1HAAALYNDsQD88eaNkkGxmHJyyNRU7WkqxcrK\n6ty5c3Xr1hWJRFxnAQDQPih2APzRtKmSQWtrrWl1rMaNG3MdAQBAW+FQLAB/jB1LDRooDi5Y\nwEUUAADgAoodAH/UqUORkeTs/M+ksTEtW0ZTp3Ka6VPkWnQCIACAxkOxA+CV9u3pxg169ozi\n4igjg+bPJ4GA60ylu3jxYqdOnVJTU7kOAgDAEyh2ADzUsCG1a6fpt687cuRIv3797t+/f/36\nda6zAADwBIodAHBgz549gYGBQqHw6NGjvr6+XMcBUB+5nHbsoP79Dd3cao8cKYyL4zoQ8Auu\nigUAddu4cePs2bPNzMwiIyNdXV25jgOgVhMn0o4dRCQkoocP6dAhOneOevXiOhbwBfbYAYBa\nLV26dNasWfXq1YuNjUWrg5omNpZtdR8ZN44Yhos0wEcodgCgVoMHD3Z2dr548WLr1q25zgKg\nbrGxSgaTk+n5c7VHAZ7CoVgAUCsnJ6cbN25wnQKAGzql7E4pbRygovBRAgAAUBOl59K1aEF2\ndmqPAjyFYgc1iERCz54JCwu5zgEANVXnzjRr1kcjhoa0axdHaYCPUOygRsjKouBgsrAwcnau\nXbeuwezZlJvLdaaa4cmTJ+np6VynANAg69fT4cPk6yt1cZGOHy+Pj6du3bjOBDyCc+yA/xiG\nvviCjh//Z7KggDZsoJwc2r6d01g1QFxcnLe3d8OGDWNjY/X19bmOA6Aphg4lb29JXl6eubm5\nri72sEB1wucJ+O/mzX9bXZEdOyg5mYMwNcelS5c8PDzS0tL8/PzQ6gAA1APFDvjv0aOKjUPV\nRUZGenl55eTkbNu27f/+7/+4jgMAUFPgUCzwX926ysctLNSbo8bYt2/fuHHjdHR0fv31Vz8/\nP67jAADUINhjB/zXowc1bqw42LYtdejAQRjee/v27fTp042NjU+fPo1WBwCgZthjB/xnbEwH\nDpCfH7169c9IkyZ08CDuCKoSlpaWx44dq127drt27bjOAgBQ46DYQY3QtSs9fEgREQWJidJW\nrfSGDNEzNOQ6E395eHhwHQEAoIZCsYOaQiSiwECZWJwrEokMDLhOAwAAoAI4FgUAAADAEyh2\nAFB5aWlpU6dOzc/P5zoIAAAQ4VAsAFRaUlJS3759ExMTW7ZsOW3aNK7jAAAA9tgBQKU8ePDA\n3d09MTFx+vTpU6ZM4ToOAAAQodgBQCVcv37d3d09JSVl4cKFP/zwgw7uHAMAoBlwKBYAKubE\niRMBAQESiWTLli0TJkzgOg4AAPwLxQ4AKkwgEBw4cMDf35/rIAAA8BEUOwC+EYvp2jXKyqK2\nbalZs+pfv4+PT3JysgW+ahcAQPPgzBgAXomJoebNqXdv8vMjBwf6z3+osLD6nwWtDgBAM6HY\nAfDH8+cUGPjvV+IS0a5dtHgxd4EAAEC9UOwA+GPvXsrOVhz86SeSyyu/TolEUqiKnX5QEVlZ\ndOUKPXpEMhnXUQBAs6HYlUUqJYbhOgRAuRXfV1ckO5tyciq5QrFY7OvrO2bMGHlVuiFUAcPQ\nN9+QjQ199hk5OlK7dnT9OteZAECDodgp98cf9NlnZGJCIhH5+tLjx1wHAiiHhg2VDNatSyJR\nZdb29u1bDw+PmJiYrKwsiURSxWxQOevX03//S0Wb//598vWlN284zQQAGgzFTom4OPL0pCtX\nqKCAxGL67Tfy8KD0dK5jAXzKF1+QpaXi4Jw5JBBUeFWvXr3q1avXzZs3R4wYcfz4cSMjo2pJ\nCBW1YoXiyOvXtGsXF1EAQBug2CkREkJ5eR+NvHpFq1ZxlAag3Gxs6MgRcnD4Z1JPj778kr7+\nusLrSUhI6Nq1671796ZOnbpv3z49Pb3qzQnllJNDb98qGX/6VO1RAKB0WVm0aBF9/jn5+9Mv\nv1TptOaqw33slIiPVzIYF6f2HAAV5+pKDx7QX3/Ru3fUujVZWVV4De/fv+/Zs+fbt28XL178\n3XffqSAjlJeJCZma0vv3iuO2tlykAQBl3ryhTp3o5ct/JsPD6ehRioyszKGSaqGVxY5hGLFY\nrLr1Gxsbl9yXaWwszcnJV92TVp1cLpdKpTmVPk++BpBKpUSUn5/P+8s8i3baVeLjYGJiMm/e\nPIFAEBwcjI9TaWQymXo2zn/+o79hg37xkVq1GD+/vJwcjb6cRSaTMQyDz09pUlIE0dE6qanG\nbdoUDhiQr6uVf4rVQSs+RdOnG758+dFbeOIEbd0qCQpS1R8aHR0dY2Pj0uZq66dJpceG/PyY\nkgde/fwYDT8gJZPJ5HK5hofkFsMwUqlUKBTWhK0klVLl/loUFhZOnjxZwNU/NrVBfn6+QCBQ\nz6fo+++ZV6/khw79809NCwtm61apg4OQSKiGZ68K/DoqTXi4zsSJekV7J1q1YqKjC21scAsG\nJSQSieZ/in7/XckP4++/640dq6pnLPv3s4DB/TxKkEjI05NiY/8dCQ6m7du5C1Q+Uqk0NzfX\n1NSU6yCaKy8vTywWi0QiAwMDrrOoyps3NG8eHTtGHz5Q69b0/ffUv3/F1pCdnS0SiXR0cAJu\nqdLT03V1dc3NzdX2jPfv0507VKcOffYZmZmp7WkrTyKRSKVSExMTroNonOfPyclJcVf655/T\niRMcBdJsmZmZderU4TrFJ9SpQ+/eKQ76+VF4OBdptHePnUoZGNCFCxQRQVevkoEB9e1LPXty\nnQmgHCQS8vam27f/mbxzhwYMoKgo8vHhNBZUmZMTOTlxHQKqw/HjSk6QOHmSMjKobl0uAkGV\nde+upJe7unIRhYhwVWxpdHTI35/WraPly9HqQGvs3ftvqysyfXpZD5HL5QcOHMCeewD1yMxU\nMsgwSnb5gLbYsEHxXqGdOtGUKRylQbED4JMzZ5QMJiWVeglFQUHByJEjR4wYsXHjRpUGAwBW\ny5ZKBkUisrNTexSoJs2a0Z07NHo0OThQhw40fz6dP0/6+p9+oIrgUCwAf7x4oXxcqOw8e7FY\nPHTo0FOnTjk7O48cOVKlwQCANXgwde6s+L1wixcTf8/7rRGaNqU9e7gO8T/YYwfAH/XrKx+X\nShVH3r1717dv31OnTvXs2fP333+3LPmFFQCgAnp6dOwYBQYSe61n3bq0di3NmsV1LOAR7LED\n4A9PT4qIUBxs1Ejx/I/U1FQvL6/4+PhBgwYdOHDA0NBQbQkBoF49OniQ3r0Tp6RIHB1NdXEX\nO6hW2GMHwB9jxlDbtoqDJU+fO3r0aHx8/OTJkyMiItDqADihr082Nhp9l2nQUih2UCPI5bRj\nB7m6GrRqVcfDQ+/QIa4DqYahIZ06RSNHkkhEOjrUqhVFRJCvr+JiU6ZMOXXq1ObNm3GzOgAA\nnsEeYKgR5s+nlSuJ/ZfM27cUGEgpKTR7NtexVKBePQoNJYah/HwyMip1sX79+qkxFAAAqAn+\nvQ78l5jItrqPzJtHGRlcpFELgaCsVgcAAHyFYgf8d+OGkkGJhO7cUXsUAAAAVUKxA/4r7UaR\nNeTGUStXrly+fDnXKQAAQB1Q7ID/3N2pVi3FQSsrcnbmIo0aMQwza9askJCQzZs3Z2dncx0H\nAABUDsUO+M/SkjZv/mjEwIB27+b5WWhSqTQ4OHjjxo1NmjS5cOGCmZkZ14kAAEDlcFUs1Aij\nRlHbtrRlizQpSd6ihXDaNKGDA9eZVCk3N9ff3z86OrpTp07R0dFWVlZcJwIAAHVAsYOaol07\nWreuUCwWi0QiAwNlX57KF7m5uX379v3jjz969ep17NgxkcL3TgAAAH+h2AHwjbGxcadOnSws\nLA4cOGDE7+PNAADwMRS7UuXlUUICGRlRs2b/fFszgLZYt24dEQmFfN4xCQAAJeHiCeW2biVb\nW+rUiVq1oubN6dQprgMBVIRQKESrAwCogVDslIiMpEmTKCvrn8nkZBo6lBISOM0EAAAA8Cko\ndkqUvJmrWEwbNnARBapVZqbg3j1d/t3QLTk5mesIAACgEVDslHj6VMngkydqzwHV5+1b8ven\nBg0Me/Y0r1fPYNw4+vCB60zVZOfOnS1atDh48CDXQQAAgHsodkrY2CgZzkBuZAAAIABJREFU\ntLVVew6oJnI5jRxJ4eH/Tu7cSZMmcZqpmmzcuDE4ONjY2Lh+/fpcZwEAAO6h2Cmh9E/++PFq\nzwHV5No1OnNGcTAsjBITuUhTTRiGCQkJmTVrlrW19fnz593c3LhOBAAA3EOxU2LiRJox499J\nY2PatInwd1N7lXYYXXuLnUwmmzBhwsqVK+3t7S9dutS+fXuuEwEAgEbAfeyUEAho40aaNo2u\nXyd9fXJ1pXr1uM4EVVDa92kpPeauFTZt2rRjx4727dufPHnSRntfBgAAVDcUu1I5OBC/v060\n5nB3p+bN6e+/Pxrs3JnateMoUJVNnjz57du3X331lZmZGddZAABAg+BQLPCfoSFNnEgCwb8j\nOjo0a9ZHI9pFT09vyZIlaHUAAKAAxQ74LzOTliwhhvl3RC6nuXNJIuEuEwAAgAqg2AH/Xbr0\n7/eIFElJoZs3uUgDAACgMih2wH+5uRUb1zSXL1+OioriOgUAAGgBXDwB/Nehg5JBXV3tuHgi\nKioqICBAKBQmJSVZWFhwHQcAADQa9tgB/7VsSdOmKQ4uXFjqbVA0R2ho6JAhQ+Ry+a5du9Dq\nAADgk1DsoEZYt47WrKFmzRh9faZlS2bbNpo/n+tMn/Ljjz+OGTPGwMAgMjJy6NChXMcBAAAt\ngEOxUCPo6dGcOTRlSr5YLBaJRAYGBlwnKgvDMIsXL168eLG1tfXJkyc7KD2WDAAAUAKKHYDG\nKSwsvHTpkr29/enTp5s1a8Z1HAAA0BoodgAaR19f/9ixY2KxuB6+zA4AACoCxa4sWVmkr0/G\nxlzngJrH1NTU1NSU6xQAAKBlcPGEcidPUqtWVLs2iUTk7k5xcVwHAgAAAPgUFDslrl6lIUMo\nIYGISC6n2Fjy9KRXr7iOBfwlk8m4jgAAAHyAYqfEggWUn//RSHo6rVrFURrgu4SEBCcnpytX\nrnAdBAAAtB6KnRIPHigZvH9f7TmgBrh69aqrq+ujR4+uXbvGdRYAANB6KHZKmJsrGaxTR+05\ngO/OnTvXr1+/7OzsLVu2zJ49m+s4AACg9VDslBgxoryDAJUWFhbm7e0tkUjCwsImTpzIdRwA\nAOADFDsl5s2j/v0/Gpk7l3x9OUoDfPTrr7+OGjXK0NAwOjo6ICCA6zgAAMATuI+dErq6FBlJ\nZ87Qn3+Svj55elLHjlxnAn7p06ePm5vbmjVrnJ2duc4CWuDKFbp7l+rUIQ8PsrbmOg0AaDAU\nu1J5epKnJ9chgKfq1q174cIFrlOAFsjLo6FDKTr6n0lTU9q6lYYN4zQTAGgwHIoFANBcISH/\ntjoiev+exo2jR4+4CwQAmg3FDgBAQ8nltHOn4mBuLu3fz0UaANAGKHYAKpeenp7AfpMJQEXk\n5lJOjpLxN2/UHgUAtASKHYBqvXjxwt3dvU+fPqmpqVxnAS1TqxbZ2CgZd3BQexQA0BIodgAq\n9PDhQ1dX14SEhEGDBlnjakaouG+/VRxp0IDGjuUiCgBoAxQ7AFW5efOmu7v78+fPv/76602b\nNuno4McNKmzyZFq+nExM/pl0caETJ/BFOABQKvylAVCJ8+fP9+7dOz09fd26dStWrOA6Dmgr\ngYBCQigjg+Lj6cULun6d2rblOhMAaDDcxw6g+slksmnTpuXn54eFhQ3DPcegygwMqE0brkMA\ngDZAsQOofkKhMDIy8smTJ564yTUAAKhRJYtdYWFhdHS0rq6uh4eHSdHZHwDwP02aNGnSpAnX\nKQAAoGYp7zl2YrF43Lhxbdq0ISKGYby8vAYNGtS/f39nZ2fcxAEAAABAE5S32C1cuHDnzp0d\nOnQgorNnz/7++++zZ88+cuRIamrqsmXLVJkQAAAAAMqlvMUuIiJi4MCBe/fuJaLIyMg6deos\nX7588ODBAwcOjImJUWVCAE0nFovHjx//6tUrroMAAEBNV95i9/r1axcXF/b/L1261LNnTwMD\nAyJq27bty5cvVZUOQONlZmZ6enru2LEDu64BAIBz5S129evXj4+PJ6KHDx/evXu3d+/e7Pjf\nf/9tZWWlqnQA1Sc/n06cEO7YYRgToyOVVs86U1NTe/bsefXq1UGDBq1du7Z6VgoAAFBZ5b0q\n1s/Pb/369bNnzz537py+vr6vr+/79+83bNiwZ8+ewMBAlUYEqLq4OBoyhJKS9In0icjJiY4f\np6ZNq7TOp0+f9u3b98mTJ1988cX27dt1dXHzIAAA4Fh599gtWLDA29t748aNDx48WL16ta2t\n7ePHjxcuXGhvb//999+rNCJX5HJ6+pRSUrjOAVWWn08BAZSU9O/I/fs0fDjJ5ZVf5+3bt7t1\n6/bkyZNvvvlm165daHUAAKAJylvsRCLR8ePH3717l52dPWPGDCJq3Ljx+fPn4+LiGjdurMKA\nHDlwgOrXp6ZNqUEDcnKiy5e5DgRVcOkSPX6sOHjjBt29W/l1ikQigUCwevXqpUuXViUbVJpE\nQuPGkYMDOTjQuHEkkXAdCABAA1RsN4OOjs6VK1fS09N79uwpEol69OghEAhUlIxDZ8/SiBH/\nTj54QD4+dOcO4XazWurNG+XjaWmVX6eDg0NCQkLt2rUrvwqoArGY6ten7Ox/JhMTKSKCUlII\nt0sHgBquvHvsiGjr1q316tXr27fviBEjHj16dOzYsUaNGh0+fFh14biyeLHiyPv3hDPjtVdp\n59I5OFRptWh1HPLz+7fVsbKzafBgjtIAAGiM8ha7qKioSZMmubi4HDhwgB1xdna2tbUNDAzk\n333sSh62I6K//1Z7DqgmXbtSv36Kg0FBVb14Ajh05YqSwatX1Z4DAEDDlLfYrVy5sn379mfP\nnh02bBg70qJFi0uXLnXo0GH58uUqi8cNS0slg7ipi/YSCGjfPgr4f/buNCCqQn/j+G9mgAHZ\nXUpNxTJ30UTcEglzxcotFTM1NcvMLW+attyy7Jq7eSvNyqU098Ru7mkQWVlqbn9zTb1qJqko\nCjMss/xf4EXFwzAIM2c4fD+vnB+HmScaDs+ctfeNh3q9DBkic+cW4hnsdvu1a9dckQ13x2p1\ndggApYqzxW7//v3du3c3GAy3Dn18fOLi4vYX5RB0jzRokMJw4EB3x0AxqlBBVq6U8+czEhOv\nXriQ+emnEhjo7PdardbnnnsuOjr66tWrrsyIQggLUxhWq+b2HADgYZwtdqGhoRkZGXfOz58/\nH+j8X8gS4qWX5Jlnbj40GuW996R9e/UCoZiEhNjr17cEBRXiW8xmc7du3RYsWKDX67Ozs10W\nDYWzfLnob1976fXyv+NEAKD0crbYtWjRYsmSJXm2WJw8eXLFihXNmjVzQTA16fWyeLHs2SMf\nfSSffiqHDsmECWpnghquXr3asWPH9evXP/LIIwkJCRUUd9JDDY0ayfbtUq2a6PWi10vVqrJt\nmzRurHYsAFCbs5c7mTp1aqNGjRo3bvzcc8+JyLZt2xISEubPn28ymaZMmeLKhKqJiJCICLVD\nQD0XLlyIjY3dt29fly5dVqxY4efnp3Yi3CYmRv77X7HbRUS0eNklALgbzm6xq169+o4dO6pX\nr/7666+LyLvvvvv222+Hh4f/8MMPDz74oCsTAiqw2Ww5rW7w4MFr166l1XksnY5WBwA3FeIC\nxeHh4QkJCVeuXDl69KiPj0+NGjWCg4NdlwxQkV6vnz179rZt2yZNmqTJq3ADADSp0De4DA0N\nbdGihSuiAB4lJiYmJiZG7RQAABSCs8UuPDzcwVcPHjxYHGEAAABw95wtdtWrV7/1odVqPX36\n9OHDh/39/YcMGVL8uQAAAFBIzha7b7755s5hYmLi448/fvny5WKN5BFsNvnqK/nlF/HxkQ4d\nhD1ymrds2bLu3btzkgQAoERz9qxYRTExMS+99NLSpUs11u0yM6VNG+ndW2bOlPfekzZtZOhQ\ntTPBZex2+4QJE55++ukRI0aonQUAgCIpUrETkRo1auh0Oo1t53jnHUlKum3yySeyYoVKaeBK\nVqt16NChU6dOrV69+gSuQw0AKOGKVOysVmt8fHyVKlXKlClTXIE8wZo1zg5RomVmZvbp0+fT\nTz+tX7/+jh07atasqXYiAACKxNlj7J544ok8E7vdfuTIkT/++OMf//hHgd++du3axYsX5z40\nGAzx8fF3PuGyZcsSEhJsNltUVNQzzzxjMBicjFe8rl9XGF675vYccKW0tLQePXp8++23zZs3\n37BhQ7ly5dROBABAUTlb7M6dO3fn8J577nnqqafeeOONAr89OTk5IiKiS5cuOQ8Vr/i6atWq\njRs3jhgxwsvL68MPPxSRwYMHOxmveDVsKH/9lXfYqJEaUeAyu3fvzjn7Z9WqVRo7lgAAUGo5\nW+z27t1blJdJTk6uU6dORP73XrVarRs3bhwwYEDLli1FZPDgwfPmzevbt6+vr29RXvfuTJki\nSUliNt+cVK4sr7zi/iBwoZiYmISEhObNm3t5Ffoy3QAAeKainjzhpOTk5IoVK2ZkZFxX3M0p\ncu7cuStXrjRp0iTnYZMmTUwm08mTJ90TL4+HHpKtW6VlS/H2Fj8/efxxSUiQChVUyQIXatWq\nFa0OAKAljv6q5Ww8c8bPP//s4Kt2uz05OXn9+vWzZ8+22+1Vq1YdOXJknTp1bl0mJSVFp9OV\nLVs252FAQIDRaLxy5Up+T3jNxYe8hYfLpk2SlSUGg+Qc6Zea6tIXLAZ2u91qtaZ6flD1WK1W\nETGZTBkZGWpn8VwWi+XatWvcIdcxftccs9lsdrvdYrGoHcRz5ayO0tLS+F1zwGaz8Yt2J71e\nHxgYmN9XHRW74tqYkZKSotfr69at+8Ybb1gslkWLFk2aNGnu3LnBwcG5y1y/ft1oNOr1N7cg\n+vn55dfe7HZ7dnZ2sWTLz7lz+unTy+zZ4+3jY4+JyX7pJVNQkN2lr1hcXP2T0QCr1ZqzSkV+\n+HtcIDeshTTAZrOpHcHT8btWIH7R7uT41FJH1e2HH34olgTlypVbc8vFQkaNGtW/f/89e/Y8\n+uijuUN/f//MzEy73Z772cVsNgcEBCg+oV6vL1++fLFkU3T2rLRtKykpNx4ePOiVkOD366/i\n4UfYWywWk8kUFBSkdhBPtGzZss2bN8+bN89sNgcGBhqNRrUTea7U1NTAwMBbP2Uhj0uXLnl5\neYWEhKgdxHNlZmZaLBZ/f3+1g3iu9PR0s9kcEhLCASEOpKSk5O7Kg5OKuu7evn17p06dCvUt\nRqOxQoUKV69evXUYGhpqt9tzh2azOTMzMzQ0tIjx7s64cTdbXY7/+z+ZNUuVLCgGc+bM6dev\n39dff338+HG1swAA4EKF+KCwatWqbdu2mW89WVTkp59+SktLc/yNe/bsWbhw4XvvvZezMclk\nMv3999/VqlW7dZmwsLDg4OC9e/fmbMbbt2+fn5+fWheM/fFHheGOHW7PgeIwderUCRMmlC1b\ndsOGDbVr105PT1c7EQAAruJssfvkk0+GDh0aFBSUs78vLCzMarX++eef995778yZMx1/b3h4\neFpa2qxZs7p16+bt7b1ixYqqVavmXPpk+/btWVlZsbGxBoOhc+fOS5curVKlil6vX7RoUYcO\nHdTaX5aVpTDkaPsSx2q1Dh8+fP78+WFhYVu2bKldu3aejyUAAGiMs8Vu7ty5jRo1+vXXXy9e\nvFitWrXvv/8+LCxsx44d3bt3j4mJcfy9Pj4+M2bM+Oyzz2bOnGkwGCIiIl555ZWcI3gSExPT\n09NjY2NFpE+fPtnZ2dOmTbPZbK1atRo0aFDR/tOKGYdvljh9+/ZdtWpVgwYNNm/efN9996kd\nB8UvKUl++EHsdomOluhotdMAgAdwttj98ccfw4cP9/Hxue+++x566KHdu3eHhYVFRUX16NFj\n/PjxX375peNvr1ChwquvvnrnfNKkSbn/1ul0AwYMGDBggPPpXcSudP4rR9uXOG3btj19+vSG\nDRtceqoNVGG3y8CB8sUXNyf9+8vnnwsXjgBQyjl78oSfn1/uieuNGzfe8b8jzpo3b75Dc0ef\nVa6sMGSLT4nz/PPP//jjj7Q6Tfr009tanYgsWSLz56uUBgA8hrPFrm7dulu2bMnKyhKRRo0a\nff311znzo0ePuvpawe73wgsKw+eec3sOFBnXEdCq5csVhsuWuT0HAHgYZ4vd+PHjDxw4ULNm\nzfT09KioqNOnT7/wwgvvv//+ggULnL9BRUkxdKiMGnXzYZky8tFH0rq1eoEA3O72yyU5GgJA\nqeJssevcufPixYvr1Kljt9sbN2789ttvL1y4cMyYMUFBQbM0d4U3nU7at5fq1W/8u25dadFC\n5Ugo0IkTJ+yKR0dCi+rVUxjWr+/2HADgYXQO/hY+/fTTvXr16tSpk6+v751fvX79+pkzZ2rV\nquXt7e3KhCr48UeJirptUq6c7NsnVaqoFMg5pfnOE1u3bu3Ro8fIkSPfe++9/JYxm+Wbb7L+\n+CO7bl2fxx7T3tu22JSIO08cOyZNmsit19AMCJBdu+T2e1C7CneeKBB3nigQd55wBneeuAuO\n1t3Lli3r3r17hQoV+vbtGx8fn+caYIGBgfXr19fkn8d//jPv5PJlmT5djShwQnx8fNeuXbOz\nsx966KH8ltm7V+rVk7g4n9de8+/e3btRIzlxwp0ZUcxq1ZItW6RZM9HrRaeTpk1lyxY3tToA\n8GSOit2RI0emTJnSoEGDFStW9OjR45577nnqqafWrl2r+au8Hj6sMPz9d7fngBPmzp3bs2dP\ng8Hw9ddfx8XFKS6TkSG9e8vp0zcnhw/LU08JNygv0R5+WH75Ra5dk2vX5Ndf5eGH1Q4EAB7A\nUbGrXbv2+PHjf/755z///PPjjz+Oiopau3btk08+WaFChbi4uDVr1phMJrcFdSfFW9SWK+f2\nHCjI1KlThw8fHhIS8u233zq4Z/H33ytsn9u9W/btc208uIG/vwQEqB0CADyGU4fRVKpUaejQ\noZs2bbp06dLKlSu7dOmyZcuWXr16VahQoXfv3qtXr3Z1Sjfr109h+PTTbs8Bh7Zt2zZhwoSq\nVavu2LHD8anZFy8qz//+2yXBAABQi6OTJxzIzs7+7rvvJk+enJSUJCIaOxvRYpFevWTdupuT\nV1+VyZPVC+ScUnjyxOzZs3v27Fm1alXHi+3cKYrF748/5IEHXBKsRCsRJ0+oi5MnCsTJEwXi\n5AlncPLEXbib99PBgwdXr169evXqI0eOiEh9zV1jwMtL4uMlIUF27hQfH2nfXho2VDsTlIwZ\nM8aZxZo3l06dZPPm24b9+9PqAABaU4hit2/fvjVr1qxevfrYsWMi8uCDD77xxht9+vTRXrHL\n0aaNtGmjdggUB51OliyRkSNl5Uqx20WvlyFDZOZMtWMBAFDcCi52v/32W872uT/++ENEqlWr\nNm7cuD59+kRERLg+HlA8ypeX5cvl/fczjh3LqFfPr1w5o9qJAAAofo6K3fjx41evXn3q1CkR\nqVSp0qhRo+Li4lq2bKnT6dwVT01JSbJzp/j6Sps2Eh6udppS7+jRo9u2bRs+fHhRnsTPzx4U\nZFO63jYAAFrgqNhNmzatfPnyQ4cOjYuLe+SRR0rPwdRWq/TpI2vW3Jy88YZMmqReoFJv9+7d\nnTt3vnjxYvPmzSMjI+/iGVJT5ZVXZNEiv+xsP19fGTFC3nlH/PyKPSkAAGpyVOw2bdrUrl27\nUnjCzvTpt7U6EXn3XWnRQh57TKVApVtiYmLXrl2vX78+c+bMu2t1IjJokMTH3/h3RobMmCHX\nr8vHHxdbSAAAPIGjjXCdOnUqha1ORJYscXYIV/v6669jY2NNJtOCBQv+8Y9/3N2T7N59s9Xl\nmj9f/vvfosYDAMCjlMbeVqCUFIXh5ctuz1Hqff7550OGDPHx8fnPf/4TGxt7189z5Ei+87Cw\nu35WwE2uXpUPP5R9+6RsWenalV0HAByh2CmoXVsuXMg7rFdPjSil265du4KCgtavX+/4xhIF\nyu/yltwmDp7vzBlp1kySk288/PRTGTVK5sxRNRMAD3aXd57Qth9+kOjo2yZly8q+fVLQDQ5U\npr07T9hstjNnzlSvXr2Iz5OeLvXqyZkztw0bNJB9+8RgKOJzaxB3niiQO+880bmzbNqUd7ht\nm7Rt64YXv3vceaJA3HnCGdx54i6w7lbQurWsW3fztgTNmsnmzZ7e6jRJr9cXvdWJiL+/LF8u\n9957cxIWJitW0Org6bKzZetWhfmGDW6PAqCEcPRBITU11clnCQ4OLo4wHqRrV2neXBITpUwZ\niYkRDW0FK6UefliOHpXVq7NOnLDUq+fdq5c31zqB57NYxGpVmGdmuj0KgBLCUbFzfkeD9vbn\nvvOOTJ58Y+1Ztqx88IH07at2Jq3Lzs729vZ23fMHB8vTT1vT002BgYFGbjyBksDPT+rXl0OH\n8s6bN1cjDYCSwFGxmzFjRu6/7Xb73LlzT5061bZt28aNGwcEBBw6dCg+Pr5FixYTJkxwfU63\nWrJE3nrr5sOUFHn2WalVS+72Gmoo2IULF2JjY0ePHj1w4EC1swAepHJlhWJXu7YaUQCUBI6K\n3csvv5z7748++ig5Ofn777+PvuW0gv3797du3frkyZMuDKiGO884y8iQjz6SRYvUSFMKHD9+\nvGPHjqdOndq1axfFDshltUpSksJ87Vo22gFQ5uzJEwsXLhwwYED07SeLNmrUaODAgYsXLy7+\nXKo6d05hePas23OUDv/3f/8XExNz6tSp8ePHf/jhh2rHATxIRoby4XTXrrk9CoASwtlid/z4\n8XJKV/0KCQk5ceJEsUZSn+IJsFzJ1hW+//77qKiov/76a9q0aVOmTNHpdGonAjyIv//N0/Nv\n1bCh26MAKCGcLXYNGjRYu3Ztenr6rUOTybRmzZqGmlvH3HnnqpzbxqN47dq1q1OnTunp6YsX\nLx43bpzacQBPNHNm3kl4uHDAAoD8OFvsRo8effjw4ejo6Pj4+NOnT58+fTo+Pj46Ovrw4cMj\nR450aUT3i4mRgIDbJpUrc+eJ4te4ceMuXbrEx8cPGDBA7SyAh+rWTVatkjp1RKeTMmXkqadk\n0ybhYj0A8uPsBa/j4uIuXLjw5ptv9ujRI3cYEhIyZ86c3r17uyabasaNk7S02yYnT8qcOfLK\nKyoF0igvL6+VK1eqnQLwdL16Sa9eYjKJr69wQxAAjhXiTiajR4/u169fYmLi8ePHvby8atSo\n0aZNG/fcVMfNFE9DS0yk2AFQTZkyaicAUBIU7hZ1Pj4+QUFBYWFhbdq0CQwMLKPRNY3iEfx8\nUEaJYLfLl1/K119LSoo89JCMHSuVKqmdCQDgLoVoK/Pnz69UqVKHDh369u179OjRdevWhYWF\nrV692nXh1PLoowpDD7/ltuczm80//vij2im0b+BA6d9f1qyR776TWbOkbl05dkztTAAAd3G2\n2K1fv/6FF15o2rTp8uXLcyaRkZGVK1eOi4vbsmWLy+KpY9o0ue++2yYtW3JWbJGkpqZ26tSp\nXbt2u3btUjuLlm3cKF98cdskNVWef16lNAAAt3O22E2dOvWhhx7atm1bnz59cia1a9dOSkpq\n3Ljxe++957J46qhQQQ4ckPHj5ZFHpH17mT5dEhLElXcx1bjk5OQ2bdokJSW1b9++QYMGasfR\nsm3bFIZJSdwzHgBKC2ePsdu/f//YsWMNBsOtQx8fn7i4OO0VOxEpW1amTFE7hCacPn26Q4cO\nx48f79+//4IFC7wpyK5ksSgM7Xax2dweBcXq0iX5/XcJCZF69cSrcIdGAyhdnN1iFxoampGR\ncef8/PnzgYGBxRoJ2nHo0KGoqKjjx4+PGjVq8eLFtDpXi4pSGEZGctmzEsxmk3HjpHJleeQR\nadRIGjSQn35SOxMAD+ZssWvRosWSJUuuXr166/DkyZMrVqxo1qyZC4JBC958883z589PnTp1\nzpw5es4rdr1evSQ29raJr698/LFKaVAcpk+XGTMkO/vGw6NHpVs3uXBB1UwAPJhh4sSJzizX\ntGnTOXPmfPHFFyaT6bvvvqtSpUpSUtKLL75oMplWr15dtmxZF+dEwWw2W3Z2ttFoVDvITZ07\nd46MjBwyZIjaQW6wWCw5PyIvje7N0umkVy8JCpK0NAkIkPbt5csvpVGjwj1JZmam0Wjkvr0O\nmEwmvV7v6+vr6hey26V7dzGZ8ry6lCsnrVu7+sWLxGq12mw2Hx8ftYN4ruzsbIvF4uvry4de\nB8xmsx97HApJZ7fbnVz04MGDo0aNSkxMzJ20b99++vTpjQr7dwOuYbFYTCZTUFCQ2kE8l9ls\nTk9PDwwM9Kj662lSU1MDAwP5Y+PApUuXvLy83HB59rQ0UTzU5fnnZf58V794kWRmZlosFn9/\nf7WDeK709HSz2RwSEqLVz5nFIiUlhS1HhVWI91N4eHhCQsKVK1eOHj3q4+NTo0aN4OBg1yUD\ngFLO31+CgyU1Ne+8ShU10gAoCZz9UN6/f/8jR46ISGhoaIsWLSIiInJa3Q8//DCCK7yhJLDb\n5cgR3Xff+Zw8yU5GlAw6nQwfnncYFCT9+6uRBkBJUECxS0tLu3z58uXLl5cuXXrs2LHLt7t4\n8eLmzZsXLVrknqzwZFardfTo0b/++qvaQZSdOSMxMRIR4RsXF1Svns/jj8ulS2pnApwwcaIM\nGHDzYcWKsmKFVK+uWh4AHq6AY+wGDRq0ePFix0/Rpk2b7777rjhD4a6oeIxdZmZm3759165d\nGxMTk5CQ4P4AjlmtEhUlO3feNnzsMVm/XqVALnbokKxfL1euyEMPSc+ehb7sGcfYFchtx9jl\nOnZM9u6VcuWkRQsJCHDby949jrErEMfYOYNj7O5CAe+nuLi4nFsFjB07dtiwYTVq1MizgLe3\nd9euXV2VDiVBWlrak08+uXXr1mbNmq1Zs0btOAp+/DFvqxORDRvk8GGpW1eNQK70/vsyfrxk\nZd14OGWKJCRIaKiqmVBktWpJrVpqhwBQEhRQ7Dp16tSpUycRWb9Fn0ekAAAgAElEQVR+/dCh\nQzkBFnmkpKQ89thjO3fubNeuXXx8fIBHbkz473/znWus2P32m4wZc9tk/34ZOVKWLlUpEADA\nvZzd25KQkFCtWrU5c+Z8//33OZOPP/54xowZqXeer4VS48yZMy1btty5c2e/fv02btzoma1O\n8j+FUHunFipuMF29WvlWYwAA7XG22F2+fDkiIuKll176/fffcyZHjhwZN25co0aNzp4967J4\n8GjBwcFlypQZMWLE559/7sm3C4uKksaN8w7btpUGDdRI40qKn7OyssRsdnsUAIAanC1248aN\nu3LlyrZt24YNG5Yzef/993fs2HHt2rXXXnvNZfHg0YKDg5OSkj744AMPP9De21tWrZLIyJuT\nRx6RJUvUC+Qy9esrDKtXV77ILQBAe5z9e5yUlPTcc8+1bdv21mGrVq2GDh2au3MWpVBgCakM\nDz4ov/wiP/yQuXDh9V9+yUpMlEqV1M7kAoMGKXS76dPViAIAUIOzxe7KlSuKR1D5+/unpaUV\nayTAJfR6adLE9sQTmY0aOXsbvRLHz082bZJevSTnlmkPPijLlknPnmrHAgC4i7PFrkmTJmvW\nrDHdfjPqjIyMNWvWREREuCAYPNHly5fVjoACVK0qq1ZJWppcuybHj8tTT6kdCADgRs4Wu7ff\nfvvo0aMtW7ZcsGDBzp07d+/evXTp0qioqIMHD77++usujQgPMXXq1Hr16h07dkztIHfJbJb/\n/MfwySd+Gzfqs7PVTuNiXl4cVwcApZGzF7xu2bJlfHz8mDFjhgwZkju87777vvzyyzZt2rgm\nGzyFzWYbMWLEvHnzwsLCHN+qxGPt3i0dO0pKio+Ij4hUriyJiVKzptqxAAAoVgXcUiwPi8Xy\n22+/nThxIisr68EHH2zSpImfn5/rwqFQXHRLsaysrAEDBqxcubJu3bpbtmypWrVq8T6/G5jN\nUrmyXL162/C+++TMGfHs03nVwS3FCuT+W4qVONxSrEDcUswZ3FLsLhTu/eTl5dWsWbNmzZq5\nKI3n2L9fXn1Vdu4Uo1Hat5fJkzV4MVtnpKen9+zZc/PmzU2bNt24cWP58uXVTnQ3li/P2+pE\n5M8/JTFRHn1UjUAAALiGo2LXvn17f3//devW5fzbwZLffvttMedS1ZEj0qqVpKffeLhkifzw\ng+zdK6Xw8/mQIUM2b97csWPHr776quR++N6zR3m+cyfFDgCgKY6K3dWrVy3/uxVRqbqmybhx\nN1tdjtOnZfp0+de/VAqknsmTJ5cvX37mzJk+Pj5qZ7l7+e09fvBB9+YAAMDFCneMXSlx331y\n/nzeYYcOsmWLGmmc5qJj7DTg4kWpWFFsttuGPj6Smiq+vipl8mAcY1cgjrErEMfYFYhj7JzB\nMXZ3wdH7KVXxxpNKgoODiyOMp1A8IaRMGbfnQDHJyBC9Pm+x0+slK4tiBwDQFEfFzvnPoxrb\n7Neli8yenXfYtasaUVAcfvpJ/ndMwU0ZGbJ7N8fYlWBms/z735KUJCLyyCMycqTyRzIAKFUc\nFbsZM2bk/ttut8+dO/fUqVNt27Zt3LhxQEDAoUOH4uPjW7RoMWHCBNfndKvevWXOnNs28Pj5\nyWOPqRfIXdatWxcZGVlFcycA63SFm8Pzmc3SooUcOHDj4caNsnSp7NzJlnUApZ2jYvfyyy/n\n/vujjz5KTk7+/vvvo6Ojc4f79+9v3br1yZMnXRhQDRMn5t1tZzbL9OkybZpKgdzi448/Hj58\neGRk5M6dO3XaqjytWonRKJmZtw2DgiQyUqVAKLJ//etmq8tx8KC8+65MnqxSIADwDM4eH71w\n4cIBAwbc2upEpFGjRgMHDly8eHHx51JVnj8YOfbtc3sON5o6deqwYcOCg4Nnz56tsVYnIvfd\nJ1Om5B3OnctNt0qwbdsUhtq67BIA3A1ni93x48fLlSt35zwkJOTEiRPFGkl9RqPCUKtH2dvt\n9rFjx06YMKFSpUqJiYkPP/yw2olc4qWXZOtW6d7d+tBDlt69bT/9JE8/rXYmFIHV6uwQAEoV\nZ8+ybtCgwdq1a1999dVbT183mUxr1qxp2LCha7KpxttbYajJE9ItFstzzz2/ePGicuVqvPLK\nlvvuq6F2Ihdq316iorLS09MDAwONiuUdJUdUlOzenXfYurUaUQDAkzi7xW706NGHDx+Ojo6O\nj48/ffr06dOn4+Pjo6OjDx8+PHLkSJdGdL+LFxWGycluz+F6Bw6cWbr0G5HIy5d/HjOmRq1a\nsmGD2pkAJ7z1loSF3TapVk0mTlQnDAB4Dmc3Q8XFxV24cOHNN9/s0aNH7jAkJGTOnDm9e/d2\nTTbVKN5lQ3OniIiITJ78gMXynUh1kUARSUmRfv3k0CGpXFntZIBDISGye7dMmiRJSWK3yyOP\nyBtvSGio2rEAQG2Fu/PE5cuXExMTjx8/7uXlVaNGjTZt2mjy2usGQ96zYkUkOFjhRvIepbB3\nnrh6VcqVU/gv/eADGTGimLN5CLPZzK7YAnHniQJx54kCceeJAnHnCWdw54m7ULj3k4+PT1BQ\nUFhYWJs2bQIDA8to9JpRd96lQLR454mUFIX/TMlnTzQAAPB8hfhQPn/+/EqVKnXo0KFv375H\njx5dt25dWFjY6tWrXRdOLYr3jG/a1O05XKxyZVH8OF2rltujAACA4uBssVu/fv0LL7zQtGnT\n5cuX50wiIyMrV64cFxe3ZcsWl8VTx3PP5Z3odDJqlBpRilViYmJ0dPTV/+1R9vWVO28a0rCh\nPPmku4MBAIBi4Wyxmzp16kMPPbRt27Y+ffrkTGrXrp2UlNS4ceP33nvPZfHUkZCQd2K3y8aN\nakQpPmvXru3UqdPOnTt/+eWX3OGrr8qbb968w2anTvL115q9Yh8AAJrnbLHbv39/9+7dDQbD\nrUMfH5+4uLj9+/e7IJiaDh50dlhSfPHFF3FxcQaDYd26dR07dsydGwzy9tuSmiq//y4pKbJp\nk1Svrl5KAABQNM4Wu9DQ0IyMjDvn58+fD9TcjZmCg50dlghz5swZOHBgQEDAli1bOnfufOcC\n3t5Sty6XigAAoMRztti1aNFiyZIlV2+/4MfJkydXrFjRrFkzFwRTk+KF+eLi3J6jOLz55psv\nvfRSpUqVkpKSoqKi1I4DAABcqBDH2F27dq1x48aTJ08WkW3btk2cOLFVq1Ymk2nKnfdXL+He\neEPatbttMmKE9OypUpqiiY2NbdCgwffffx8eHq52FgAA4FqFuEDxwYMHR40alZiYmDtp3779\n9OnTGzVq5JJoqrLbZf16+ekn8fWV9u3l4YfVDuSE/C5QbLPZuNhsDi5Q7AwuUFwgLlBcIC5Q\nXCAuUOwMLlB8FwrxfgoPD09ISLhy5crRo0d9fHxq1KgRXHKPOyuITidPPCFPPKF2juLAX2gA\nAEoJp/7kX7169cEHH1ywYIGIhIaGtmjRIiIiQsOtDgAAoCRyqtiFhITUqFFjx44drk7jUY4f\nl+XL5auv5MIFtaM47cyZMyaTSe0UAABAHc7uiv3www+7d+/+2WefDRo0KM/V7FTh/KGBd/Xk\n8vLLuvffv/HQ31+mTbMPG+a6Fywe+/fvf+yxxyIjI9etW+cJ/488UM7bxm63u/T9owH8iJzB\nj8iB3N81tYN4On7XCsTPR5FOp8v3S07+yHr16pWamvrtt98GBARUq1atTJkyt351165dRc1Y\nGHa7Pc+FV4rXokXGl18uk2e4ceP1Fi0srnvRIvrpp5/69u17/fr1t956a5QGbn/mGjabzW63\n6/V6B78S4GybAlmtVhHh45MDOX2FN5IDdrvdZrPxLnLMarXyI7qTXq93cDics1vsLl26JCIx\nMTHFkqmIdDpdqCsvp/vFFwrDZcsCY2Nd95pFsn79+t69e2dlZc2ZM2fkyJFqx/FcOWfF+vv7\nc1asA5wVWyDOii0QZ8UWKOes2MDAQM6KdSAlJcWlf+41ydn3U8Kd90/VLsWD6v76y+05nLN0\n6dLBgwfr9fply5Z16tRJ7TgAAEA1BRe7tLS0o0ePZmZm1q1bt5QU5/vvl+TkvMMaNdSIUpD/\n/ve/zz77bJkyZf7zn/88/PDDnDkBAEBp5mhvi91uf+utt8qVKxcZGdmqVasKFSq89tprFovn\nHmdWXF59Ne/E319Gj1YjSkHCwsK+/PLLxMTE6OhotbMAAACVGSZOnJjf1xYsWDB27Nh77713\n4MCBLVq0OHPmzKZNm0JCQlq2bOnGhCqoXVvuvVd+/FEyMkREwsLk88899+YT9erVq1ixoojY\nbLbs7GyOHnPAYrHk/Ig4qMWBzMxMo9HI+SUOmEwmvV7v6+urdhDPZbVabTabj4+P2kE8V3Z2\ntsVi8fX15XhWB8xms5+fn9opShhHZ8U2adLk3Llzhw4dKl++vIikpKTUq1cvJCTkyJEjbkyo\nGpNJDh8Wo1Fq1xZvb7XTOCG/W4ohF7cUcwYnTxSIkycKxMkTBeKWYs7glmJ3wdG6+9ixY127\nds1pdSJStmzZ7t27//HHH24Jpr4yZaRJE2nQoGS0OgAAAEfFLi0t7Z577rl1cu+995aGY+w8\nVnJy8htvvGGz2dQOAgAAPFEBW4DzHGfDYTcqOnXqVMeOHY8fP16rVq0BAwaoHQcAAHgcDqMp\nGX7//ffo6Ojjx4+PHDmyX79+ascBAACeqIAtdgcPHvzyyy9zHx44cEBEbp3kePrpp4s9GXL9\n+uuvnTt3TklJeeuttxycxQwAAEo5R2fFOr/jVXv36E1JkUmTJClJjEbp2lVeeknUOo1y27Zt\n3bt3N5vNc+fOff755x0syVmxBeKsWGdwVmyBOCu2QJwVWyDOinUGZ8XeBUfvpxUrVrgth0e5\neFFq1pTU1BsPf/5ZFi+WAwfUOT32/PnzFotl+fLlvXr1UuHlAQBAyeGo2MXFxbkth0fp0+dm\nq8tx5Ii8/rpMm6ZCmAEDBrRp06Zq1aoqvDYAAChR2NuiYMcOheHKlW7P8T+0OgAA4AyKnQKr\nVWGYnu72HAAAAIVBsVOgePpBWJg7XjorK0t7Z6IAAAD3oNgpeO01heGsWS5/3fT09C5dukyY\nMMHlrwQAALSIs6wVjB0rJ0/KJ5/c2Cfr4yMzZ8ojj7j2RS9evNi5c+fdu3frdLrs7Gxv7lAL\nAAAKiWKnbO5cGTdOdu0So1FatpTbb5lb/M6cOdOhQ4ejR4/27dt38eLFtDoAAHAXKHb5uv9+\nuf9+d7zQ4cOHO3bsePbs2eHDh//73//mwrAAAODu0CFUduHChdatW589e/add9758MMPaXUA\nAOCuscVOZRUrVhw2bFjlypWHDRumdhYAAFCyUezUN2nSJLUjaJ/dLitWyMcfG0+f9qlZUzdm\njDz2mNqZAAAobuz4Q6nwzjvSt68kJenPnDFs365//HGZN0/tTAAAFDeKHbTv1CmZODHv8OWX\n5coVFcIAhZWaKpMnS1ycvPCCbN6sdhoAno1i51Y2m20zK2a3++UXhaHZLL/95vYoQCGdPSt1\n68rrr8uqVTJ/vsTGypgxamcC4MEodu6TlZX19NNPx8bGLl26VO0spYtXPoeS5jcHPMcLL8hf\nf902ef99+e47ldIA8HgUOzdJT0/v1q3bihUrIiMjO3bsqHac0iUqSsqUyTssW1aaNlUjDeC0\n7GzZskVhvn6926MAKCEodu5w5cqVDh06bNq0qU2bNtu3b69QoYLaiUqXihXl/fdvmxiN8tln\nCm0P8CgWy40bG+aRmen2KABKCPZFudxff/3VqVOnAwcOdOvWbfny5b6+vmonKo2ee07Cw+Xj\nj61//GGrXVs/erQhPFztTEBB/PwkPFwOHsw7b95cjTQASgKKncvNnz//wIEDw4YN48YS6mrR\nQho1ykpPTw8MDDQaDWrHAZzy0UcSHX3bpHVrefppldIA8HgUO5d78803GzRo0LNnT7WDACh5\nWreWH3+Ut9+WvXslJER69JDXXxcDH0wA5INil69ffpH//Ef8/aVXL6lZ8+6fR6/X0+oA3LWH\nH1Y+hQIA7kSxU2CzSXS0/PjjjYdvvCHPPiuffqpqJgAAgIJwyJeCoUNvtjoRsdvls89k0SL1\nAgEAADiBYqdA8frBkyY59b2TJ09esmRJ8eYBAABwBrtiFSheIyrPxd/vZLfb//GPf7z//vth\nYWG9evXisiYAAMDNKHbOcnyhEqvV+vzzzy9cuPCBBx7YunUrrQ4AALgfxU6Bn5+YTHmH99+f\n7/KZmZlPPfVUfHx8kyZNNm7ceM8997g0HgAAgCKKnYKgIIViV7688sLXrl3r3Lnzjz/+2KZN\nm3Xr1gUFBbk6Hu5OerrExxtOnvSrU0ffrZv4+KgdCACA4kaxU2CzKQy98vlR+fv7V6xYsUuX\nLitWrPDz83NpMNy1XbukRw85d85HxEdEataUb76R2rXVjgUAQLGi2CkIC5O//847rF5deWGD\nwbB06VJvb28DF4P3VGaz9Okj587dnBw/Ln36yJ49BRw6CQBAycKfNQVjx+ad+PnJyJH5Lu/r\n60ur82Tffy8nT+Yd7tsne/eqkQYAAJeh2Cno3VumTxd//xsPK1aUpUulUSNVM6EILl1Snl+8\n6N4cAAC4GLtilY0dK0OGyL594ucn4eFSpszNL/3999+c91qy1KqlPOcYOwCAxrDFLl8hIRIT\nI82b39bqvvjii/vvv3/r1q3q5UKhNW0qjz+edzhokKNL2AAAUBJR7PJ17Jh8+aWsXn3znhNz\n5swZOHCgj48PZ7+WLDqdfP65DBhw41QJLy8ZMUI++EDtWAAAFDd2xSqw2+Wll+Tf/77xsEwZ\nmTbNfubMhGnTplWsWHHTpk0PPfSQqgFRaGXLyuefy6xZ5mPHMurVKxMcbFQ7EQAAxY9ip+Dj\nj2+2OhExmawjRrwg8tn999+/ZcuWmjVrqhcNRVKmjNSsaeV+bwAAraLYKZg/P89gsshnoaEP\n/fTTpooVK6oSCQAAoEAUOwUXLuQZjBH5u2HDdytWDFYlDwAAgDM4eULBAw/kGQSIfFCnDq0O\nAAB4NIqdgldfzTvx95fRo9WIAgAA4DSKnYInnpCPP5bQ0BsPq1eXNWukbl1VMwEAABSEY+zy\n2rFjh8FgGDq05TPPyO+/i6+v1KolXvycAACAx6Ow3Gb9+vW9e/cODAw8ceJEYGBgRITagQAA\nAJzGrtibli5d2qNHD5vN9tFHHwUGBqodBwAAoHAodjd88MEHzzzzjNFo/Oabb3r27Kl2HAAA\ngEKj2Indbp84ceKoUaMqVKiQlJTUvn17tRMBAADcDY6xk/T09LVr13K7MAAAUNJR7CQgIGDz\n5s0iUrlyZbWzAAAA3D2KnQiVDgAAaALFTllGhsybJzt3iq+vtGsnTz8teg5HBAAAnq00Fju7\n3a7T6RwscO2atGghhw/fePjFF7JqlXz9Nd0OAAB4tFJXVQ4fPhwZGXn06FEHy7z22s1Wl2P9\nevnsM9cGAwAAKKLSVex27twZFRX122+/bd++3cFiGzcqDDdscFUqAACAYlGKit327ds7dOiQ\nmpo6b968F1980cGSmZnODgEAADxHaSl2y5cvj42NzczMXLZs2QsvvOB44ebNFYYtWrgkGAAA\nQHEpFcXuiy++6Nevn6+v78aNG3v37l3g8tOnS1DQbZOaNWXsWFfFAwAAKBalotg9+uijDRs2\n3LJlS9u2bZ1ZvkYN+fVX6dVLqlSRGjVk2DDZsUMCAlwdEwAAoEhKxeVOqlSp8ttvvzm+xEke\nFy/K2bOSnCxeXnL2rFy9Kvfc47qAAAAAxaBUFDsRKVSr27tXOnQQs1lEJDtb1q+XPXtk/36p\nUMFV8QAAAIquVOyKLawJE260ulx//SXTpqmUBgAAwDkaLHaXLl36888/i/IMBw8qDPfvL8pT\nAgAAuJzWdsWePXu2Y8eOOp3up59+Cg4Ovrsn8fZWGPr4FCkYAACAq2lqi92RI0eioqIOHz4c\nExMTGBh418+jeE9Ym+3ugwEAALiBdord7t27o6Ojz5w5M378+I8++kiv2M6ck+cAuxzZ2Xef\nDQAAwA00UuwSEhLatm176dKlWbNmTZkypYjPFhqqMCxbtojPCgAA4FpaKHYZGRn9+/fPyMhY\ntmzZmDFjiv6Effs6OwQAAPAcWjh5wtfXd926dSkpKR06dCiWJ3z1VfnlF9mw4eZk7Fjp2rVY\nnhsAAMBVtFDsRCQyMrIYn83LS9avl2+/lZ9/FqNR2rWTJk2K8ekBAABcQiPFzhXat5f27dUO\nAQAA4DQ3FbusrKyFCxfu3bs3NTW1Zs2agwcPvv/++/Mss3bt2sWLF+c+NBgM8fHx7okHAACg\nAW4qdlOnTj116tTQoUNDQkJWrFgxceLEjz76KCAg4NZlkpOTIyIiunTpkvPQwd1dMzIysrKy\ngoKCXBsaAACgRHHHWbGXLl3atWvXiy++2Lx589q1a48fP95kMu3evTvPYsnJyXXq1In4n8aN\nG+f3hNevX//Xv/7l4tQAAAAljDuK3bVr1x588MHatWvnPDQajb6+vlevXs2zWHJycsWKFTMy\nMq5fv+74CUNCQt555x2XZP2fM2fk2Welfn2JiJDx4yU11aWvBgAAUAzcsSv2gQcemDVrVu7D\nXbt2paam1q9f/9Zl7HZ7cnLy+vXrZ8+ebbfbq1atOnLkyDp16ig+obe3t8VisVgsLgp89qxE\nRnqlpNx4uHevbNhg//lnq5+fi16weFitVrvd7rofiwbYbDYRsVqt/JQcyHkXFeXeLaUBv2uO\nWa1Wm83Gj8iB3NWR2kE8He+iO+l0OoPBkO9X7Xa726LY7fZvv/12/vz5HTt2fP7552/90uXL\nl4cOHRobG/vkk09aLJZFixbt27dv7ty5wcHBdz6PzWZLya1dLvDcc4Hr1hnzDF97zTRmjMl1\nLwoAAFAgg8EQqniPLBFxZ7FLTk6ePXv26dOnn3nmmdjYWMcLZ2Zm9u/f/4UXXnj00Ufv/Krd\nbjeZXNixatUqc/583lM32rWzrluX4boXLTqbzZadnW005q2kyGWxWLKzs318fBx81kFmZqaP\nj4+Ds5dgNpv1ej2/aw7kbLHz9vZWO4jnys7OtlgsRqORreMOZGRk+Pr6qp3C4+j1er/89yG6\n6azYY8eO/fOf/2zYsOH8+fMVN8LlYTQaK1SocOdxeDl0Op2/v39xZ7xJ8Ri/v/82uPRFi85i\nsdjtdg8PqS6z2ZzTffmT7IDFYilTpgx/bBzIKXb8rjmQmZlpsVj4ETmQnp5usVj8/Py8vLig\nbL4yMzN5FxWWO9bdVqv1vffei4mJee211/JrdXv27Bk+fPi1a9dyHppMpr///rtatWpuiOck\nN+6yBgAAuBvu+KCwd+/elJSURo0aHTp0KHdYuXLlsmXLbt++PSsrKzY2Njw8PC0tbdasWd26\ndfP29l6xYkXVqlUjIiLcEO9O/v4KG+0qVVIjCgAAgNPcUezOnTtnt9unTJly63Do0KGPPfZY\nYmJienp6bGysj4/PjBkzPvvss5kzZxoMhoiIiFdeeUWtnUF16siFC3mH4eFqRAEAAHCaW8+K\nLSkaNZIDB/IOBw+WBQvUSOM0i8ViMpm4IYcDZrM5PT09MDCQY+wcSE1NDQwM5Bg7By5duuTl\n5RUSEqJ2EM/FMXYFSk9PN5vNISEhHGPnQEpKStmyZdVOUcKw7lZw5IjCcNs2t+cAAAAoDIqd\nAptNYZiZ6fYcAAAAhUGxU6B45u4DD7g9BwAAQGFQ7BQMH64wfOUVt+cAAAAoDIqdgtGjJc+x\n9ZUqSZcuKqUBAABwDsVOQf/+eY+o++svefddldIAAAA4h2KnYOtWheHcuW7PAQAAUBgUOwUW\ni8Iwn/vWAgAAeAqKnQKdTmHo7e32HAAAAIVBsVNQrZrCsE0bt+cAAAAoDIqdgvnz806MRpk9\nW40oAAAATqPYKejYUZYskfLlbzx88EHZulVq1FA1EwAAQEG497Cyfv3kqafk5EkxGpX3zAIA\nAHgail2+DAapWVPtEAAAAE5jVywAAIBGUOwAAAA0gmIHAACgERQ7AAAAjaDYAQAAaATFDgAA\nQCModgAAABpBsQMAANAIih0AAIBGUOwAAAA0gmIHAACgERQ7R7KyxGpVOwQAAIBzKHbKfvhB\nWrQQf38JCJDHH5djx9QOBAAAUBAvtQN4or17pUMHycgQEbFYZMMG+e032b9fKlRQOxkAAED+\n2GKnYMKEG60u119/ydSpKqVBcbBaZd48adHCWKtWuagony+/FLtd7UwAABQ3ttgpOHhQYXjg\ngNtzoPhMmCAzZkjOJ5ndu6VfPzl/XsaNUzsWAADFii12CgIDFYZBQW7PgWJy/HhOq7vNG2/I\npUtqpAEAwGUodgp69VIY9u7t9hwoJrt3KwyzsmTvXrdHAQDAlSh2Cv75T4mJuW0ydCjFrgQz\nGpXnfn7uzQEAgItxjJ0Co1G++07WrpWdO8XHRzp2lOhotTOhCKKjJTBQrl+/bVixokRGqhQI\nAADXoNgp0+nkySflySfVzoHiUL68zJ8vAwdKVtaNia+vfP65+PqqGgsAgOJGsVN2/brMni0/\n/SRlyki7dvL88+LFj6oke+opadhQ5s+3nDxpq1vXMGyY4YEH1M4EAEBxo60oSEmRyEg5derG\nw/h4Wb1atm0Tg0HVWCia+vVl6tTs9PT0wMBAo5H/lwAADeLkCQXjx99sdTkSE2XuXJXSAAAA\nOIdip+DbbxWGW7e6PQcAAEBhUOwUWK3ODgEAADwHxU5Bq1YKw6got+cAAAAoDIqdgunTpWzZ\n2yYNGsiYMSqlAQAAcA7FTkHVqrJvnzz7rNStK40by/jxsmMHdykAAACejsudKKtaVT7+WP74\nQ3x8pHp10enUDgQAAFAQttgp++ILqVRJ6tSRBx6QOnUkIUHtQAAAAAWh2CnYvFmeeUYuXbrx\n8Ngx6dpVjh9XNRMAAEBBKHYK3n037+T6dZk1S40oAAAATs1p5TAAABbBSURBVKPYKThxwtkh\nAACA56DYKbjnHoXhvfe6PQcAAEBhUOwUDBmiMBw82O05AAAACoNip2DkSHnuuZsPfX1l5kx5\n9FH1AgEAADiB69gp0Onkk09k5EjZuVOMRnnkEQkLUzsTAABAQSh2+QoPl/BwtUMAAAA4jV2x\nAAAAGkGxAwAA0AiKHQAAgEZQ7AAAADSCYgcAAKARnBULaI3ZLLt3y5Ur0qgRV+oBgNKFLXaA\npmzbJrVrS3S0dO0q1avL0KFisaidCQDgLhQ7QDvOnpXeveXs2ZuTTz6RSZPUCwQAcC+KHaAd\nX3whV67kHf7732K3q5EGAOB2FDtAO/78U2F49apcv+72KAAANVDsAO2oWlVhGBoqgYFujwIA\nUAPFLl+//Sbz5smCBXLypNpRAOc884yUK5d3OGaM6HRqpAEAuB3FToHNJoMGSZMm8uKLMmSI\n1KsnU6eqnQlwQuXKsmaN3H//zcmIEfLaa+oFAgC4F9exUzBnjixefPNhZqZMmCAREdK+vWqR\nACfFxMjhw7J/v1y+LA0byn33qR0IAOBGFDsFCxcqDBctotihZDAapVkztUMAANTArlgFFy86\nOwQAAPAcFDsFNWsqDGvVcnsOAACAwqDYKXjrrbyToCB5+WU1ogAAADiNYqegXTv58kupWPHG\nw3r1ZP16eeABVTMBAAAUhGKnrG9f+fNPOXFCzp6VQ4ekdWu1A6ForFaZN09atDDWqlUuKsrn\nyy+5yxYAQIM4KzZfer3UqKF2CBSTCRNkxgzJ+SSze7f06yfnz8u4cWrHAgCgWLHFDtp3/HhO\nq7vNG2/IpUtqpAEAwGUodtC+3bsVhllZsnev26MAAOBKFDton9GoPPfzc28OAABcjGIH7YuO\nlsDAvMOKFSUyUo00AAC4DMUO2le+vMyfLz4+Nye+vvL55+Lrq14mAABcgLNiUSo89ZQ0bCjz\n51tOnrTVrWsYNszAhQkBANpDsUNpUb++TJ2anZ6eHhgYaDQa1I4DAEDxY1csAACARpTILXZ2\nu91kMqmdwuPYbDar1Zqenq52EM9lsVhEJDMzM+cfUGS1Wk0mk06nUzuIR7PZbPyuOWC1WvkR\nOZadnS0iZrNZr2cLS77sdjvvojvp9Xq//C/rUCKLnYgYDOxKy0un01mtVn4yDlitVhHR6/X8\nlBzQ6XQGg4FiVyDeRQ7Y7Xa73c6PyIGc1ZHBYKDYOca76E6O188lstjpdDpfTmi8g8VisVgs\n/GQcsNvtWVlZ3t7exvwubQeRzMxMo9HIHxsH0tLS9Ho9v2sOZGZmsqJ2LKfY+fj4eHmVyD/E\n7mEymXgXFRbrbgAAAI2g2AEAAGgExQ4AAEAjKHYAAAAaQbEDAADQCIodAACARlDsAAAANIJi\nBwAAoBEUOwAAAI2g2AEAAGgExQ4AAEAjKHYAAAAaQbEDAADQCIodAACARlDsAAAANIJiBwAA\noBEUOwAAAI2g2AEAAGgExQ4AAEAjKHYAAAAaQbEDAADQCIodAACARlDsAAAANIJiBwAAoBEU\nOwAAAI2g2AEAAGgExQ4AAEAjKHYAAAAaQbEDAADQCIodAACARlDsAAAANIJiBwAAoBEUOwAA\nAI2g2AEAAGgExQ4AAEAjKHYAAAAaQbEDAADQCC+1A3gou12++UZ27hRfX2nXTh5+WO1AAAAA\nBaHYKcjKkscek23bbjx86y0ZOVL+/W9VMwEAABSEXbEK/vWvm60uxwcfyJo1KqUBAABwDsVO\nwcqVzg4BAAA8B8VOQWqqs0MAAADPQbFT0KCBwjA83O05AAAACoNip+C998TX97bJvffKK6+o\nlAYAAMA5FDsFkZGyYYNERIheL97e0qGDfPed3Huv2rEAAAAc4nInyh59VPbskYwMMRjE21vt\nNAAAAE6g2DmSZ4csAACAJ2NXLAAAgEZQ7AAAADSCYgcAAKARFDsAAACNoNgBAABoBMUOAABA\nIyh2AAAAGkGxAwAA0AiKHQAAgEZQ7AAAADSCYgcAAKARFDsAAACNoNgBAABoBMUOAABAIyh2\nAAAAGkGxAwAA0AiKHQAAgEZQ7AAAADSCYgcAAKARFDsAAACNoNgBAABoBMUOAABAIyh2AAAA\nGkGxAwAA0AiKHQAAgEZQ7AAAADSCYgcAAKARFDsAAACNoNgBAABoBMUOAABAIyh2AAAAGkGx\nAwAA0AiKHQAAgEZQ7AAAADSCYgcAAKARXu55GbvdvmzZsoSEBJvNFhUV9cwzzxgMhrtYBgAA\nAPlxU7FbtWrVxo0bR4wY4eXl9eGHH4rI4MGD72IZAAAA5Mcdu2KtVuvGjRsHDBjQsmXLpk2b\nDh48eOvWrRkZGYVdBgAAAA64o9idO3fuypUrTZo0yXnYpEkTk8l08uTJwi4DAAAAB9yxKzYl\nJUWn05UtWzbnYUBAgNFovHLlSmGXyWW3269du+bSzCWR3W63Wq2pqalqB/FcVqtVREwmExuD\nHbBYLNeuXdPpdGoH8Wj8rjlms9nsdrvFYlE7iOfKWR2lpaXxu+aAzWbjF+1Oer0+MDAwv6+6\no9hdv37daDTq9Te3Dvr5+eVpZs4sk8tut2dnZ7sobUnHT6ZAVqs1Z5WK/PD3uECshZxhs9nU\njuDp+F0rEL9od3J8aqk7ip2/v39mZqbdbs/9XGI2mwMCAgq7TC69Xl+uXDmXZs7Kks8+k19+\n0RmN0q6dvVcv8fzPVBaLxWw2O2jxMJvNJpMpZ3uw2lk817Vr1wICAm79lIU8Ll++7OXlFRwc\nrHYQz5WZmWm1WsuUKaN2EM9lMpnMZnNwcLCXl5vOYiyJrly5EhoaqnaKEsYd76fQ0FC73X71\n6tWc/z1mszkzMzPP/ypnlrmVS7ddp6XJww/LwYM3Hn72mW7lSvnqK0/vdjk/E7bqO5D7I+Kn\n5Bg/ImfwI3KA1ZGT+F0rED+fwnLHh/KwsLDg4OC9e/fmPNy3b5+fn1/NmjULu4zbvP76zVaX\nIz5eFi5UJQsAAICz3LHFzmAwdO7ceenSpVWqVNHr9YsWLerQoUPOvrDt27dnZWXFxsY6WMb9\n1q9XGH7zjTz7rNujAAAAOM1Nu/b79OmTnZ09bdo0m83WqlWrQYMG5cwTExPT09NjY2MdLON+\nimdMcholAADwcDq73a52Bo/TrZt8/XXe4T//Ke+8o0Yap1ksFpPJFBQUpHYQz2U2m9PT0wMD\nAzl5woHU1NTAwEBOnnDg0qVLXl5eISEhagfxXJmZmRaLxd/fX+0gnis9Pd1sNoeEhHDyhAMp\nKSm510GDk1h3K5g2TfKcj1ujhowdq1IaAAAA51DsFNSqJTt3SrduUqGCVK0qzz4rP/wgbAgD\nAAAeji3AyurXl/h4tUMAAAAUBlvsAAAANIJiBwAAoBEUOwAAAI2g2AEAAGgExQ4AAEAjKHYA\nAAAaQbEDAADQCIodAACARlDsAAAANIJiBwAAoBEUOwAAAI2g2AEAAGgExQ4AAEAjKHYAAAAa\nQbEDAADQCIodAACARlDsAAAANIJiBwAAoBEUOwAAAI2g2AEAAGgExQ4AAEAjKHYAAAAaQbED\nAADQCIodAACARlDsAAAANIJiBwAAoBEUOwAAAI3wUjsAio1Op/Py4n+oIwaDwdvbW6/n84wj\nvIsK5O3tbTAY1E7h0fR6PT8ix3JWRzqdTu0gHs3b21vtCCWPzm63q50BAAAAxYBNFwAAABpB\nsQMAANAIih0AAIBGUOwAAAA0gmIHAACgERQ7AAAAjaDYAQAAaARXIoUGrV27dvHixbkPDQZD\nfHx8nmXsdvuyZcsSEhJsNltUVNQzzzzD9VRxq59++mnKlCl5hm3bth09evStE2febCi1Pv/8\n87i4OF9f35yHzqx2WDWhiCh20KDk5OSIiIguXbrkPFS8tvuqVas2btw4YsQILy+vDz/8UEQG\nDx7s1pTwbPXq1Zs4cWLuQ5vN9v777zdq1CjPYs682VA6HT58+KuvvurRo0dusXNmtcOqCUVE\nsYMGJScn16lTJyIiIr8FrFbrxo0bBwwY0LJlSxEZPHjwvHnz+vbtm7v+BUJCQm59C23durVG\njRoxMTF5FivwzYZSaN++fZs3b961a9etQ2dWO6yaUHQcYwcNSk5OrlixYkZGxvXr1xUXOHfu\n3JUrV5o0aZLzsEmTJiaT6eTJk27MiJLEbDavWrXqxRdfvPNLBb7ZUAoZjcY6dep06tTp1qEz\nqx1WTSg6tthBa+x2e3Jy8vr162fPnm2326tWrTpy5Mg6dercukxKSopOpytbtmzOw4CAAKPR\neOXKFTXyogRYvXp106ZN77333jxzZ95sKIXq1q1bt27dEydOfPPNN7lDZ1Y7rJpQdGyxg9ak\npKTo9fq6det+/vnnCxcurF69+qRJk1JTU29d5vr160ajUa+/+f738/O7du2a28OiBPj77783\nbtzYs2fPO7/kzJsNyOHMaodVE4qOYgetKVeu3Jo1a5599tmQkJDy5cuPGjUqOzt7z549ty7j\n7++fmZlpt9tzJ2azOSAgwO1hUQKsWbMmMjKyXLlyd37JmTcbkMOZ1Q6rJhQdxQ4aZzQaK1So\ncPXq1VuHoaGhdrs9d2g2mzMzM0NDQ9UICI+WlZWVlJR05zkTihTfbEAOZ1Y7rJpQdBQ7aM2e\nPXuGDx+eu/PCZDL9/fff1apVu3WZsLCw4ODgvXv35jzct2+fn59fzZo13Z0VHi/nxMbGjRsr\nftWZNxuQw5nVDqsmFB0nT0BrwsPD09LSZs2a1a1bN29v7xUrVlStWjXnahTbt2/PysqKjY01\nGAydO3deunRplSpV9Hr9okWLOnToYDQa1c4Oj7Nv3746derkuUJs7hvJwZsNyMPBaodVE4oR\nxQ5a4+PjM2PGjM8++2zmzJkGgyEiIuKVV17JORg5MTExPT09NjZWRPr06ZOdnT1t2jSbzdaq\nVatBgwapHRye6MCBA3fuh819Izl4swF3ym+1w6oJxUh360GaAAAAKLn4ZAkAAKARFDsAAACN\noNgBAABoBMUOAABAIyh2AAAAGkGxAwAA0AiKHQAAgEZQ7AB4rrNnz+r1ep1O98EHH6gSoHXr\n1i1btrxzvnnzZp1ON378+Du/dOTIEZ1O169fv7t7ZgAoCoodAM+1atWqnIuor1q1yvnv2rx5\n86BBg9LS0lyWS9q1a1euXLnVq1ff+aWvv/5aRHr27Om6VweA/FDsAHiulStXBgYGxsbG/vjj\nj3/++aeT33Xo0KHFixdnZma6LpiXl1fPnj1PnTr122+/5fnSunXrAgICOnXq5LpXB4D8UOwA\neKhTp07t2rWrS5cuffr0sdvta9asUTvRbeLi4kQkz0a7v/7665dffnniiSd8fX1VygWgVKPY\nAfBQK1euFJFevXo99thjBoPhzv2eO3fu7NSpU/ny5WvVqvXss89evnxZRNq0aTN27FgRKV++\nfP/+/UWkcePGTzzxxK3f+MQT/9/e/YU01cZxAH8mS+nM3KBIS12WNgc6GiZoNJ2mZOWfYrja\nnOCVo1HkLMTM0UXMblx2Iiqpq4rCUKEVuotaRbWswPxXZi1jpkYXK1NbppZ7Lw4d9vpmbXXR\n3sP3c3We5/H8zu8MJj+e5zxnhTKZjG22t7dnZWVFRkZGRESkpKScO3fOn/SUSuWKFSvmZXXt\n2jWv16tWqwOK/MsMXS6XVqtdvXq1UChUKpVtbW3s0OTkZE1Nzdq1aymKio+Pr6qq8ng8/uQP\nAJyEwg4AghSzDpuXl7d06VKFQvHgwYORkRF2tK2tLTMzc2RkZO/evYWFhS0tLampqWNjYzRN\nGwwGQojVaq2trf3lVc6fP5+fn//hw4eysjKDwTA3N6fX6/15pC8kJEStVg8ODnZ3d7Odvuuw\nvx15nr6+Prlcfu/ePY1Gc+DAgfHx8cLCwrNnzzKjpaWlFotFLpcfOnQoKSnJYrHs27cv0EsA\nAHd4AQCCz4sXLwghOp2OaTY0NBBCjh8/zjRnZ2clEolMJvv06RPTc+PGDUIITdNer9disRBC\n3G43MySXywsKCnyDFxQUJCcnM8ebN28Wi8XT09NMc3p6OiIiory8nGkqFIr09PSFknQ4HISQ\n2tpapjkxMREaGsosHAcU+ecZZmdnr1q1amxsjL13pVIpEAgmJiY+fvzI4/GMRiN7YmlpqUwm\nWyhhAOA8zNgBQDBqamoihLBrmtu3byc+D7Q9efLk5cuXFRUVAoGA6cnNzT19+rRcLg/0Qq2t\nrf39/aGhoUzT7XZ//fp1amrKn3M3bNggFovZrGw228zMDJvzn0RmjY2N3b59u7y8XCQSMT18\nPn/37t0ej+fhw4d8Pj8kJMRutzN1MCHk4sWLvb29AV0CALiE/7cTAAD4AWbJ0ul0njp1iukR\niUQdHR3Dw8OxsbGvXr0ihCQlJfmewqzABio8PLyrq8vhcPT09HR1dXV3d3/79s3Pc3k83q5d\nu+rr6/v6+mQy2dWrVwUCAbsf9k8is5iKzWQymUymeUNut1sgEFgslurqaqlUKpfLMzIyioqK\ncnJyeDxeoBcCAG5AYQcAQefp06fPnj0jhFRVVc0bamlpqaysnJmZIYTw+b/5H8y3wDKbzYcP\nHxaLxTt27Dh48GBqampWVpb/oTQaTX19fXNzs1QqbW9vz8/PpyjqzyOzGTITfiaTKTc3d97f\nJCYmEkKMRuPOnTutVuvNmzcvXbp08uTJnJwcm822aNEi/+8CADgDS7EAEHSY/bCXL1/2fXDk\n+fPn5PtMXkJCAiFkYGDA9yyj0XjmzJkfBvR6vb5Nl8vFHExOTh45ckSv17tcLpqmi4uL4+Li\nAppXS0lJSUhIaG5uvnXr1vj4OLsOG2jkhTKMj48nhPD5fKWPqKiokZGRJUuWvH//vrOzMzw8\n3GAwtLa2vn371mg02u329vZ2/28BALgEhR0ABJ0rV65QFFVUVOTbKZVK161b9+jRozdv3qxf\nv37lypU0TbNvIXY4HCdOnPD9tYm5uTnmYPHixQMDA2xRdefOHaZGJIQMDQ3Nzs5KpVL2rPv3\n7/v/JmSGRqMZGBg4evQoRVHbtm37jcg/yVAoFObm5jY2Nr5+/ZrpmZmZKSsrq6mpoSiqr68v\nNTWVpmlmKCwsLDMzk/zBXCYA/N/hyw8AwaWrq8vpdJaUlLAbI1gajaanp6elpWX//v3Hjh3T\n6XTp6enFxcWfP39ubGwUi8V6vZ4QwqxC0jS9detWhUKxadOmuro6lUqlUqkGBwcbGhrYyImJ\niXFxcWaz+d27dxKJ5PHjx62trZGRkR0dHXa7PScnx5+ENRqN2Wy+e/euWq1m12EDivyTDAkh\n9fX1mZmZGzdu1Gq1UVFRzc3NnZ2dTU1NPB4vLS1NIpHU1dWNjo5KJJLe3l6r1ZqYmKhUKn/3\n4weA/7m/sxkXAGAB1dXVhJDr16//d4iZtUpLS2Oadrs9OztbJBJFR0eXlJQMDQ0x/S6XKysr\ni6KoPXv2eL3eL1++VFZWRkdHM1sKdDpdRUUF+zKR/v7+LVu2CIXC2NhYrVY7PDx84cKF5cuX\n5+XleX/1uhNWcnIy+f7Ltiz/I/88Q6/X63Q6VSpVTEyMUCjMyMiw2Wy+n4lOp4uJiQkLC1uz\nZo3BYBgdHfX3swYAzuF5//1gBwAAV3k8nqmpqWXLlv3tRBYU/BkCQJBDYQcAAADAEdg8AQAA\nAMARKOwAAAAAOAKFHQAAAABHoLADAAAA4AgUdgAAAAAcgcIOAAAAgCNQ2AEAAABwBAo7AAAA\nAI5AYQcAAADAESjsAAAAADgChR0AAAAAR6CwAwAAAOCIfwCkaIEU9cElJAAAAABJRU5ErkJg\ngg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 AUC: 0.641025641025641 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nAAAAmAkUdgAAAABm4h+uFYioCnyw4wAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for fold 1 : 0.975609756097561 \n",
      "AUC for fold 2 : 0.752136752136752 \n",
      "AUC for fold 3 : 0.634146341463415 \n",
      "AUC for fold 4 : 0.95 \n",
      "AUC for fold 5 : 0.641025641025641 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94\nAAAgAElEQVR4nOzdZ0BTZ98G8H/CHjJFUBlai2JdIKDixIoVFw4c4KK21lGrVq2PtrVKW61a\nF9rHUa0bEUfVPiIuqNRNFQcOHKC4AWVvSHLeD6dvpCEiCZA74/p9MndOkuucxHh5nxEBx3EE\nAAAAAJpPyDoAAAAAANQOFDsAAAAALYFiBwAAAKAlUOwAAAAAtASKHQAAAICWQLEDAAAA0BIo\ndgAAAABaAsUOAAAAQEtoYbErLS0VVKl3797VfKrvvvtOIBDs3bu36sWsra2dnJyqWGDEiBFy\nkyxbtqw6MV6+fKmvry8QCBo3biyRSKoZvqLy8vJ79+49f/5cicdWrZqbqNZV3ph6enrNmjUb\nOHDguXPn6vSlK77dM2fOFAgER48ereZja/eNeOcHDwAAdI0+6wB1qF27dgKBoPJ4s2bNVJwk\nJSWFiOrXry8zbmJiUp2HR0ZGisViInrx4sVff/3Vs2dPRQM8efLEzc1t8ODBhw4dUvSx6qxN\nmzZ6enr8n4uLix89evTw4cOoqKhVq1bNnDmTbTa5tPWNAAAANaHNxS4hIUH6rz5bDx8+dHJy\nevLkiXIPDw8PJyJfX9+4uLiIiAglip22unjxopmZmfRmUVFRaGjo8uXLv/nmmyFDhjRp0qSu\nA3z66ae+vr5eXl51/UIAAADVoYW7YtVNZmZmTk6Oq6urcg9PSkq6evWqk5PThg0biOjAgQNl\nZWW1GlB7mJqa/vzzz15eXiUlJWfOnJG7TGZmJj/9WStat249aNAge3v72npCAACAmtD1Yrd7\n9+6+ffs6ODg0atSob9++u3btqnr50tLS7777rlOnTpaWlj4+PvPnzy8sLKz6Ifx+2Pfff1+5\nhPx03dixY93c3Nq3b5+Tk3Ps2DG5S/72228fffSRra1tixYtxowZc/bsWX584MCB/KsfPnxY\nIBBMmzaNiKZNmyYQCP7666+Kz3D+/HmBQDBlyhTpiEgkWrFiRY8ePezt7S0sLFq1ajVv3rxX\nr15VM/ynn34qEAjWrFkjMz5nzhyBQPD999/zNxMTE4OCgpo1a2Zqaurq6vrZZ58pPbtJRO3a\ntSOi+/fv8zcXL14sEAgSEhL+/vtvDw+PBg0aFBQUSNd3xIgRzZo1s7Cw8PLyWrdunUxpfufb\n/fXXX1c+xk6hN6K2kgAAABARcVqnpKSEXzWRSFT1kh9//DER6evru7u7u7u76+vrE9GYMWOk\nC8yfP5+IIiMj+ZuZmZn8Tjd9ff327dvzx6136tTJzMzM0dHxba8SERFBREuWLNmzZ8/kyZNH\njx69dOnSxMTE6qyLRCLh9yfevXuX47iff/6ZiEaMGCGzmFgsDgoKIiIjIyMfH582bdoQkUAg\n2L17Nx9g+vTpROTm5hYaGhodHc1x3BdffEFEcXFxFZ+HP+1g8uTJ/M3S0lJvb28isrS07N69\ne/fu3S0tLYnI3d29uLhY7iaSceLECSLq0aOHzDi/UsnJyRzHnT171tDQkIhatWrVq1evxo0b\nE5GTk1NmZmYVW4Z/iwsKCirf1bFjRyL69ddf+ZuLFi0iogMHDtjZ2Tk6Ovbu3buwsJDjuFWr\nVunp6enp6bVt27ZTp0788Y69evXi7+Wq93bPmzePiKKiopR+I2orCQAAAMdxulvsfv/9dyJq\n1qwZ35k4jktKSuLPqzhw4AA/ItNa+OPxvby8nj9/zo/s3r2bLyVV/Pv6448/EpG5uXnFPq2v\nr//dd9+9c134yZ6OHTvyN1NTUwUCgYmJSV5eXsXFtm3bRkQdOnRIT0/nR/744w89PT1bW1u+\nHCQnJxPR4MGDpQ+pTrHbsWMHEXXv3l3anwoKCnx8fIjor7/+kruJZJSXl9va2urp6b169Uo6\nGB8fT0Rdu3blb3br1q3iNheJRMOGDSOiNWvWVLFl5Ba74uLib7/9logMDAySkpL4Qb7Y2djY\nhIaGSj8St27d0tPTc3FxuXbtGj+SlpbGH7z49ddf8yPVebtlip0Sb0RtJQEAAOC0u9jJNWzY\nMH4xfofdqVOnKj72+PHj/IwUf7Nia8nMzDQyMjIwMEhNTa34EH6HWhX/vvLzgnZ2dvv3709P\nT3/69On69evr1atHRLt27ap6XSZNmkRE69evl47wvWrnzp0VF3NxcREIBNIqwxsyZAgRnTlz\nhlO22O3evXvYsGH8M0gtWbKEiLZv3155E8n12WefEdGWLVukI7NnzyaizZs38zdtbW319fXL\ny8ulC9y6dWvp0qUnT56sYsvw76aHh4fX/2vVqpWpqSk/vmrVKumSfLHz8vKq+PChQ4cSkcxL\nPH/+3NjY2MrKSiwWV/Ptlil2SrwRtZUEAACA0+5i165dO/dKvvrqK47jysrK9PT0GjZsWPnh\nDg4O0p5RsbXwk2cDBw6UWf7GjRtV//t64MCBFStWPHz4sOLg/v37icjBwaGKFSktLbWxsTE0\nNKy4U5I/Xs3f31868uLFCyLy9vaWeXhubu6zZ8+Kioo4ZYudDIlEcu3atR49eihU7GJiYoho\nwIAB0hEXFxdjY+OcnBz+Zvfu3YmoX79+MTExYrG4ig1SkdzWbm1t3a1bt2PHjlVcki92MvOj\njRs3trS0rPxy/K7npKSkar7dFYudcm9EbSUBAADgOE5HL3eSmpoqFovlXtCuadOmaWlpT548\nee+99yqO8/8kN2/eXGb5d57uGhgYWHlw2LBhjRo1evHixfPnz/mjyio7duxYVlaWhYVFSEiI\ndDAnJ4eITp06lZGR0aBBA2mwpk2byjzcwsLCwsKi6mzvlJGRcfTo0evXr1+/fv3GjRu5ubmK\nPoOvr6+dnV1MTExBQYG5uXl8fPzjx49HjhzJH65HRBs3bhw6dGh0dHR0dDR/ZkD//v1HjBjB\nr13VCgoKKl7upAoVL+RbUFDAXyL4bR+P7OxsJd5uJd6IOkoCAAA6S5uLXRW4t0z5EBF/CkXl\nS4oYGBjIXd7Y2Jg/2klRrq6uL168ePTo0duKHX8+bF5eXlRUlMxdYrF4//79U6dOJSJ+hvJt\n8RQis1lOnz49aNCg/Px8a2vr/v37BwUFeXt7nzx5kj+OrZr09PQCAwM3btx4/PjxYcOG7du3\nj4gqVtWWLVsmJiaeOnUqKioqLi7uxIkTx48fnz9/fkRERL9+/Wq+UjzpXloi4i934uDgMHny\nZLkL29vbP3z4UO5dVbzdSrwRdZQEAAB0lo4WuyZNmgiFwkePHlW+KyUlRU9PT2a6joj4EelF\nNKSePXtWxYXl8vLyzp49a2Njwx8bV9Hr16/p7fMuubm5UVFR+vr6L1++lPnJivXr10+dOjUi\nIoIvdvwzpKamyjxDcnJyQkKCh4dH5cmet5H5qatJkyYVFBTs2LFj1KhRfN8lori4uGo+m9TI\nkSM3btx46NChwMDA/fv329vby/yqm4GBQb9+/fga9+zZs5UrV4aFhU2YMIHfuVnrLC0t69ev\nLxAIFi5c+LZl0tPTScG3W4k3oo6SAACAztLR69gZGhq2bNny+fPnp0+frjgeGxv74sWLDz74\noPJcSMuWLU1MTE6cOCFzibWdO3dW8UJ6enpDhw718/OT2YmZnJx87969hg0bvu3atgcOHCgp\nKfnwww8r/xBZYGCgUCi8cOECX0ydnZ1tbGzi4+NlKsXixYuDgoIq94yKsrOzK948efKk9M9F\nRUUPHjxwcnIaN26ctNURkdw2XLXu3bs7ODgcPXr07NmzT58+HT16tPQJ792717Jly4CAAOnC\njo6Oq1atsrOze/nyZd1dqs3Dw+Ply5fSK8zxsrOz33vvvU6dOpFSb7dyb0RdJAEAAJ2lo8WO\niBYsWEBEkydP5o9hIqL79+/ze8TkTp9YWVl9/vnnZWVlI0eO5CdRiOjYsWNLly6t4lXMzMyG\nDh1aVFQ0ZswYabd7/Pjx6NGjRSLR4sWL3/bA3bt3E9GIESMq32Vvb89fIiQyMpKIhELh/Pnz\nRSLRuHHjMjMz+WViYmLCw8NtbW27dOkifWB+fr70z/yV5LZu3Sr9GYbDhw/z1zfhmZqa1q9f\n/+XLl9LtI5FIfv31102bNhFRcXFxFWstQygUBgYG5ubm8idyVtwP27Rp00ePHh05cqTiNX6P\nHTv2+vXrFi1aVPP4OSXwb/GIESOuXbvGj+Tn54eEhDx69KhPnz6k1Nut3BtRF0kAAEB3sT57\no/ZV8zp2Eolk9OjRRGRoaOjt7e3l5cUfHRUSEiJd5m0XKDYyMurYsSO/Z61Dhw4dOnSo4uTE\ntLQ0Nzc3+v/L/LZv397IyEjmhWQ8ffpUKBQaGBi87SK9//3vf4moVatW/M3S0lK+BJiZmXXr\n1s3Ly0sgEAiFwoMHD/IL8L8VYWhoOGLEiK1bt3Icl5qayp++wP86Ar+nmF8j6VmxP/zwAxGZ\nm5sPGzaM/12EevXqjRo1ioicnZ3DwsIqb6K3kf7AV9u2bWXu4q+6TEQffPCBv79/69atiUhf\nX1/mSjQy+IfIvUCxDP6s2PDwcJlx/oRWgUDg5ubWq1cvKysrIurevXtpaSm/QHXebpnLnSjx\nRtRWEgAAAE67L3fyzl+e4Dhux44dH330kb29vYODg7+/P/8LAVKVW0tJSck333zToUMHU1PT\nxo0bz5w5s6CgwNfXt+p/XwsKCpYsWeLv79+gQYPGjRv3799f+i+9XMuWLSOivn37vm2Bly9f\nCoVCIrpx4wY/IpFIwsLCfH19raysHB0dBw4ceOXKlYoPCQ0NtbGxMTU1nT9/Pj9y7dq1/v37\n29nZ8ZurQ4cOd+7cqVjsxGLx+vXr27Zta2Zm5ubmFhISkpqaWlBQMGTIEAsLi+HDh8vdRHKJ\nxeJGjRoR0cqVK2Xukkgk+/fv7969e8OGDY2NjV1dXUePHv3OX+aoebHjOO7o0aMDBw50cnLi\nf8hrzZo10i7Fe+fbLVPsOKXeiFpJAgAAwHGcgHv7+aGgI3JycvLy8pydnVkHAQAAgBpBsQMA\nAADQErp78gQAAACAlkGxAwAAANASKHYAAAAAWgLFDgAAAEBLoNgBAAAAaAkUOwAAAAAtgWIH\nAAAAoCVQ7AAAAAC0BIodAAAAgJZAsQMAAADQEih2AAAAAFoCxQ4AAABAS2hhsTt69KigEgcH\nh169esXGxrJO9263b9+2s7N7272auHYikUgkEin32Kq3xt69e2U2hYWFhbe395YtWziOUzYv\nAACAptJnHaCu9O3b19PTk/+zSCR68uTJ77//3rt379OnT/fo0aNWXiInJ2fOnDkxMTHl5eW+\nvr4rV660t7evvNjt27dbt24t9xnmz5//448/KvHSVa+dRCKZM2dORERESUlJ3759N2zYYGlp\nqcSryKjm+srIzMxs27ZtQEDAhg0bpIPx8fGhoaEJCQlmZmadO3detmyZo6Oj0sEGDRrUpk0b\nIuI4Lj09/Y8//pgwYcKTJ0++//57pZ9TszD8KAIAgFrR2mI3aNCgSZMmVRwJCQnp06fPsmXL\naqXYFRYWduzYMS0tbcyYMfr6+uHh4WfPnr169aqtra3MktbW1hMmTJAZfPbs2fHjx5s1a6bc\nq1e9dhEREQcOHPjzzz8NDQ2Dg4O/++67tWvXKvdCUtVfXxmfffbZixcvKo4cPnw4MDCwefPm\nn3/+eXFx8Y4dO3x8fM6cOdO0aVPlsg0fPnz06NHSm0uWLGnbtu3SpUvnzJljbm6u3HNqELYf\nRQAAUC+c1omKiiKijRs3Vr7LysqqadOmtfIqa9asIaKTJ0/yNy9evCgUCn/88cdqPrx37969\nevWSSCSV77p161b9+vXf9sDqrN3333+/YMECfnD58uX+/v7VTFUF5db3t99+MzIyIqLJkyfz\nI6WlpQ0aNGjTpk1xcTE/kpaWZmdn9+mnn8p9hqq3RmRkJBGFh4fLjE+bNo2Irly5Up1Vq47s\n7OxaeR6xWFxeXl4rTyVVdx9FAADQOFp4jF0VBAIB3zNqbt26da1bt+7duzd/s1OnTj4+Pv/9\n73+r89jffvstPj5+69atAoGgVsLwpGu3YMECfi/ko0ePNm/ePGTIkJo/uRLrm5ycPGPGjMWL\nF1ccvH37dkZGxoQJE4yNjfkRe3v7sWPH7tq1q7CwsOY5KyotLSWi1NTU4ODgpk2bWlpa9ujR\n4+jRoxWXiY6O9vX1tbe3t7CwaN++/ebNm6V39ezZc/jw4Q8ePPD39/fy8iKi/Pz8r7/+2tXV\n1dTUtFmzZnPmzKmY+erVq/3793dwcGjUqFH//v2vXr0qvatv375DhgzZu3dvw4YNDQwMGjZs\nOGnSpLy8vFpZTTX8KAIAACs6VOzi4uKys7MHDBhQ86cSiUTJycm+vr4VB319fdPT03Nzc6t+\n7IsXL2bPnr148WJnZ+eaJ5GSWbuysrJFixZ5enpOmTJl4sSJNXxyJdZXJBKNHj26Y8eO06dP\nrzheUFBARHp6ehUHjY2Ny8rKnj59WsOcvKysrN9//11PT69ly5Y3b950d3c/e/ZsUFDQ7Nmz\nc3NzBw4cuGnTJn7JHTt29O/fPysrKyQkZMqUKRKJZOLEifv27ZM+VW5u7pAhQ+zt7efNm0dE\nY8aMWbFihbu7+zfffNOqVasVK1ZI1y42NtbHx+fWrVvjx48PCQm5deuWj49PTEyM9Klu3Ljx\n8ccfDx8+fPPmzd26ddu0adPs2bNrvrJq+FEEAACGtPYYu6ioqLS0NP7PIpEoNTX1wIEDffv2\n/eGHH2r+5GlpaRKJROZsTf7m8+fPqz5TYcGCBba2tjJHyCmq6rXLycnp3bu3vb391atXmzRp\nUpMX4imxvqGhoffv309MTBQK//Wfh3bt2hkbG0dEREyePJmvd3l5efwe1efPn7u5uSkR7+DB\ng8nJyUTEcVxGRsahQ4fS0tK+/vpra2vrwMBAKyur69evW1lZEdE333zj5+c3a9as4ODgevXq\nRUREODs7X7lyxdDQkIh+/PFHOzu7mJiYESNG8M986tSpTZs2ffbZZ0SUm5t75MiRGTNmrF69\nmr937Nixly9fJiKJRDJr1qz69esnJCTUr1+fiGbNmtWuXbs5c+ZcvXqVnwzjZ0/5Q9wmTJjg\n7e198uRJJVZWBvOPIgAAqBVtLnb84WhSBgYGAwYMMDExqbxwWVnZ6dOn5T6PpaVlp06dZAZf\nvXpFRBYWFjJLElFGRsYHH3zwtlRJSUnbt2/fsWOHgYFB9dZDvqrXbunSpWKxODw8XCAQ5Obm\n6uvrm5mZVVy4rtf33LlzS5cu3blzp5OTk1gsrniXhYXFTz/9NGvWrE6dOg0ePLiwsDA8PFwi\nkRCR0nsDDx48ePDgQf7Penp677333uzZs7/88svs7OzTp08vWrSIb3VEpK+vP3ny5ODg4EuX\nLvXu3fv3338XCAR8qyOi169fi0Si4uJi6TObmZl98skn0scKhcLY2Nh79+61aNGCiHbt2sXf\n9ejRo8TExEWLFvGtjojs7OwmT568cOHCx48f893a3Nx8/Pjx0mdu27bt//73v8rronEfRQAA\nUCtaW+w2btxYcSri8ePHU6dOnTp1qqurq/RoJKns7Gx/f3+5z9OpU6eLFy/KDNrY2ND/71WU\n4g+Zsra2riLV999/36hRo+Dg4Gqvh3xVr9358+evXbsmTeLr6yvTFep0fXNzc8eMGRMYGDhq\n1Ci5LzFz5kxHR8eVK1cuW7bM2dl58uTJpqamM2fOdHBweOeKyxUeHl7xrFipe/fuEdH8+fPn\nz58vc9fr16+JyNzc/Nq1a+fPn79x48a1a9euX78uU0OdnZ2le43NzMxWrFgxd+5cNzc3d3f3\nbt26BQQE9OrVSyAQpKSkEJHMlUT4K7CkpKTwxc7FxaXiDmiZiUwpjfsoAgCAWtHaYifDxcVl\n1apVR48ejY2NrVzs7O3tOUWuZ8tXEL4cSPE3GzVq9LZHvXr16uDBg3Pnzn3bP+pKk1m7s2fP\nVr18na7vhg0bnjx5Mm7cOP5sTb4qJSYmrlmzpkOHDj4+PkQ0fPjw4cOHSx8yd+5c6avUIn4q\nbv78+X5+fjJ38VNuixYtWrBggbOz8+DBg+fNm+fl5SVzsJrMTOeXX345YsSIP/74IyYmZvfu\n3b/88kuvXr2OHTvG3ysz48i/y9IrM1dzYkzTP4oAAMCWrhQ7InrvvfeIKDMzs/Jdiu7/MjIy\nev/998+dO1dx8Pz58w0aNJDujKts+/bt5eXlH3/8saLJq6OKtausTte3tLSU4ziZq91euHDh\nwoUL8+fP9/Hx2bZtW9OmTStWqOjo6Pbt2/OTT7WIvzabvr5+xSsX3rt378qVK15eXvn5+T/8\n8MPEiRM3btwovVdmxq6izMzM1NRUV1fXKVOmTJkypbS0dN68eWFhYdHR0fxc3a1btwICAqTL\n37p1i4iaN2+uUGYt+CgCAABLTC+2UifedqU3/t/sgICAyg+RnohQWadOneS+ypIlSwQCweXL\nl/mbSUlJBgYG8+bNqyKYp6dnq1at3plfuevYVbF2lalmfXn8lJX0OnYcx3Xp0sXR0TEvL4+/\nuX//fiLavHmz3Icrdx07KT8/PwcHh5SUFP5maWlpx44dnZycJBLJzZs3iWj16tXShc+ePSsQ\nCMaMGcPf9PX19fLykt7L963vv/9eOsIf2BcVFSUWi1u3bt24cePMzEz+rtevXzdu3Lh169Zi\nsZjjOH9/f3d394rBJkyYIHe91OqjCAAAGkeHih3HcY0bN27btm2tvMrr169btGjRsGHDpUuX\n/vzzz87Ozk2aNHn+/Dl/765duxwdHVetWiVd/tWrVwKBYNKkSe98ZqUvUFyLa1eZousrVbnY\nnTp1SigUtmnT5qeffho/fryhoWGHDh2k1yuWUcNid+3atXr16jk4OMycOXPZsmVeXl4CgWDv\n3r0cx5WVlTVp0sTW1nbu3LlbtmyZNGlS/fr1HRwcmjVrFhMTw1UqdkVFRc2bNzc0NJw4ceKK\nFSvGjRtnaWnZokWL/Px8juNOnjxpYGDQtGnTBQsWfPfdd02aNDE0NOSfh1Ok2Cmh7j6KAACg\ncXTrCJtWrVolJiYeOnSo5k9la2t79uzZXr16rV+/fs2aNT4+PufOnZMe1VRYWPjs2bOKV6Dl\n/43v0qVLzV/6bWpx7SpTdH2r4OfnFxUVZWxs/NNPP926dWvOnDmxsbHS6xXXLnd396tXr3bu\n3Hn//v0//fSTiYlJdHQ0fzUTAwOD6Ohob2/vjRs3hoaG5uXlXbt27eeff87Pz1++fHnlpzIx\nMTl+/Pjw4cOjo6O//fbbc+fOjRo16s8//+R/tax3794XLlxo0aLFr7/+umnTppYtW166dKlX\nr151sVIy1PCjCAAArAg4RY7UBhW4ffu2r68vfxkLwNYAAACoPt2asQMAAADQYih2AAAAAFoC\nxU7tNG3a9I8//mCdQl1gawAAAFQfjrEDAAAA0BKYsQMAAADQEih2AAAAAFoCxQ4AAABAS6DY\nAQAAAGgJFDsAAAAALYFiBwAAAKAlUOwAAAAAtASKHQAAAICWQLEDAAAA0BIodjqqsLCQdQTN\nJpFICgoKSktLWQfRbGVlZeXl5axTaLaSkpKCggL8hlANFRUVsY6g2cRicUFBQVlZGesgauHP\nP/+cNGnSlClTsrOzFXpgaWmpSCSq4auj2OkoNJIa4jiupKQEpaSGRCKRWCxmnUKzlZWVlZSU\nsE6h2TiOw1diDUkkEnwlEpFYLA4NDf3oo4+2bNlib29vYWGh0MNr5StRv4aPBwAAAIBnz56N\nHj36zJkzjo6OERER3bp1YxIDxQ4AAACgRuLj4/v375+ZmRkQELB161ZbW1tWSbArFgAAAKBG\nWrRoYWNjs3Tp0sOHDzNsdYQZOwAAAIAasrKyunXrlqGhIesgmLEDAAAAqDF1aHWEYgcAAACg\nEHW+wBCKHQAAAEB1JSUleXt7x8fHsw4iH4odAAAAQLX89ttvXl5eCQkJx48fZ51FPpw8AQAA\nAPAO+fn5kydPjoiIsLCwiIiICA4OZp1IPhQ7AAAAgKokJCQEBQUlJyd7enpGRka+//77rBO9\nFXbFAgAAAFRl8eLFKSkps2bNunDhgjq3OsKMHQAAAEDVNm3a9Nlnn/Xt25d1kHfDjB0AAABA\nVerXr68RrY5Q7AAAAAC0BoodAAAAwD9evnx59OhR1imUh2IHAAAAQER04sQJd3f34cOHp6Sk\nsM6iJBQ7AAAA0HUikSg0NLRfv35ZWVn/+c9/mjZtyjqRknBWLAAAAOi0J0+ejBo16vz5887O\nznv27OncuTPrRMpDsQMAAADddefOnS5duuTk5AwbNmzz5s1WVlasE9UIih0AAADoLjc3t65d\nu/r5+c2YMYN1llqAYgcAAAC6SygUHjlyhHWKWoOTJwAAAAC0BIodAAAA6IrS0lLWEeoWih0A\nAADohNu3b3t5ee3atYt1kDqEYgcAAADab926dV5eXrdv3759+zbrLHUIJ08AAACANsvLy5s4\nceLevXstLS137NgxYsQI1onqEIodAAAAaK3Lly8HBQU9fPiwQ4cOe/bsee+991gnqlvYFQsA\nAADKu3GDQkL0eve2Gj7ceN8+1mkqOXToUGpq6rx5886fP6/1rY4wYwcAAABKO3WKBgygsjIh\nkfD6dTp2jC5fpuXLWceq4Icffujfv3+XLl1YB1ERzNgBAACAMiQS+uQTKiv71+CKFXTtGqNA\n8ujr6+tOqyMUOwAAAFBOcjI9eyZnPC5O1UlACsUOAAAAtMHTp09/+eUX1ikYQ7EDAAAAZTRr\nRo6OcsZ9fVWdhIiOHDni4eExffr0c+fOMXh5tYFiBwAAAMrQ06MtW2QHZ88mDw+VxigvLw8N\nDR08eHBBQUFYWJhOHVFXGc6KBQAAACV99BFdv04//yy5dUvSuLHg44/1VHz139TU1ODg4EuX\nLjVp0mTPnj2dOnVS6curHxQ7AAAAUF67drR9uzg3N9fExMTMzEyVL52WltauXbitqloAACAA\nSURBVLu8vLzRo0dv2LChXr16qnx19YRiBwAAABrJwcHh888/d3V1/eSTT1hnURcodgAAAKCp\nlixZwjqCesHJEwAAAABaAsUOAAAANEBeXl55eTnrFOoOxQ4AAADUXUJCgqen57fffss6iLpD\nsQMAAAD1xXHc6tWrO3funJKSIpFIWMdRdzh5AgAAANRUZmbmxx9/HBUVVb9+/W3btg0YMIB1\nInWHYgcAAADq6OzZs6NGjXr27FmPHj12797duHFj1ok0AHbFAgAAgDpKS0t78eLF3LlzY2Nj\n0eqqCTN2AAAAoI6GDx/etm3bFi1asA6iSTBjBwAAAGoKrU5RKHYAAAAAWgLFDgAAABhLTU2d\nMWOGWCxmHUTjodgBAAAASwcPHvTw8Fi7du3+/ftZZ9F4KHYAAADARmlp6YwZMwIDA0tKSsLC\nwoKCglgn0ng4KxYAAAAYuHfvXlBQ0PXr11u0aBEZGenu7s46kTZAsQMAAABVKyoq6tGjR3p6\n+scff/zf//7XzMyMdSItgWIHAAAAqmZqarpixQoiGjNmDOssWgXFDgAAABhApasLOHkCAAAA\nQEug2AEAAEDdys7Ofv36NesUOgHFDgAAAOrQ5cuXvby8xowZI5FIWGfRfih2AAAAUCckEsmy\nZcs6d+6cmpravn17juNYJ9J+OHkCAAAAat+rV69CQkKOHTtmZ2e3c+dOf39/1ol0gqqL3Y4d\nO0aOHGlsbMzf5DguIiLi9OnTEomka9euISEhenp6Mg+pzjIAAACgPs6ePTty5MiXL1/6+fnt\n2rXLwcGBdSJdodJdsUlJSb///nt5ebl0ZN++fdHR0Z9++umUKVP++uuvHTt2VH5UdZYBAAAA\n9WFlZZWfn79w4cITJ06g1amSimbsrl+/fvz48cuXL1ccFIvF0dHR48aN8/HxIaJPPvlkw4YN\no0aNks7nVXMZAAAAUCtt2rR5+PChnZ0d6yA6R0UzdkZGRm5ubjL71589e5adne3p6cnf9PT0\nLCoqevjwoaLLAAAAgLpBq2NCRTN2LVu2bNmyZXJy8pEjR6SDWVlZAoHAxsaGv2lubm5kZJSd\nnV3xgdVZRorjuJKSkrpZA23DcVxxcTHrFBqMP7dLLBZjM9aESCQSCAQ4Ua4mxGIxEeGrryY4\njsNXYg3xn0ORSITNWBMikUgikbzzojACgaCK/ZYsz4rNz883MjISCt/MGpqYmOTl5Sm6jBTH\ncYWFhXWUVvtgW9VceXl5xWNGQTmlpaWsI2g8/HWuOWxDpT169Cg0NDQsLMza2hpfiTX3zq9E\nPT09NS12ZmZmpaWlHMcJBAJ+pLi42NzcXNFlpAQCQb169eo0s9YoKCh422aE6pBIJIWFhQYG\nBjjcsyZKS0uFQqGBgQHrIBqsuLhYJBLhq68mOI4rKioyMzNjHUQj7d27d9q0aXl5eV27dp06\ndaqRkRHrRBqsml+J0kYkF8tiZ21tzXFcTk6OtbU1ERUXF5eWlvJ/VmgZKYFAgI9UNRUWFmJb\n1YRYLC4sLNTT08NmrAmxWCwUCrENa4L/z72hoWHV3/VQBX4/LD6HiiopKZk7d+7atWtNTExW\nrlw5btw4fCXWkEgk0tfXr+E2ZPnLEy4uLpaWlteuXeNvXr9+3cTExNXVVdFlAAAAQJWSkpI6\ndOiwdu3ali1bxsfHT5s2jXUi+AfLGTs9Pb1+/fqFh4c7OjoKhcJt27Z99NFHfFGNjY0tKyvr\n27dvFcsAAACA6kkkkmHDht25c2fSpEmrV682MTHBoXXqg/FPigUFBZWXl//8888SiaRLly7j\nx4/nx+Pi4goLC/v27VvFMgAAAKB6QqFwy5YtT58+HT58OOssIAsXGtBRWVlZ0ovIgBLEYnF2\ndraxsTHOQamJoqIioVCIE1BqIi8vr6yszNbWFsfYKa3ikdygnPLy8tzcXBMTE5yDUhOFhYWa\nfYwdAAAAANQiFDsAAAB4q8zMzKSkJNYpoLpQ7AAAAEC+M2fOuLu7BwQE5Ofns84C1YJiBwAA\nALI4jluzZo2fn9/z588DAwNNTU1ZJ4JqYXxWLAAAAKibjIyMcePGnThxwt7efufOnR999BHr\nRFBdKHYAAADwRnx8fEBAQEZGRt++fXfs2GFnZ8c6ESgAu2IBAADgDVdXV3Nz84ULF0ZFRaHV\naRzM2AEAAMAbNjY2t2/fxgUmNRRm7AAAAOBf0Oo0F4odACgpK4uSkqisjHUOAAD4fyh2AKCw\nhw+pd2+ytaUPPiArKwoNJYmEdSYAUNy9e/d8fHxu3LjBOgjUGhQ7AFBMcTEFBFBMzJub339P\ny5YxzQQAituxY4enp+elS5cOHz7MOgvUGhQ7AFDM/v10+7bs4E8/UWkpizQAoLiCgoKxY8d+\n/PHHQqFw165dCxcuZJ0Iag3OigUAxTx4IGewoIBevKCmTVWeBgAUdPv27ZEjR96+fdvDw2Pv\n3r2urq6sE0FtwowdAChG7mWt9PTI1lblUQBAcV999dWdO3emTZt28eJFtDrtg2IHAIoZOpQs\nLWUHhw0jCwsWaQBAQVu2bDl8+PDatWuNjIxYZ4Hah2IHAIpxdKRdu8jG5s1I5860YQO7QACg\niEaNGgUEBLBOAXUFx9gBgMIGDqR79ygmhl6+pHbtqGdPEghYZwIAABQ7AFBO/foUFMQ6BABU\nKT09/ebNm35+fqyDgOpgVywAAIAWio2NdXd3Hzp06OPHj1lnAdVBsQMAANAqIpEoNDS0T58+\nr1+/njVrlpOTE+tEoDrYFQsAAKA9nj59Onr06LNnzzo5OUVERHTt2pV1IlApFDsAAAAtcevW\nrR49emRlZQ0ePHjLli02FU9fB92AYgcAAKAl3Nzc3N3dAwICpk+fLsDJ6joJxQ4AAEBL6Ovr\nx8TEoNLpMpw8AQAAoD3Q6nQcih0AAIBGKi8vZx0B1A6KHQAAgOZJSkry8vI6dOgQ6yCgXlDs\nAACU9OeftGQJrVlDt26xjgI6ZvPmzZ6enomJiRcvXmSdBdQLTp4AAFBYeTkFBtKRI//cNDSk\nr7+m0FCWkUBH5OXlTZ48ec+ePRYWFlu2bAkODmadCNQLih0AgMIWL37T6oiorIy+/558fKhP\nH3aZgJH8fLpxg8rKyMODrK3r9rUSEhKCgoKSk5O9vLwiIyObNWtWt68HGgi7YgEAFBYeLmdw\n1y6V5wDWdu+mJk2oWzfq1YucnCgsrG5fLjw8PCUlZfbs2RcuXECrA7kwYwcAoLDsbDmDWVkq\nzwFMxcfThAlUUvLPzcJCmjmTmjShwYPr6hWXLl0aEBDQs2fPunoB0HyYsQMAUJibm5zBDz5Q\neQ5g6pdf3rQ6qZUr6/AVjYyM0Oqgaih2AAAKW7RIdsTOjmbNYhEF2HnyRM7g48cqzwFQAYod\nAIDCevakgwfpvff+udmlC504QY0aMc0EKufoKGfQ2bl2nvzFixe//fZb7TwX6BIUOwAAZQwZ\nQikplJFBubl07hx5eLAOBCo3daqcwenTa+GZjx075u7uPnHixPj4+Fp4OtAlKHYAAMqzsyML\nC9YhgJEuXWjLFrK0/OemsTH99BONGFGj5xSJRKGhoQMGDMjOzl6wYIG3t3fNc4JOwVmxAAAA\nSvrkExoyhK5cIZGIPD2pQYMaPduTJ0+Cg4MvXLjg4uISERHRuXPnWooJOgTFDgAAQHnW1tS7\ndy08z/Pnz9u1a5eTkzN8+PDNmzdbSmcCARSBYgcAAMBe48aNx4wZ07p160mTJrHOAhoMxQ4A\nAEAt/PLLL6wjgMbDyRMAAAAAWgLFDgAAQNUKCgrEYjHrFKCFUOwAAABU6tq1a+3bt//xxx9Z\nBwEthGIHAACgIhzH/fLLLz4+PsnJyXl5eazjgBbCyRMAAACqkJubO3HixH379llaWu7atWv4\n8OGsE4EWQrEDAACoc3///XdwcPDDhw87dOgQGRnZtGlT1olAO2FXLAAAQJ27efPmo0ePpk+f\nfu7cObQ6qDuYsQMAAKhzn376qbe3d9u2bVkHAS2HGTsAAABVQKsDFUCxAwAAANASKHYAAAC1\n6enTp19//bVEImEdBHQRih0AAECt+d///ufu7r506dKDBw+yzgK6CMUOAACgFpSXl8+bN2/w\n4MGFhYVhYWGBgYGsE4EuwlmxAAAANZWamhocHHzp0qXmzZtHRkZ6eHiwTgQ6CsUOAACgRgoK\nCjp06PDq1asxY8asX7++Xr16rBOB7kKxAwAAqBFzc/NFixYZGBiMHz+edRbQdSh2AAAANTVx\n4kTWEQCIcPIEAAAAgNZAsQMAAFBAbm5udnY26xQA8qHYAQAAVNeVK1e8vLzGjRvHcRzrLABy\noNgBAAC8G8dxq1at6tKlS0pKipubG35YAtQTTp4AAAB4h9evX48fPz4qKqp+/frbt2/v378/\n60QA8qHYAQAAVOX8+fMjR458/vy5r6/v7t27GzVqxDoRwFthVywAAEBVDA0NX79+PXfu3JiY\nGLQ6UHOYsQMAJUkkVFBAFhascwDUMW9v74cPH6LSgUbAjB0AKCwjg0JCyNycLC3J2Zk2b2Yd\nCKCOodWBpsCMHQAoRiSiwYPp4sV/bj59ShMnkkRCkyYxjQUAAJixAwBF/f77m1Yn9fXXVF7O\nIg1ArXr06FFwcHB+fj7rIABKQrEDAMXcvi1nMDubnj9XeRSAWrV//34PD4/IyMidO3eyzgKg\nJBQ7AFCMpaWcQYFA/jiARigtLZ0xY8aIESNKS0vDwsKmTp3KOhGAkrTtGDuxWMw6gsbAtqoJ\n/qLzHMfp4GYcMIC++06vuPhfgx99xFlYSBTdGPxm1MFtWIv4H7YSi8UCgYB1Fk119+7doKCg\nW7duubm57dmzp02bNvhMKkqXvxJrEcdxEonkndtQIBAIhW+dmBNo06/dcRyXl5fHOoVmKC8v\nNzAwYJ1Cs5WXlwuFQj09PdZBGNi923D2bJOSkn9uNmsmiYoqbNhQ4V9Y4utIFd9Q8E5isVgi\nkeCvs9LEYnGHDh1SUlLGjBmzfPlyExMT1ok0EsdxIpFIZ78Sa0s1vxKFQmG9evXedq9WFTuo\nvqysLBsbG9YpNJhYLM7OzjY2NjY3N2edhY2UFDp4kDIyqE0bGjmSjIyUeZKioiKhUGhsbFzb\n6XRIXl5eWVmZra0tZuyUFhcX9+DBg88++4x1EA1WXl6em5trYmJiZmbGOosGKyws1NfXN1Lu\n+/T/aduuWABQjWbNaM4c1iEAakOPHj3atWvHOgVA7cAeEAAAAAAtgWIHAAC6IisrKyUlhXUK\ngDqEYgcAADrh77//9vb2DggIKCoqYp0FoK6g2AEAgJbjOG7NmjVdu3Z99OiRn58fTiIGLYaT\nJwAAQJu9evUqJCTk2LFjDRo02LFjh7+/P+tEAHUIxQ4AALRWfHz84MGD09LSPvroo507d9rb\n27NOBFC3sCsWAAC0VpMmTfT09BYuXHjs2DG0OtAFmLEDAACtZW9vf+/ePVw1F3QHZuwAAECb\nodWBTkGxAwAAANASKHYAAKANUlNT/fz87t27xzoIAEsodgAAoPF2797dtm3b2NjYvXv3ss4C\nwBKKHQAAaLDi4uIZM2aMGTNGJBKFhYUtWLCAdSIAlnBWLAAAaKqkpKSRI0fevHnzgw8+2Lt3\nb+vWrVknAmAMM3YAAKCpvvjii5s3b06ePDkhIQGtDoAwYwcAAJpr69atly9fHjZsGOsgAOoC\nxQ4AADSVi4uLi4sL6xQAagS7YgEAAAC0BIodAABogFevXp07d451CgB1h2IHAADqLi4uzt3d\nPSAg4Pnz56yzAKg1FDsAAFBfYrE4NDTUz88vPT19+vTpDg4OrBMBqDWcPAEAAGoqPT197Nix\np06dcnR03L17d/fu3VknAlB3KHYAAKCOEhMTe/Xq9fr16wEDBmzbtq1+/fqsEwFoAOyKBQAA\nddSiRYv33ntv4cKFf/zxB1odQDVhxg4AANSRkZHRhQsX9PT0WAcB0CSYsQMAADWFVgegKBQ7\nAABgTywWs44AoA1Q7AAAgLG7d+96eXkdOXKEdRAAjYdiBwAALG3bts3Ly+v69etnz55lnQVA\n4+HkCQAAYCM/P//zzz8PDw+vV69eeHj46NGjWScC0HgodgAAwMDVq1eDgoIePHjQvn37yMhI\nV1dX1okAtAF2xQIAAAObNm1KTk6eMWPGhQsX0OoAagtm7AAAgIFVq1YNGTKkT58+rIMAaBXM\n2AEAKOPuXRo2jJydqXlzmj6dMjNZB9I0pqamaHUAtQ4zdgAACrt/n7y9qaDgn5sPHlBMDF25\nQqamTGMBgM7DjB0AgMJmz37T6nhJSbR6NaM0ai8tLW337t2sUwDoBBQ7AACFxcfLGbx0SeU5\nNMGpU6fc3d1DQkKuXr3KOguA9kOxAwBQmKGhnEEjI5XnUG8ikSg0NNTf3z8zM3P+/Pnu7u6s\nEwFoPxxjBwCgsH79aPNmOYMg9fTp01GjRp07d87JyWnPnj1dunRhnQhAJ2DGDgBAYcuW0fvv\n/2tk0CAaP55RGvXz5MkTd3f3c+fODRky5MaNG2h1ACqDGTsAAIVZW9ONG7R+PV26RKam1KcP\njRpFAgHrWGrD2dl50KBB7du3/+KLL1hnAdAtKHYAAMowNaWvvmIdQo1t3bqVdQRQnVev6No1\nAxcXQatW+B8OY9gVCwAAAEoqKaGJE8nJyWDAAMs2bUy7daMHD1hn0m0odgAAUCNFRUUSiYR1\nCmBjzhzavJmk7//58zRkCBUXM82k21DsAABAeXfu3OnUqdOKFStYBwEGcnJo40bZwdu36cgR\nFmmAiFDsAABAaRs3bvTy8rp58+bz589ZZwEGnj4lkUjO+MOHKo8C/w8nTwAAgMLy8vImTZoU\nGRlpYWGxdevWoKAg1omAAQcHEgiI42THGzVikQaICDN2AACgqCtXrrRv3z4yMtLb2/vq1ato\ndTrLzo6GDZMddHSkgAAWaYCIUOwAAEBRFy5cePjw4fTp08+dO9esWTPWcYCljRupV683N5s2\npf37ycqKXSCdh12xAACgmGnTpnXu3NnLy4t1EGDPxoZiYujiRVFCQrGLi4Gfn7GJCetMug3F\nDgCUJxKRPr5FdI9AIECrg4q8vDhX11ITEyFaHXPYFQugsNhY6tlT2Ly5rYeH6XffUWEh60Aq\nl51NX3xB9euTsTG1akWRkawDAQAAEaHYASjq6FHy86OzZwXZ2YLkZOGiRRQYKOekMC0mkdDQ\nobRuHWVmklhMd+5QcDDt2sU6FtSN58+f//DDD6xTAEB1odgBKIDjaOpU2cETJ+jQIRZpGDl0\niOLiZAdnzZJ/OSvQaEePHnV3d1+4cOH//vc/1lkA1NrRozRxIg0bRosWUU4OyyQodgAKyMig\nx4/ljF++rPIo7CQmyhl8/ZpwhVptIhKJQkNDAwICcnJyFi5cOGDAANaJANTXjBk0YABt3ky/\n/07ffUdubvTkCbMwOOwZQAHGxvKvxmlszCINI+bmcgYFAvnjoIkeP34cHBx88eJFFxeXPXv2\n+Pj4sE4EoL7+/JPWrv3XSHo6TZpEx46xyYMZOwAFWFpS165yxvv3V3kUdgYOlFNkfX3J1pZF\nGqhteXl5Xl5eFy9eHDlyZGJiIlodQNWio+UMnjpFZWUqj0JEKHYAitqyherX/9fIwoWkU1d+\ncHOjFSvI0PDNiJMTbd3KLhDUKgsLi2+//XbTpk38z4WxjgOg7kpL5QyKxVRervIoRIRdsQCK\ncnWle/do3TpJQkK5vb1w9GiD7t1ZZ1K5qVOpRw86cIAyMqh1awoJITMz1pmg9nz55ZesIwBo\njA4d5Ay2acPsW1HA6dR1GuD/ZWVl2djYsE6hwcRicXZ2trGxsTmOLKuBoqIioVBorFOHKNa2\nvLy8srIyW1tbgUDAOoum4jguJyfH2tqadRANVl5enpuba2JiYqZ7/8kTi+nDD+nMmX8NnjlD\n3bop/FSFhYX6+vpGRkY1yYNdsQAAuis/Pz8/P591CgANpqdHUVE0bx61aEENGlCfPnThgjKt\nrrag2AEA6KirV696enpOmDCBdRAAzVavHi1ZQnfvUno6HT9ObM84QrEDANA5HMetXbu2c+fO\nycnJjRo1EovFrBMBQO3AyRMAALolNzf3s88+279/v6WlZXh4+LBhw1gnAoBag2IHAKBDLl68\nGBQU9OTJk65du0ZERDg5ObFOBAC1CcUOAECHlJWVvXz5cvr06StWrDAwMGAdR+OVldGGDXT6\nNJWVUdeuNGMGLv0DjKHYAQDokB49ejx48MDFxYV1EG1QXk49e9KFC//cPHaMduygy5cJ13UG\nhnDyBACAbkGrqy1hYW9aHe/+ffr2W0ZpAIgIxQ4AAEA5p07JGTx5UuU5ACpAsQMA0E5PnjwZ\nP358UVER6yBaS+5VYnDpGGALxQ4AQAsdPnzYw8Nj+/btO3bsYJ1Fa3XpImewa1eV5wCoAMUO\nAECrlJaWzpgxY+jQoYWFhWFhYZMnT2adSGv95z/UosW/Rho0oKVLGaUBICK2Z8VeuHBhaaW/\nAb169ZoxY0bFkYMHD27fvl16U09P79ChQyqIBwCgce7fvx8UFHTt2rXmzZvv3bvX3d2ddSJt\nZm5OFy/S4sUUG0tlZdStGy1YQA4OrGOBbmNZ7D744IPQ0FDpTYlEEhYW1q5dO5nF0tPT27dv\nHxAQwN8UCAQqSwgg1+3b9NNPwuvXrRo0EAQF0YQJpKfHOhMAkUgk6tu378OHD8eNG7du3Tpz\nc3PWibSftTWtWME6BEAFLIudlZVV+/btpTdPnjzZrFkzX19fmcXS09Pd3NwqLgnA0MWL1LMn\nlZYKiPTv3KG4ODp3jnbtYh0LgEhfX3/dunXp6ekhISGsswAAG+pyjF1xcfG+ffs+//zzynel\np6c7ODiUlJTk5+erPhiAjAkTqLT0XyPh4bjAAagLf39/tDoAXaYuvzyxf/9+b29ve3t7mXGO\n49LT06OiolavXs1xnJOT07Rp09zc3OQ+CcdxeXl5dR9WG0gkktzcXNYpNE9mpuDOHTkXlT95\nsrRjxxLV59F0YrFYIBCUyjRlUIRYLCYifPXVkFgsxldiTXAcR0SlpaUikYh1Fg0mFovLyspK\nSt7xr4lQKKxXr97b7lWLYpeRkREdHb1u3brKd2VlZQmFwpYtW86fP18kEm3btu3HH39cv369\npaVl5YU5jisvL6/7vFoC20oJIpH8Qzw5ToztqTQxLvyllNzc3Pz8fEdHR8Jf59qAbVhzEolE\nIpGwTqHx3vmVqFflYd0CvmWztX79+qKioq+++uqdS5aWlo4dO3by5Mkffvih3AXUYXU0QnZ2\ntrW1NesUGqlePUFhoezghg3cpEks0mi44uJioVBoZGTEOojmuXLlSnBwsJWV1bFjxwQCgY2N\nDasTy/Ly6OlTatKEzMyYvH4t4DguNzfXysqKdRANVl5enpeXZ2JiYmpqyjqLBisqKtLT06vO\nV2IVf9/Zz9iVlZWdOXOmOq2OiIyMjOzs7HJyct62AM6ZrT5sKyXk5pLcy/inpWFzKg/bTiEc\nx61du/Y///lPeXn5tGnT+K0nEAhUvxmzsmjGDNq9mziOhEL69FNauZLevoNI3eFzWBPSrYfN\nWEM1/7vM/uSJy5cvE5GHh4fcexMSEqZOnSo9fKSoqCgjI8PZ2Vl1+QAqKCkhuZPCOEgMVOP1\n69cDBgz48ssvLS0to6Ki1qxZY2hoyCQJx1FICIWH//M3QiKhzZsJ10IGYI59sbt+/bqbm5vM\nDuPY2Nhjx44RUZs2bQoKClatWnX9+vXbt28vWbLEyckJlz4BVho0IBcXOeMdOqg8Cuie+Pj4\ndu3aRUdHf/jhhzdu3OjXrx/DMFevUlSU7GBEBN2/zyINAPw/9sUuMTGxhcxvshDFxcWdOnWK\niAwNDVesWGFkZLRy5crly5fb2dl9//33QiH72KCbBAJav1520N+fBg9mkQZ0TKNGjUQi0cKF\nC0+ePNmwYUO2YR48kD+OYgfAllqcPAGql5WVZWNjwzqFpvrzT/rhB+7mTbK15YKDhXPnEg4X\nVk5RUZFQKDQ2NmYdRGPk5ubKXBMgLy+vrKzM1tZWxcc2xcaSn5+c8fh4zZvA5jguJycH55PV\nRHl5eW5uromJiZnmnkSjBgoLC/X19Wt4Phn7kycANM6HH1KPHpLs7GxjY2P8ahOoktwrPTHR\ntSs1by47P+fuTp6ejAIBABGpw65YAADQOEZGtHcvNW36ZqRFC4qMxO8mAzCGYgcAoHYeP37c\nr1+/1NRU1kGq4u5Ot2/ToUO0ejX98QclJlKl46UBQNWwKxYAQL3s27dv4sSJubm54eHh8+fP\nZx2nKiYmOHMIQL1gxg4AlFFaSufO0eHDOAuyNpWUlMyYMWPkyJGlpaVhYWFq3uoAQA1hxg4A\nFHbxIo0dSykp/9wcOZK2bSMTE6aZNN/du3dHjhyZmJjo5ua2d+/etm3bsk4EAJoHM3YAoJjM\nTAoMfNPqiGjvXqrejwJCVSZMmJCYmDhhwoSEhAS0OgBQDoodAChm/356+VJ28LffqLCQRRot\nsnXr1j179mzevBk/ow4ASsOuWABQzLNncgbLyigj418XvwBFNW/evHnz5qxTAIBmw4wdAChG\n7q/lGhmRg4PKowAAwL+h2AGAYoYPJycn2cHPP8fJEwrIysr6+++/WacAAC2EYgcAirGyokOH\nqFWrNyOffEJLlrALpGnOnTvn7u4+cODA9PR01lkAQNvgGDsAUJinJ924QYmJlJFBrVqRoyPr\nQBqC47i1a9fOmTNHJBJNmzbNxsaGdSIA0DYodgCgDD098vBgHUKjZGRkhISEHD9+vEGDBjt3\n7uzTpw/rRACghVDsAADq3I0bN/r06ZOent6nT5+dO3c2aNCAdSIA0E44xg4AoM61aNGicePG\nCxcujI6ORqsDgLqDGTsAgDpnbGx86dIlAwMD1kEAQMthxg4AQBXQ6gBArtQTdQAAIABJREFU\nBVDsAABqGcdxrCMAgI5CsQMAqE3379/39vb+888/WQcBAF2EYgegsEuXaOBAoYeHTffupsuW\nUVkZ60AsFBVRbCxFRtLNm6yjqJNdu3Z5enomJCQcP36cdRYA0EU4eQJAMbGx5OdHRAIiwbNn\ndO0aXbpEhw6xjqVaZ87Q2LH05Mk/NwcPpvBwMjNjmom14uLiefPmrV271sTE5Ndff504cSLr\nRACgizBjB6CYSZNkRw4fpiNHWERh5NUrGj78TasjosOHaeZMdoEYSUqiwEBydiZXVxo9+o6n\nZ4e1a9e2atXq77//RqsDAFYwYweggIwMSkmRM37hAg0cqPI0jOzfTxkZsoPbt9OqVWRuziIQ\nC/fvU4cOVFDwz83k5GVEtyZNmhoWtsLY2JhpNADQaSh2AAp42wUrDA1Vm4Oply/lDJaX06tX\nOlTsZs160+qIiOgXouFOTgNQ6gCALRQ7UNjx43TyJBUUUMeONG7cW7uOVrK2po4dKT5edtzf\nn0UaRlxc5AwaG5ODg8qjsPP33zIDFkQDLl1ikgUA4A0cYweKmTiR+val1atp82aaMIG8vSk/\nn3Um1dqyhSws/jXy1Vfk48MoDQsjRlCTJrKD06eTiQmDMKzInaM1MlJ5DgCAf0OxAwUcPEib\nN/9r5MYNmjuXURpGWrWiu3fpP//h/PzKRo0SRUXR8uWsM6mWhQUdPkzu7m9GpkyhRYvYBVKt\njIyMgwcP9usn567+/VWeBgDg37ArFhRw+LCcwUOHaP16lUdhqmFD+uknSXZ2nrGxsbnuHFZW\nQbt2lJBA9+5RRga1bEm686P2p0+fHj169OvXr+PiEuLi2jx48OauQYPo44+ZBQMA4KHYgQIK\nC6s7CFpPKKSWLallS9Y5VEUsFv/444+LFi0iovnz53fs+MH167RhA126RKam1KcPBQeTQMA6\nJQDoPBQ7UICHBx08KDvo6ckiCoAKPXv2bPTo0WfOnHF0dIyIiOjWrRsRmZrS7NmskwEA/BuO\nsQMFzJhB77//rxFjY1q5klEaAJV49OiRh4fHmTNnBg4ceP36db7VAQCoJxQ7UEC9ehQXR2PH\nkq0tmZqSry/9+Se1b886FkBdatKkSc+ePVevXv3HH3/Y2tqyjgMAUBXsigXFNG5MO3eyDgGg\nQgKBYN++faxTAABUC2bsAAAAALQEih0AwBslJSWsIwAAKA/FDgDgH3fv3u3YseO6detYBwEA\nUBKKHQAAEdGWLVs8PT0TExPv3r3LOgsAgJJw8gQA6Lr8/PwpU6bs3r27Xr16u3fvHjVqFOtE\nAABKQrEDAJ129erVkSNHJicne3p6RkZGvi9zqUYAAI2CXbEKy8+nmzcpN5d1DgCoDTExMSkp\nKTNnzrxw4QJaHQBoOhQ7BRQU0MSJZGVFbduSlRUFB9Pr16wzAUDNfPXVV2fPnl21apWhoSHr\nLAAANYVdsQr4/HPatevNzchIys6mY8fwy98AGkwoFHbp0oV1CgCA2oEZu+p69OhfrY534gRd\nvMgiDQAAAEAlKHbVlZwsf/zBA9XmAABlpaWlLV++nHUKAIA6hF2x1WVnJ3/c3l61OQBAKSdP\nnhw3blx6enqrVq369evHOg4AQJ3AjF11tWtH3t6yg++/Tz16sEgDANUmEolCQ0P79u2bmZm5\ncOFCf39/1okAAOoKZuyqSyCgiAgaNIju3PlnpEkT2rePTEyYxgKAKj158mTUqFHnz593dnaO\niIjAeRIAoN1Q7BTw/vt0/TqdOkUPHpCLC/Xpg1YHoNays7Pbt2+fmZkZGBj422+/WVlZsU4E\nAFC3UOwUY2BAODgHQFNYW1vPnDnTxsZmypQprLMAAKgCih0AaLNvv/2WdQQAANXByRMAAAAA\nWgLFDgC0REFBQXFxMesUAAAsodgBgDa4c+eOj4/Pl19+yToIAABLKHYAoPHWr1/v6el569Yt\nQ0NDiUTCOg4AADM4eQJAYS9e0KpVwuvXLezshKNG0cCBrAPpsLy8vIkTJ+7du9fS0nL79u0j\nR45knQgAgCUUOwDF3LpFnTtTfr6AyJCIIiNp1ixauZJ1LJ0UHx8fFBSUmprq4+OzZ88eFxcX\n1okAABjDrlgAxXz6KeXn/2tk1So6f55RGkY4jvbsocGDqXNnmjiRHjxgEyMrK+vp06fTp0+P\ni4tDqwMAIKVn7MrLy6Ojo/X19X19fc3MzGo3E4Days6mv/+WM37iBOnUT1VNm0br1v3z54sX\nadcuioujjh1VHaNv375JSUmurq6qfmEAAHVV3Rm7wsLCTz/9tE2bNkTEcZy/v//gwYMHDBjg\n5eX18uXLukwIoEbKy+WPi0SqzcHU+fNvWh2vpITGj2cTBq0OAKCi6ha7hQsXbt261cPDg4hi\nYmL+/PPPmTNnHjx48OXLl4sXL67LhABqpEEDklskdGq67vRpOYNJSfTihcqjAADAv1W32P3+\n++8BAQE7d+4koiNHjtjY2CxZsmTIkCEBAQEnTpyoy4QA6uXXX2VHhg6l/v1ZRGHkbZcT4bg6\nfNFnz559/vnnpaWldfgaAACar7rFLi0tzdvbm//zmTNnevbsaWRkRERt27Z99uxZXaUDUD89\ne9Lff9PAgZyTk9jTU7J8Oe3ZwzqTavXoIWfQ1ZUaN66rV4yKinJ3d9+wYQP/f0sAAHib6ha7\nxo0bJyYmEtHdu3dv3LjRq1cvfvz+/fsNGjSoq3QAasnbmw4dkly9mh0XV/TVV2RoyDqQavXo\nQZ988q8RIyPasqVOXkskEoWGhg4aNCgvL2/p0qUTJkyok5dRSloazZhBHTvShx/S0qVUUsI6\nEABA9c+KDQwMXL169cyZM2NjYw0NDfnv2bCwsB07dvwfe/cd31S9/3H8k3TTFjrYo1QBQbGW\ntugtskWRoSCgtIyyvAyvLK8LB8Or/qQq8yqKgqCMgiCIchEUBRFxsMqSPZQhZXTPpMn5/VFG\nR1pOmzQnSV/PP+6j+TQk7xvb9J0zvocVQYGq5uOPpU0bWblSkpIkLEwmTZKwMNs/y59//hkb\nG/vrr7+GhoYmJCRER0fb/jkq6vx5iYiQy5ev3dyyRdatk23bxMND01gAqjydou64mIyMjMGD\nB3/99dd6vX7mzJnjx4/fvXt369atmzdvvnHjxtDQ0ErOCRtLTk4OCgrSOoUTM5lMKSkp3t7e\nfn5+WmdxYtnZ2Xq93tvbu+S3jEZjs2bN/vzzzwEDBnz44YfVq1e3f7wyxMbKypXFh7Nmif2v\nVZuenm4wGIKDg3U6nb2f21UoipKamhoYGKh1ECdmNBrT0tJ8fHxYAc0aWVlZ7u7uBYe6VZja\nLXb+/v7r1q1LS0tzd3cv+M8WGhq6ZcuW6Ohoi2/KAGANDw+PWbNmJScnP/nkk1pnsWDrVstD\n+xc7ACisfAsU6/X6HTt2XLlypXPnzv7+/h07duQzIoBK0qdPH60jlA9vhwA0V45Lis2fP79e\nvXpdu3YdOHDg0aNHv/zyy8aNG69atarywgGAY+rcWe0QAOxJbbFbv379mDFj7r333oTrSzu0\nbt26fv36MTExrGMHwErp6enOdQ2bGTOk2HoAbdvKv/6lURoAuE7tyRPt27fPzMzctWuXm5ub\nTqfbunVrx44dDQZDmzZt/P39t1o83gQOjJMnrMTJEzZRcPLEH3/8ERMTExQUtH37dg/nOa30\n0iWJj5dffxUfH3n4YRk/Xqw74rmCOHnCepw8YT1OnrAJu548sW/fvueee87Nza3w0NPTMyYm\n5q233rImAYAqS1GU995775VXXjEajY8++qjKz5kOonZtmTFD6xAAUJTaYhcYGJhraf3NCxcu\n+Pv72zRSxSmKwhWHVFIUxeJ/UKhkNptFxGQy8TJWWHJy8j//+c9vvvkmODj4o48+6tGjh9ls\n5vUsr4IfRd76rKEoCm+JVjKZTCKSn5/Py2gNk8lU8NNY9t10Ol0ZW/XUFrvo6OglS5a88MIL\nAQEBN4anTp1asWJFu3btVD6IHRT8bEENXitrFPziKYrCy1gxO3fuHDx48Pnz5++///5FixY1\naNCAV7JiCn4UefWsUfCnlNfQGgUfMHgZrXRjk0HZd9PryzpBQm2xi4+PDw8Pj4iIGDlypIhs\n3rx5y5Yt8+fPz87Onj59usoHqWw6nY69+yrl5eXxWlWYySRr15p//11fv75bnz5ejRtrHcgJ\nNWzYMDMz81//+tf06dMdZ6u/MzKZTCaTqVq1ahxjV2GKohiNRt4SrWE0GvPy8jw8PHgZrWGT\nY+zUnjwhIgcOHBg/fnzh8yQeeuihd955Jzw83JoE0AQnT1TYlSvy4IOyb9+1m9WqyYcfSlyc\nppmc09WrV318fEq78gRU4uQJ63HyhPU4ecIm7F3sCqSkpBw9etTT07NJkyY1atSw5rmhIYpd\nhT3+uHzxRZFJtWqSmCjNmmkUyJmVcUkxqESxsx7FznoUO5uw61mxNwQGBjrUpbgBe8rKki+/\nLD7MzpYvvpBJk7QIBO3k58uSJfLbb1KtmnTtKt26aR0IANQXu7CwsDK+e+DAAVuEARxderpY\nPKo1OdnuUZzHmTNnxo8fX3DpGq2z2ExWlnToIHv2XLs5a5YMHy6ffKJpJgBQf+WJ0KIaNWpk\nMpkOHjx4+vTpLl26VGpEwHHUri3BwRbmd91l9yhOYs2aNREREV9//fWiRYu0zmJLL798s9UV\nWLRIVqzQKA0AXKd2i93XX39dcrh169ZHHnnk6tWrNo0EOC43N3njDXnqqSLD8HAZMECjQA4s\nLy/vhRdemDt3rre39+zZsydMmKB1IlsquUdeRNauldhYu0cBgELUbrGzqFOnThMnTly6dCnd\nDlXHmDHy3/9KrVoiIu7u0revrF+vzbWkHNnRo0ejo6Pnzp3bvHnzX3/91cVanYhkZakdAoA9\nWVXsRKRJkyY6nc7Hx8cmaQCnMHas/P236cCB5IsXM7/4Qho21DqQ44mLi0tMTBw6dOju3btd\nckWkiAgLw8hIu+cAgKLKfVZsYSaTae3atQ0bNqxWrZqtAgHOom5dMxvqSrNw4cLExMQ4113f\n7913pU0bycm5OQkNlWef1S4QAIiI+mL36KOPFpsoinLkyJGTJ0/++9//tnUqx5WVJWvWyMmT\nEhIifftKoeurAbgpLCys7FPpnV14uGzZIi+9JL/9Jt7e0q2bvPWWsLInAM2pLXbnzp0rOaxd\nu/aAAQNeffVVm0ZyXAcPSqdOcuN4wokTZf166dBB00wANPKPf8gPP2gdQmt//y3Llslff8lt\nt0lcnNSsqXUgoMpTW+z27t1bqTkcn8kkDz0khc8SyciQRx6RCxfEz0+7WIDWUlNT//zzT5c8\nkA5l27hRHntM8vKu3Xz1Vdm0Sdq10zQTUOVZe/JE1fHjj3LxYvFhRoYsX65FGsAx/PrrrxER\nET179uTU+KomI0P69bvZ6kQkO1seeaTIBID9lbXFrk2bNiof5ZdffrFFGIe2e7fleZXflIkq\nSlGUuXPnvvDCC0ajcdy4cdWrV9c6EexqxQrJzi4+TEuTDRukTx8tAgEQkbKLnbu7VefMupgm\nTSzPQ0PtGgNwBJcvXx42bNiGDRtq1ar16aefdu/eXetEsLdDhyzP9++n2AFaKqu6/fTTT3bL\n4fh69BAfnyKrG4iIu7sMH65RIEAjiYmJPXr0+Pvvvx988MElS5bUrVtX60TQQPPmluctW9o3\nB4CirD3G7vvvv+/WrZtNojg4b29ZulT0RV+w2bOldm2NAgEaadq0aWBg4NSpUzdu3Eirq7IG\nDhRv7+JDPz955BEt0gC4rhw7Wz///PPNmzfnFN1mtWPHjszMTFunclD794vZXGRy8KBGUQDt\n+Pn57dmzx4vVmau2GjVk6VIZMECMxmsTT09Zs8ZC2wNgT2qL3UcffTR69Ojq1avn5+dnZ2c3\nbtzYZDKdP3++Tp06M2bMqNSIDiIpSd54o/jwww/l6afl7ru1CARoh1YHEenXT44dk8WL5cQJ\nad5cRoyQBg20zgRUeWqL3bx588LDw3///ffLly+HhIT8+OOPjRs33r59e58+fTp16lSZCR3F\nwYNiMlmY791LsQNQRYWGyrRpWocAUIjaY+xOnjzZrVs3T0/PBg0atGrVateuXSLSrl27vn37\nvvjii5WZ0FGUdjlcVieGC/vzzz87dOhQFdYzAgDXoLbY+fj4mK8fXxYREbF9+/aCr//xj3/c\n+Nq1RUVJSEjxYVCQVI3tlaiKVqxYcc899/z0009r1qzROgsAQBW1xe7OO+/ctGmTwWAQkfDw\n8HXr1hXMjx49mp6eXlnpHImnpyxbJoUXYfXxkUWLJDBQu0xA5cjNzZ0wYcKAAQMMBsPs2bPf\neeedkvfZsEGGDpUePeT55+X8eftnBABYoPYYuxdffLFnz57NmjX7448/2rVrN2HChDFjxrRo\n0WLhwoXqL1Dh7Nq1k6NHZfFiOXZMQkNlyBBWJ4YLOnLkSExMzP79+++8886VK1eGhYWVvM9L\nL8n06de+/uYb+fBD2b5duFosAGhObbHr0aPH4sWLly9frihKRETEa6+99vrrrxuNxttuu23m\nzJmVGtGh1K0rkyZpHQKoTNOmTdu/f/+oUaNmz57t4+NT8g47d95sdQUyM2XYMC6vBwDa0ymK\nUtr3Bg0a9MQTT3Tr1s3b0sJEGRkZf/311x133OHh4VGZCVEpkpOTg4KCtE7hxEwmU0pKire3\nt5/LnT6TnJy8devWvn37lnaH//s/eeUVC/O//5byLlecnZ2t1+stvsNApfT0dIPBEBwcrNPp\ntM7irBRFSU1NDeTAGisYjca0tDQfHx9fX1+tszixrKwsd3d3K9eTKusYu+XLl/fp06dWrVoD\nBw5cu3ZtsaWJ/f39W7ZsSasDXExQUFAZrU7k5oK0KucAALspq9gdOXJk+vTpd99994oVK/r2\n7Vu7du0BAwasWbOmWMMDUKVYPKq2cWNp2NDuUQAARZVV7Jo3b/7iiy/+8ssv58+f//DDD9u1\na7dmzZp+/frVqlUrJiZm9erV2dnZdgsKwOauXLmyYcOG8v6rrl2lf//iw/nzhT2BAKA5Vcud\n1KtXb/To0d98882VK1dWrlzZq1evTZs2PfHEE7Vq1erfv/+qVasqOyUAm9u2bVtERES/fv2O\nHDlS3n+7dKnMmCH33ishIdKzp+zYIQ8/XBkZAQDlU9bJE2UwGo0//PDD//3f/23btk1EKvYg\n0BAnT1jJqU+eUBRl7ty5zz//fH5+/gsvvPDmm2+6ublpkoSTJ6zHyRPW4+QJ63HyhE3Y5OQJ\ntcudFHbgwIFVq1atWrWq4IN+y5YtrUkAwJ4uXbo0ZMiQTZs21alT57PPPuvatavWiQAANlOO\nYpeYmLh69epVq1YdO3ZMRJo2bfrqq6/GxsZS7ABnceLEibZt2166dKl79+6ffvpprVq1tE4E\nALClWxe7PXv2FGyfO3nypIiEhIQ8//zzsbGxkZGRlR8PgC3dfvvtrVu3vvfee6dMmaLXq72i\nIEqTkyOHD4uvrzRpIu4V2f8BADZW1lvRiy++uGrVqtOnT4tIvXr1xo8fHxMT06ZNG47kAJyU\nXq9fv349v8I28f778sorkpYmInL77TJ/vjz4oNaZAFR5ZRW7t99+u2bNmqNHj46JienYsSOf\n7wEXQKuziTVrZOzYmzdPnZI+fWTPHmnWTLtMAFB2sfvmm28efPBBd3YwAM7JYDB4enpqncI1\n/d//FZ9kZsqcOfLee1qkAYDrytoI161bN1od4KSOHj36j3/8Y/HixVoHcU2nTlkYnjxp9xwA\nUBR7VwEX9Omnn7Zu3ToxMXHXrl1aZ3FNdetaGNarZ/ccAFAUxQ5wKTk5OaNGjRo2bJhOp1uy\nZMl77BqsHKNHWxiOHGn3HABQFHtaAddx6NChmJiYQ4cORURErFy5shlH8lea8ePlyBH58MNr\nN319ZcYMadNG00wAQLEDXMmqVav++OOPcePGvfPOO1ZelAZl0+nkgw9k4kT5/Xfx9pb27S3v\nnAUAOyur2KUVLNCkQo0aNWwRBoBVJk+e3LFjx86dO2sdpKpo3lyaN9c6BAAUUlaxCwgIUPko\niqLYIgwAq7i5udHqAKAqK6vYvfvuuze+VhRl3rx5p0+f7tKlS0REhJ+f36FDh9auXRsdHT1p\n0qTKzwkAAIBbKKvYPfvssze+fv/995OSkn788ccOHTrcGO7bt699+/anLC7oBKAyXbp0adWq\nVU8//bTWQQAADkTtcieffPLJkCFDCrc6EQkPDx82bBgroAJ29sMPP7Rq1Wrs2LGbN2/WOgsA\nwIGoLXbHjx8PDg4uOQ8ICDhx4oRNIwEolclkmjZtWteuXS9fvjx16lSOqAMAFKa22N19991r\n1qzJysoqPMzOzl69evU999xTCcEAFHfu3LnOnTu/9tpr9evX/+GHH6ZNm+bm5qZ1KACAA1Fb\n7CZMmHD48OEOHTqsXbv2zJkzZ86cWbt2bYcOHQ4fPjxu3LhKjQg4puxsndlsv6e7evVqRETE\nTz/91Lt378TExPbt29vvuQEATkLtAsUxMTEXL16cMmVK3759bwwDAgLmzJnTv3//ysnmiLKz\nZc0aOXlSQkKkTx9RvSAMXMrixfLaa25nzgRXqyaPPy5vvy116lT6kwYHB48aNapOnTrjxo3T\n6XSV/nwAACekK9cSdFevXt26devx48fd3d2bNGnSuXNn9WvduYCDB6VnT/nrr2s3a9eWNWuk\nbVtNM1VUcnJyUFCQ1imc0uLFMnx4kcm998rPP4uHh0aBnFl2drZer/f29tY6iBNLT083GAzB\nwcHU/QpTFCU1NTUwMFDrIE7MaDSmpaX5+Pj4+vpqncWJZWVlubu7W3ndILW7Ygt4enpWr169\ncePGgwcP7tq1a5W64ITJJLGxN1udiFy6JLGxkpmpXSbYndksL75YfLhzp6xYoUUaAACKKkex\nmz9/fr169bp27Tpw4MCjR49++eWXjRs3XrVqVeWFcyiJiXLoUPHhuXOydasGYaCVS5fk0iUL\n84MHbfxE2dnZeXl5Nn5QAICrU1vs1q9fP2bMmHvvvTchIaFg0rp16/r168fExGzatKnS4jmQ\n5OTyzeGS/PxEb+mXxrYbrw8fPhwdHf1iyW2DAACUSW2xi4+Pb9Wq1ebNm2NjYwsmzZs337Zt\nW0RExFtvvVVp8RxIixaW53fdZd8c0JSfn/TsWXzo4yN9+tjsKT7++OOoqKgDBw7k5uZyFWYA\nQLmoLXb79u3r06dPsUWzPD09Y2Ji9u3bVwnBHE6jRvKvfxUf9usnrVtrkQba+egjad785k0v\nL5k9W+680waPnJGRMWjQoFGjRnl4eCxfvvzDDz/kcHgAQLmoXe4kMDAwNze35PzChQv+/v42\njeS4ZswQf3/5738lO1u8vGTECJk+XetMsLu6dWX/flm+3LxrV17dum79+3vecYcNHnb37t2x\nsbEnTpxo3bp1QkJC06ZNbfCgAIAqRu0Wu+jo6CVLlqSmphYenjp1asWKFffdd18lBHNE3t4y\nfbqkp8vZs5KZKfPmSfXqWmeCFjw9JS5OmTYta+JEg01anYicOXPm5MmT48eP//nnn2l1AICK\nUbvFLj4+Pjw8PCIiYuTIkSKyefPmLVu2zJ8/Pzs7e3oV22zl5iYNG2odAi6nX79++/fvv/vu\nu7UOAgBwYmq32IWGhm7fvj00NPSVV14RkTfeeOO1114LCwv76aef2LoA2AStDgBgJbVb7EQk\nLCxsy5YtKSkpR48e9fT0bNKkSZVaoBgAAMDBqd1iFxcXd+TIEREJDAyMjo6OjIwsaHU//fTT\n2LFjKzEg4HIuXLjw7LPP5ufnax0EAOBqblHsMjMzr169evXq1aVLlx47duxqUZcvX964ceOi\nRYvskxVwARs3bmzVqtXMmTOXLFmidRYAgKu5xa7YcePGLV68uODr3r17W7xP586dbZsJcEn5\n+flvvPHG66+/rtfrp06dOnToUK0TAQBczS2KXUxMTMEB3c8999xTTz3VpEmTYnfw8PAorfAB\nuOGvv/4aMGDAjh07GjduvHz58vvvv1/rRAAAF3SLYtetW7du3bqJyPr160ePHh0eHm6XVIBL\nycvLa9u27blz55544omPP/6Ys44AAJVE7VmxBefDzpkzp1WrVh07dhSRDz/8MDMzc+TIkfyV\nAsrm5eUVHx+fkZExevRorbMAAFyZ2rNir169GhkZOXHixD/++KNgcuTIkeeffz48PPzs2bOV\nFg9wEQMHDqTVAQAqm9pi9/zzz6ekpGzevPmpp54qmMyePXv79u3p6ekvv/xypcUDAACAWmqL\n3bZt20aOHNmlS5fCw7Zt244ePfrHH3+shGCAs8rMzLx8+bLWKQAAVZHaYpeSkuLn51dy7uvr\nm5mZadNIgBM7dOhQdHR0//79TSaT1lkAAFWO2pMnoqKiVq9e/fzzz1erVu3GMDc3d/Xq1ZGR\nkRV++jVr1txYJ09E3Nzc1q5dW+w+iqIsX758y5YtZrO5Xbt2Q4cOdXNzq/AzApXn/ffff+65\n5/Ly8saOHWsymfhBBQDYmdpi99prr3Xs2LFNmzbjx49v2bKlu7v7kSNHZs+efeDAgc2bN1f4\n6ZOSkiIjI3v16lVwU6fTlbzP559/vmHDhrFjx7q7u7/33nsiMmLEiAo/I1AZ0tLSRo8evXLl\nyho1anz66af9+/fXOhEAoCpSW+zatGmzdu3aZ5555p///OeNYYMGDZYtW2bNlSeSkpJatGhR\nxjY/k8m0YcOGIUOGtGnTRkRGjBjxwQcfDBw40Nvbu8JPag2jUb77Tk6elJAQ6dpVfHw0SQHH\nsnPnzieeeOLPP/+8//77ExISQkJCtE4EAKii1BY7EenZs+fDDz+8Z8+eEydOGAyGpk2bRkVF\n+VhXbZKSkjp16pSbm2s0Gv39/Uve4dy5cykpKVFRUQU3o6KisrM78nVlAAAgAElEQVSzT506\nddddd1nzvBVz8qT06iXX13uR226TL76QiAj7B4FjqVat2pUrV8aPH//uu+96eHhoHQcAUHWV\no9iJiLu7+3333XfffffZ5LkVRUlKSlq/fv2sWbMURWnUqNG4ceNatGhR+D7Jyck6nS4oKKjg\npp+fn5eXV0pKik0ClDOtDBhws9WJyOnT0r+/7N/PdruqrmXLlidOnKhbt67WQQAAVV1Zxe6h\nhx7y9fX98ssvC74u457fffddBZ47OTlZr9ffeeedr776an5+/qJFi15//fV58+YVvpRFRkaG\nl5eXXn/z7F0fH5/09HSLD2g2myuv8x086L5zZ/FrbJw4IV9/nd6li7GSnrTyKIpy9epVrVM4\nvby8vLy8PBHx8PDg9awARVFEJCsrS+sgTqzgNUxOTtY6iHPjLdEmcnJycnNztU7hxBRF0el0\nt1xsxM3NLSAgoLTvllXsUlNT8/PzC76ujDVNgoODV69efePm+PHj4+Lidu/e/cADD9wY+vr6\n5uXlFfxfLZjk5ORYXHhFRHQ6XeEKaFvJyZbPcLxyxU2vd76FLcxmc+W9VlVEwYImvIzWKCgl\nFs+agkpms1lRFH4OrcRbopUURSn4S83LaA2Vb4ll36GsYrdz584bX//yyy/lyVYRXl5etWrV\nSk1NLTwMDAxUFCU1NTUwMFBEcnJy8vLyCr4uSafTlfYt67VqZXkeEeEbGOhbSU9aeZKTkyvv\ntXJhZ8+effbZZ+fNmxcYGJiSkuLl5VXaxwyokZ2drdfrtToXyjWkp6cbDIaAgAD6cYUV/iuD\nijEajWlpad7e3r6+zvcH0XFkZWW5u7t7eXlZ8yBlNes01Sr23Lt373766adv7FfNzs6+dOlS\nsTMKGzduXKNGjb179xbcTExM9PHxadasWcWe0Rq33SZxccWHDz8s0dH2zwJtfPXVVxEREatW\nrVqwYIGIKIrWgQAAKKqsLXZl7MEtRqnQn7iwsLDMzMyZM2c+9thjHh4eK1asaNSoUcHSJ99/\n/73BYOjevbubm1uPHj2WLl3asGFDvV6/aNGirl27WllmK2zePPH2loULxWwWEYmJkffeEz4k\nVwVGo3Hy5Mlvv/22p6fn7NmzGzYcHxkphw/XDAxUYmLkP/8R1b8rAABUIl0ZnWzGjBk3vlYU\nZd68eadPn+7SpUtERISfn9+hQ4fWrl0bHR09adKknj17VuzpL1++vGDBgj/++MPNzS0yMnL4\n8OEFi55Mnjw5Kytr5syZBU+9ZMmSbdu2mc3mtm3bDh8+XNtd+BkZcvq0hIQ499/y5OTkG+ca\no2xnzpwZMGDAr7/+GhoaumLFir/++kex5Yc7dJAffhAuM1EB7Iq1XsGu2ODgYHbFVhi7Yq1X\nsCvWx8eHXbHWsMmu2LKKXWHvv//+Cy+88M0333To0OHGcN++fe3bt3/zzTfHjRtnTQjYX4WL\n3ZUrMnWqbNok2dly333y+usSFmbzdA5EUZTIyMjExMRBgwZ98MEHvr7+jRrJhQvF75aQILGx\nWuRzchQ761HsrEexsx7FziYq/Ri7wj755JMhQ4YUbnUiEh4ePmzYsMIXe4Vry8mRTp1k3jw5\neVL+/lvWrZPoaDl0SOtYlUmn03300UeffPLJ0qVL/f39L12y0OpE5PpRoAAAaEltsTt+/Hhw\ncHDJeUBAwIkTJ2wayaFt2SKPPSZhYdKzp3z1ldZp7O6//y1e47Kz5ZlnNEpjL/fee+/w4cML\nvq5WzfJRlZwaCwBwBGqL3d13371mzZpi64hmZ2evXr36nnvuqYRgjmjxYnngAVm3Tg4elA0b\npHdviY/XOpN9/f67heFvv9k9h3aqV5cHHyw+9PaW3r21SAMAQFFqi92ECRMOHz7coUOHtWvX\nnjlz5syZM2vXru3QocPhw4eryAF26elS8v/olCny559apNGIxf3+rnR8VFpa2uHDh8u+z4IF\n0qhRkcmbb0qV+XQDAHBoaq8VGxMTc/HixSlTpvTt2/fGMCAgYM6cOf2LnSLoovbskZJX3zAY\nZMcOadxYi0Ba6NlTli8vPnzkES2iVILdu3fHxsYaDIa9e/eWcWZJSIgcPiwLF5p37TLUrq0f\nNMgzIsKeMQEAKJXaYiciEyZMGDx48NatW48fP+7u7t6kSZPOnTurX+vO2ZV2wlmVunrKwIHy\n1VeycuXNSbNm8u672gWyEUVR5s6d+8ILLxiNxnHjxt3yYhK+vvL000pKSqa3t7efn6d9QgIA\ncEvlKHYi4unpWb169caNG3fu3Nnf379atWqVFMsBRUVJ9epy/TIZ13h6Srt2GgXSSEKC9Osn\nGzdKZqZER8uYMeLjo3Um61y5cmX48OHr16+vWbPm4sWLK7woIwAAmitHsZs/f/6zzz5bcP7E\n1q1bz50799JLL82YMeOJJ56otHgOxM9P7rlHtm8vMrz9dqlXT6NAGtHp5IknxGX+m+/du/fR\nRx89f/58p06dli1bVr9+fa0TAQBQcWr3I65fv37MmDH33ntvQkJCwaR169b169ePiYnZtGlT\npcVzIKdOFW91InLkiPz6qxZpYCOhoaFeXl5Tp07dvHkzrQ4A4OzUFrv4+PhWrVpt3rw59vr6\n+s2bN9+2bVtERMRbb71VafEcSGmr9R0/bt8csKnAwMADBw5MmzbNjSuCAQCcn9pit2/fvj59\n+hT74+fp6RkTE7Nv375KCOZwate2PK9Tx745YGtV6lBRAIBrU1vsAgMDc3NzS84vXLjg7+9v\n00gOKjxc7ruv+LBZM+nUSYMwAAAAJaktdtHR0UuWLElNTS08PHXq1IoVK+4r2XdckU4ny5dL\ny5Y3J7ffLp9/7lLL87q2v/76q0uXLnu5qisAwHWpPSs2Pj4+PDw8IiJi5MiRIrJ58+YtW7bM\nnz8/Ozt7+vTplZnQgTRpIomJsnmzHD8uoaHy0EO0OqexevXqkSNHpqamJiQkRLCgMADARekU\nRVF51wMHDowfP37r1q03Jg899NA777wTHh5eKdFQmZKTk8u4uIIryc3NffHFF+fOnevt7T19\n+vQJEybY5GFNJtOFC6kBAV7+/rdYzdhVHT0qa9dKUpKEhcnAgRX8kJOdna3X6735hGSF9PR0\ng8EQHBysK20VddyKoiipqamBgYFaB3FiRqMxLS3Nx8fH19dX6yxOLCsry93d3cvi5TtVK8c6\ndmFhYVu2bElJSTl69Kinp2eTJk1q1KhhzXMDle3o0aMF5/e0aNFixYoVtvoQsmSJ/Oc/bidO\nBPv5KY8/LvHxpZ5b46oWLJCxYyUv79rNN9+UH3+Uhg01zQQAUHmMXWpqatOmTRcuXCgigYGB\n0dHRkZGRtDo4vkmTJu3bt2/48OG7du2yVav77DMZMuTa8jeZmbrFi6VXLzEabfLYzuHYMRk/\n/marE5FTp2TECO0CAQCuU1XsAgICmjRpsr3k+ryAY/vwww8TEhI++eQTW+0dMJvl2WeLD3/7\nrcj1c13eV19JTk7x4ebNkpysRRoAQCFqz4p97733du7cuWDBApPJVKmBABuqU6fOjSW1beLS\nJblyxcL8229t+CSOrtgVkwsoiuU5AMCe1B5j9/LLL9evX3/kyJHPPPNMSEhIsTVdd+7cWQnZ\nAIdT2uea8+ftm0NTYWEWhsHBHGMHANpTW+yuXLkiIp1YjReOKiUlZc+ePV26dKnUZ3Ev5Tfm\n9tsr9WkdS9++0q5d8Usnx8eX+uIAAOxG7Tvxli1bKjUHYI0dO3YMGDDg6tWriYmJTZs2rbwn\nql1bGjaUc+eKz3v0qLzndDhubrJ2rbz0knz+uaSnS5Mm8uqrMmyY1rEAAGqOscvMzNy9e/eO\nHTtSUlLsEAgoF0VR5syZ06lTp7Nnzz755JONGzeu1KfT6WT+/OLDbt3ksccq9WkdTs2a8vHH\nkpYmmZly4gStDgAcRVnFTlGUqVOnBgcHt27dum3btrVq1Xr55Zfz8/PtFg4o2+XLl3v27Dlx\n4sSAgIANGzbMmTPHw8Ojsp+0Rw/5/nvp2FEJClKaNTNPmSJffCFVdmlY1iIFAIdS1q7YhQsX\n/uc//6lfv36/fv10Ot3q1avfeuutmjVr/vvf/7ZbPqA0x48f79Chw8WLFx966KElS5bUqVPH\nbk/9wAPSsaM5JSXF29vbz6+KXnkCAOCAyrqkWFRU1Llz5w4dOlSzZk0RSU5OvuuuuwICAo4c\nOWLHhKgULnBJMbPZ3K1bt/vvv3/KlCl6vdqFe2zFZDJR7KzHJcWsxyXFrMclxazHJcVsotIv\nKXbs2LEBAwYUtDoRCQoK6tOnz4IFC6x5PsBW9Hr9xo0b7V/pAABwWGX9UczMzKxd9BKYderU\nqcrH2CmKfPqpREZK9eoSFibvvSdV+MVwCLQ6AAAKu8VyJ8W27VfxTf1vvy2TJl37+uBBGTdO\nTp6UWbM0zVRlmEwmNzc3rVMAAODQ2OCh1pUrMnly8eHs2cIBh3Zw5syZdu3aLV++XOsgAAA4\ntFtssTtw4MCyZctu3Ny/f7+IFJ4UGDRokM2TOZp9+8RotDDftUtatLB7mqpk2bJlTz31VEZG\nRmRk5MCBA7WOAwCA47pFsVu3bt26deuKDQcPHlxsUhWKnY9P+eawXk5OzqRJk+bOnevj4zN7\n9uwJEyZonQgAAIdWVrFbsWKF3XI4vqgoqVdP/v67yLB6denYUaNAru7w4cMxMTEHDhy46667\nVq5ceffdd2udCAAAR1dWsYuJibFbDsfn5SVDhkh8fJFh795yfTUY2NiiRYsOHDgwZsyYWbNm\nsc4ZAABqlLVAMQrLyJAGDSQjo8jQ01NOnJBGjTTKZAXHX6DYYDD88MMP3bp10zqIZSxQbBMs\nUGw9Fii2HgsUW48Fim3CJgsUc1asWnv2FG91ImIwyM8/a5GmCvD09HTYVnfxorz8sn7AgOpj\nxnj/739apwEA4LpbnDyBG3JzLc+Tk+2bA1o7dEjatpW0NJ2Ip4gsWybPPSfvvKN1LAAA2GKn\nXmm7rNn7Yb0rV6588sknWqdQ68knJS2tyOTdd2XHDo3SAABQCMVOrdJ2eQcE2DeHy/nxxx9b\ntWr1z3/+c9u2bVpnubWUFPntNwvzTZvsHgUAgBIodmpFRYl7iR3XOp20aaNFGpegKEp8fHyX\nLl0uXrw4ZcqUtm3bap3o1iwuUl3GHAAAe+IYO7WuXpX8/OJDRZELFyQ0VIM8zi4pKWnIkCHf\nfvttnTp1lixZ8tBDD2mdSJXataVJEzl5svj8/vu1SAMAQFFssVPrxAnL82PH7JvDJVy6dCks\nLOzbb7/t2bPnwYMHnaXVFZg/v/ikd2955BEtogAAUBRb7NQqubmuQGqqfXO4hNq1aw8YMCAk\nJOTf//630y2+1aWL/PKLvP66sm+fUru2xMToJ07UOhMAACJCsVPPzc3y3N/fvjlcxZw5c7SO\nUHHR0fLVV2YWKAYAOBp2xapVq5bleb169s0BAABQCoqdWs2bWzgrVq8Xrk1/Szk5Ofml7ckG\nAAC2Q7FTa+9eC4fZmc3y++9apHEeR48ebdOmzdSpU7UOAgCA66PYqZWXZ3le2qXGICKLFy+O\niorat2/fpUuXlNKu3QEAAGyEYqdWcLDluaenfXM4iYyMjLi4uOHDh+v1+qVLl3788cdOd/Yr\nAABOh2Kn1s6dlud79tg3hzM4dOhQmzZtli5dGhkZuXv37kGDBmmdCACAKoFip5bBYHnOtaRK\nOnDgwKFDh0aNGrVjx45mzZppHQcAgKqCdezU6tzZ8typLppgJ7Gxsc2aNYuKitI6CAAAVQtb\n7NTS81KVB60OAAD7o62otXmz5fmmTfbNAQAAUAqKnVolVycuUNqlxqqIpKSkV1991Ww2ax0E\nAABwjJ1qd9xheX7bbfbN4Ui+++67uLi4pKSkFi1aDB48WOs4AABUdWyxU6u0s1+r5iXg8/Pz\np02b1q1bt6tXr06dOnXgwIFaJwIAAGyxU61BA8vzhg3tm8MBnD17duDAgdu3b2/UqFFCQkLb\ntm21TgQAAETYYqdey5YWVjwJD5f27bVIo52cnJzo6Ojt27f36dNn3759VbbVXbkiv/7qceyY\n3mTSOgoAANexxa4cliyR2FjZvv3azVatZOXKKndJMR8fn9deey03N3fs2LFaZ9FGfr48+6zM\nm+eWn19DRMLDZfFiadVK61gAAIjouDR7uSiK7N4tx49LaKjcd58TnxKbnJwcFBSkdQqnNGWK\nvP56kUlIiCQmSmCgRoGcWXZ2tl6v9/b21jqIE0tPTzcYDMHBwVyOucIURUlNTQ3kd9gKRqMx\nLS3Nx8fH19dX6yxOLCsry93d3cvLy5oHYYtd+eh00rq1tG6tdQ5oxGiUWbOKD//6SxIS5F//\n0iKQds6elf/9T5KSJCxMevUqdT0gAIA98WaMsmRlZRkMBj7I3nD5smRmWpifPm33KJpasUL+\n+U/Jyrp2MyxMNm+W2rU1zQQA4OQJlOHw4cNt2rQZPHgw++tvCAoSi9vISztp2iWdPl2k1YnI\ngQMycqR2gQAA11HsYNn8+fOjoqIOHDgQEhKSn5+vdRxH4e0tTz5ZfBgcLDExWqTRyJdfFml1\nBdavl9RULdIAAAqh2KG49PT0gQMHjhkzxsPDIyEh4YMPPvDw8NA6lAN55x157LGbNxs0kJUr\npV497QLZXXKyhaHZTLEDAO1xjB2K2LVrV//+/U+fPv2Pf/wjISHhtqp8xbRSVKsma9dKYqL5\nl1+y6td379LFp6pdfeTOOy0Ma9SoWvujAcAxscUOxV24cGH8+PHbtm2j1ZUhLEx54om8Ll1M\nVa3Vicjjj0tkZPHha68JG3YBQHMUOxTRunXr48ePz5kzx7OqrbxcHnv2yBNP6Nu0CXzwQZ85\nc0q9jrCr8vSUr76Sfv2uLXESHCwzZ8r48VrHAgCwKxYlNWrUSOsIDm3bNunYUUR0Im4nTshv\nv8nPP8vnn2sdy74aNJDVqyUvTy5froqXSwYAh0WxK5+MDFm5Uk6dkpAQefxxqVlT60Cwu5Lr\neqxaJRs2SI8eWqTRlJcXrQ4AHAu7YsshMVGaN5eRI+Wtt+Spp6R5c9m6VetMVjh//vyQIUPS\n0tK0DuJMLl+WY8cszG9cQRhVSmKifPSRLF0qf/2ldRQAEBGKnXomkwwYIH//fXOSnCwDB0pG\nhnaZrPDdd99FREQsWbLko48+0jqLMynt6sBcUKuqMZtl+HCJiJDRoyUuTpo3lzlztM4EAC62\nK1ZRlKySC6fayO7d+iNHqhUb/v23bNyY2727M63fm5+f//bbb8fHx+v1+pdeemn06NGZFi+S\nBUs8PSUiotrevcU/EbVvn5OZadIkklPLz8/X6XTOuAL2nDkeixffvAhJbq5MnCgtWuS0bWvv\nH4OCV6/y3vqqAkVRzGYz74TWMJvNImIwGLhSkTXy8/Pz8/ONtzojT6/XV6tWvJDc4FLFTqfT\neVm83pMt5OToLM4zMz28vErZjON4zp49O3jw4F9++aVRo0bLli2Ljo7WOpHzWbDA3KGDvvCf\n0aefNnfq5O5iv032oSiKXq93xlOwP/vMwn/uZcu8HnjA3sXOZDKZzWZPT0+dzvJ7FG5JURSj\n0Vh5fz6qgvz8fIPB4O7uzstoDUVR3NzcrLwogKv9Kaq8aySEhYleL2Zz8Xl4uJuHh3MUO0VR\nevfuffDgwf79+8fHx4eGhmqdyClFRsrhwzJjhrJnT37t2rqBA9379tVzVEPFGI1GvV7vjJc2\nuXzZwvDKFb2Hh71/Egr6nIeHB8WuwhRF0el0zvhz6Gic9NfZcRgMBoqd/TRoIM2by+HDRYZ1\n60pEhEaByk+n082bN+/w4cOjRo1KtnhZKKjTqJHMmGFOSUnz9vb2q4IrFEOkaVPZubP4sFkz\nLaIAQCFsZlDr9OnirU5ELl6UX37RIk1FtW/fftSoUVqnAJze5MnFJzVqyIQJWkQBgEIodmqd\nOGF5fvy4fXMAcACPPioLF0pw8LWbLVrIV1/J7bdrmgkAKHbq1apleV6njn1zqJaRkXHy5Emt\nUwAua8QIuXhRDh++tjm/QwetAwEAxU698HALi+wHBBRcXcrh7NmzJyoqqnv37hlOus4e4Azc\n3aVFC+E0JACOg2Kn1tWrcvFi8WFqqvz5pxZpyvTZZ5+1a9fuxIkT3bt3d8aFJAAAQMVQ7NTa\nt08srqK6a5fdo5QuLS2tf//+Q4cO9fT0XLly5Zw5c1hSCACAqoPlTtTy8Snf3P4SExN79+79\n119/tWvXbvny5Y0aNdI6EQAAsCu22KkVFSX16xcf1qjhQMfY1a9fPz8/f/z48T/88AOtDgCA\nKogtdmp5eclnn0mvXpKdfXPy0UdSs6amsQqpXbv2kSNH/P39tQ4CAAC0QbErhy5d5PBh+egj\nOXlSQkJk+HBp0ULrTEXR6gAAqMooduUTEiJvvKF1CAAAAEs4xq58kpJk+nQZOVLeeEPLhU7O\nnj3brVu3wyWvcQYAAKowil05/PyzNG8uL70kCxbI5Mly553y9dcaxPjyyy9btWq1adOmTz/9\nVIOnBwAAjopip5bRKIMGSVrazUlOjgwbJikp9suQl5c3YcKEvn37ZmVlzZ49+6233rLfcwMA\nAIfHMXZq7d5tYd9rcrJs3Sp9+tgjwLFjx2JjY/fu3XvHHXesXLmyVatW9nhWAADgPNhip1Zm\nZvnmNjdx4sS9e/fGxcXt3r2bVgcAAEpii51aTZpYnjdoYKcA8+fP37p1a1xcnJ2eD2U6flx+\n/92zXj239u3F21vrNAAAiAjFTr2zZy3PL1+2U4BGjRrR6hxBbq48+aQsX+4mUl1EbrtNPvtM\n2rXTOhYAAOyKVc9sLt8crmrSJFm+/ObN06elXz+5dEm7QAAAXEexUysyUvz8ig89PeX++23/\nXGlpadu3b7f948JqeXkyf37x4aVLkpCgRRoAAIqi2KlVvbrMnVt8OG2aNG5s4yf6/fffIyIi\nHnnkkT81XAEZpbh6VXJzLczPnbN7FAAASqDYlcPw4fL999Krl9x1l3TvLl9+KS+9ZMvHVxRl\nzpw57du3P3PmzNChQ+vVq2fLR4ct1Kwp1apZmIeG2jsJAAAlcfJE+TzwgDzwQKU88pUrV4YN\nG/a///2vZs2an376aY8ePSrlaWAdT08ZN07i44sMGzSQ2FiNAgEAUAhb7MrBbJYFC+Tuu6Va\nNWnRQmbNEqPRNo989OjR8PDw//3vf507d963bx+tzpG9/rqMHHnz5p13ytq1EhysXSAAAK7T\nKYqidQan8eab8uqrRSbjxlk48K4CjEZj586dH3zwwcmTJ7u5udngEW8lOTk5KCjIDk/kqv76\ny/Tbb1n16rlHR1dzZ8N3RWVnZ+v1em9WArRCenq6wWAIDg7W6XRaZ3FWiqKkpqYGBgZqHcSJ\nGY3GtLQ0Hx8fX19frbM4saysLHd3dy8vL2sehGKn1uXLUr++5OcXnx86JHfdZYPHz8/Pd7dj\nQaDYWclkMqWkpHh7e/uVPFkaqlHsrEexsx7FznoUO5uwSbFjV6xa+/dbaHUisnu3bR7fnq0O\nAAC4JIqdWikplucVWOfCzKLGAACgElDs1Cpt63L16uV7nD///LN9+/br1q2zPhIAAEBhFDu1\nfHwszz09y/Egn3/+eXh4+I4dOzZt2mSTVAAAADdQ7NSyeICdiKg8NC43N3fChAkxMTF5eXmz\nZ8+eN2+eDbMBAAAICxSrV9qu2Bo1bv1vjxw5Ehsbu2/fvhYtWqxcufKee+6xbTYAAABhi516\nUVHSsGHxYUCAdOp063/7wQcf7Nu378knn9y9ezetDgAAVBKKnVqenrJkSZHtdt7esnChqFkM\nLj4+ft26dQsWLKhm8TqjAAAAtsCu2HLo1EmOHpUFC+TECWncWIYNk6ZNVf1Db2/vXr16VXI6\nAABQ1VHsyqdBA5k6VesQAAAAlrAr1saSk5NXrFihdQoAAFAVUexs6eeff46IiBg0aNCvv/6q\ndRYAAFDlUOxsQ1GUOXPmdO7c+ezZs2PHjo2KitI6EQAAqHI4xs4GLl26NHTo0I0bN9auXfuz\nzz57+OGHtU4EAACqIoqdtf7++++IiIikpKSHH374s88+q127ttaJAABAFUWxs1a9evV69ux5\nxx13vPDCCzqdTus4AACg6qLY2cDChQu1jgAAAMDJEwAAAK6CYlc+eXl5ZrNZ6xQAAAAWUOzK\n4dixY23atJk+fbrWQQAAACyg2Km1dOnS1q1b79279+TJk1pnAQAAsIBid2s5OTkTJkyIi4vL\nz8+fP38+p0oAAADHxFmxt/DHH3/ExMQcPHjwrrvuWrly5d133611IgAAAMvYYncL27ZtO3jw\n4L/+9a/du3fT6gAAgCNji90tjBkz5p577rn//vu1DgIAAHALFLtbK9zqTp6U+fPlxAlp3FhG\njJCwMA1zAQAAFEGxK4dNm6R3b8nLu3Zz3jxZtEgGDtQ0EwAAwHUcY3fT5cuX33jjDUVRLH43\nN1cGDLjZ6kTEYJAnn5TLl+0UDwAAoGwUu2u2bNnSqlWryZMnf/755xbv8MsvkpJSfJibK199\nVenZAAAA1KDYiclkmjZt2kMPPZSUlDR16tTHH3/c4t0OHLD8z/fvr8RsAAAA6lX1Y+ySkpLi\n4uK+++67hg0bLlu2rEOHDqXdMySkfHMAAAA7q9LFLjs7OzIy8sKFC4888sjixYuDg4PLuHOH\nDuLuLvn5xee9elViQgAAAPWqdLGrVq3apEmTTCbThAkTdDpd2XcOCpKJE+Xdd4sMBw6UZs0q\nMSEAAIB6VbrYici4ceNU3tNkko0biw9/+kmyssTX18apAAAAKoCTJ9RKTJSDB4sPz56VLVu0\nSAMAAFBCFSp2OTk5GRkZFf7nycnlmwMAANhZVSl2R44ciY6OfvLJJyv8CP7+ludmc4UfEgAA\nwJaqRLFbuHBhVFTU/v37a9SokV/yvFZ1StvYp68SLyEAABFP5hcAABxpSURBVHACLn7yREZG\nxlNPPbVs2TJ/f/9ly5YNtOLCrkFBludlrpECAABgP65c7Pbs2RMTE3PixInWrVuvWLGiSZMm\n1jxaRISEhRW//kRIiHTqZM2jAgAA2Iwr70fMyso6c+bM+PHjf/75ZytbnYjo9ZKQIKGhNyd1\n60pCAmudAAAAR+HKW+zat29/5MgR6yvdDS1byh9/yLp1cvy4hIZK795SvbqtHhsAAMBarlzs\nRMSGra6Aj4/Extr2IQEAAGzDlXfFAgAAVCkUOwAAABeh8a5Yg8HwySef7N27Ny0trVmzZiNG\njLjtttuK3WfNmjWLFy++cdPNzW3t2rV2TQkAAOAMNC528fHxp0+fHj16dEBAwIoVK6ZNm/b+\n++/7+fkVvk9SUlJkZGSvXr0Kbup0Oi2SAgAAODoti92VK1d27tw5ZcqU1q1bi8iLL74YFxe3\na9euTkWXhktKSmrRokVkZKQ2KQEAAJyElsfYpaenN23atHnz5gU3vby8vL29U1NTi90tKSmp\nbt26ubm5GaVd1QsAAADabrG7/fbbZ86ceePmzp0709LSWrZsWfg+iqIkJSWtX79+1qxZiqI0\natRo3LhxLVq0KO0xK3wp2CqI18oaZrO54H95Ga1R8DLyGlpDURQRyc/P5zCVClMURVEUfg6t\nYTKZhLdEq5nNZpPJdMvXUKfTubm5lfrdgjcFbSmK8t13382fP//hhx8eNWpU4W9dvXp19OjR\n3bt379evX35+/qJFixITE+fNm1ejRo2Sj2M2m5OTk+2VGgAAwN7c3NwCAwNL+672xS4pKWnW\nrFlnzpwZOnRo9+7dy75zXl5eXFzcmDFjHnjggZLfVRQlKyurcmK6mry8PC8vL61TODFFUfLy\n8tzc3Dw8PLTO4sQKtjOV8dETt2QwGMxms7e3t9ZBnBtviVYym80Gg4G3RCupfEvU6/XVqlUr\n7bsanxV77NixyZMn33PPPfPnz7e4Ea4YLy+vWrVqlTwOr4BOpyt2Ri1KYzAYeK2sYTKZ8vLy\nPDw8eBmtkZ2drdfrKSXWSE9PNxgMvr6+7IqtMEVRjEYjv8vWMBqNBoPB09PTlwuoWyErK8vd\n3d3KzxhanjxhMpneeuutTp06vfzyy6W1ut27dz/99NPp6ekFN7Ozsy9duhQSEmLHmAAAAM5B\nyy12e/fuTU5ODg8PP3To0I1h/fr1g4KCvv/+e4PB0L1797CwsMzMzJkzZz722GMeHh4rVqxo\n1KgRS58AAACUpGWxO3funKIo06dPLzwcPXp0z549t27dmpWV1b17d09Pz3fffXfBggUzZsxw\nc3OLjIx84YUX9HquhAYAAFCc9idPQBPJyclBQUFap3BiJpMpJSXF29ub43KswTF21is4xi44\nOJhj7CpMUZTU1NQyTjPELRmNxrS0NB8fH46xs4bTH2MHAAAAG6LYAQAAuAiKHQAAgIug2AEA\nALgIih0AAICLoNgBAAC4CIodAACAi6DYAQAAuAiKHQAAgIug2AEAALgIih0AAICLoNgBAAC4\nCIodAACAi3DXOoCTOX9eFi6UkyclJESGDZMmTbQOBAAAcB3Frhx+/FEeeUQyM6/dfPddWb5c\n+vTRNBMAAMB17IpVy2CQwYNvtjoRyc2VESMkOVm7TAAAAIVQ7NTavVvOnSs+TE2VrVs1CAMA\nAFASxU6t7OzyzQEAAOyMYqdWWJi4WzoiMTLS7lEAAAAsodipVbu2TJlSfPj003LXXVqkAQAA\nKIGzYsvhlVekTh2ZPfvaciejR8uECVpnAgAAuI5iVw56vYwaJaNGaZ0DAADAEnbFAgAAuAiK\nHQAAgIug2AEAALgIih0AAICLoNhVREaG1gkAAABKoNiVg9Eob74ptWpJ9eoSGCiTJnHZCQAA\n4EBY7qQcXnpJZsy49nVqqsTHy/nzsmSJppkAAACuY4udWhcuyKxZxYdLl8revVqkAQAAKIFi\np9Yff4jZbGF+4IDdowAAAFhCsVOrenXL84AA++YAAAAoBcVOrchIadas+LBOHenUSYMwAAAA\nJVHs1HJ3l4QEqV375qRGDVm6tNQteQAAAHbGWbHlEBUlR49KQoIcPy6hoRIbW6TnAQAAaIti\nVz4BAfLUU1qHAAAAsIRdseVgMsncudKggbi5SZ068sYbYjBonQkAAOA6il05PPOMTJggFy6I\n2SyXLsnkyTJwoNaZAAAArqPYqXXxorz3XvHhF1/Inj1apAEAACiBYqfWunWiKBbmCQl2jwIA\nAGAJxU6t3FzL87w8++YAAAAoBcVOre7dLc8ffdS+OQAAAEpBsVPrjjssdLvISHnwQS3SAAAA\nlECxK4dVq2ToUNFff81695ZvvxWdTtNMAAAA11HsysHXVxYvlowMOXhQ0tPlyy8lOFjrTAAA\nANdx5Ylyq1ZNWrbUOgQAAEAJbLEDAABwERQ7AAAAF0GxAwAAcBEUOwAAABfByRPlc/myfPaZ\nnDolISEyaJA0bKh1IAAAgOsoduXw66/So4ekpFy7+frrsmpVqVekAAAAsDN2xaplNMrAgTdb\nnYhkZcmQIZKaql0mAACAQih2au3ZI6dPFx9euSJbt2oQBgAAoCSKnVoZGeWbAwAA2BnFTq27\n7xY3NwvzVq3sHgUAAMASip1adevKiy8WHz75pISFaZEGAACgBIpdObz2msycKY0bi4jUqyfT\npsl772mdCQAA4DqWOykHd3d55hl55hnJyxMvL63TaOeXX2TTJsnOlvvuk759Rc+nAwAAHAPF\nriKqcqt7/nl5992bN9u2le++Ex8f7QIBAIDr2NiCcvjf/4q0OhH5+Wd55RWN0gAAgKIodiiH\nVassDFeutHsOAABgCcUO5ZCernYIAADsj2KHcrC4tkt4uN1zAAAASyh2KIeJE6VRo+LD+Hgt\nogAAgBIodiiHwED54Qfp1Uu8vcXNTSIi5NtvpW1brWMBAAARYbkTlFfTprJunZhMYjSKt7fW\naQAAQCEUO1SEm5vlK+cCAAANsSsWAADARVDsAAAAXATFDgAAwEVwjF355OfLd9/JyZMSEiJd\nu3L2AAAAcCAUu3I4eVJ695ZDh67dvO02+eILiYjQNBMAAMB1OkVRtM5gM4qipKamVtqDy4MP\n+u/dW6QKN2li3r493cvL+V5Dk8nkxnmt1jGZTDqdTq/neIaKM5vNOp1Op9NpHcSJmc1mRVH4\ndbaGoiiKovC7bA1FUQp+nXkZrVFQyW75lqjX62vUqFHad12q2FWqxETLG+e++Ua6dbN7Gqsl\nJycHBQVpncKJmUymlJQUb29vPz8/rbM4sezsbL1e780xDVZIT083GAzBwcH04wor2CgQGBio\ndRAnZjQa09LSfHx8fH19tc7ixLKystzd3b28vKx5EJq1WpcvW54nJdk3BwAAQCkodmo1aWJ5\n3qyZfXMAAACUgmKn1u23y6BBxYddu0p0tBZpAAAASqDYlcMHH8iTT8qNA0OfeEKWLhWOEwUA\nAA6CVlIO/v6yYIGkpMjevXL1qnz+udSqpXUmAACA61jHrtyqV5dWrbQOAQAAUAJb7AAAAFwE\nxQ4AAMBFUOwAAABcBMUOAADARVDsAAAAXATFDgAAwEVQ7AAAAFwExQ4AAMBFUOwAAABcBMUO\nAADARVDsAAAAXATFDgAAwEVQ7AAAAFwExQ4AAMBFUOwAAABcBMUOAADARVDsAAAAXATFDgAA\nwEVQ7AAAAFwExQ4AAMBFUOwAAABcBMUOAADARVDsAAAAXATFDgAAwEVQ7AAAAFwExQ4AAMBF\nUOzKzWyWCxckP1/rHAAAAEVR7MohN1deekn8/aVBA/Hzk7FjJT1d60wAAADXuWsdwJk895y8\n//61r/Py5P33JSlJVq3SNBMAAMB1bLFT6+zZm63uhtWrZdcuLdIAAACUQLFT68gRy/M//rBv\nDgAAgFJQ7NQKDLQ8Dwqybw4AAIBSUOzUioiQli2LDxs2lE6dNAgDAABQEsVOLTc3SUiQRo1u\nTmrVkoQE8fPTLhMAAEAhnBVbDmFhcviwrFkjJ05I48bSt68EBGidCQAA4DqKXfn4+kpcnNYh\nAAAALGFXLAAAgIug2AEAALgIih0AAICLoNgBAAC4CIodAACAi6DYAQAAuAiKHQAAgIug2AEA\nALgIih0AAICLoNgBAAC4CIodAACAi+BaseVz5ozMny+nTklIiIwYIXfeqXUgAACA6yh25bB5\ns/TqJTk5127+97/y6acSE6NpJgAAgOvYFatWXp4MGXKz1RVMRo2SK1e0ywQAAFAIxU6tXbvk\n77+LD9PT5ccftUgDAABQAsVOrdxcy/PC2/AAAAA0RLFTKzxcPD0tzO+91+5RAAAALKHYqVWz\nprz+evHhM89I8+ZapAEAACiBs2LL4fnnpV49mTNHjh2T0FAZPVpGj9Y6EwAAwHUUu3LQ6SQu\nTuLitM4BAABgCbtiAQAAXATFDgAAwEVQ7AAAAFwExQ4AAMBFUOwAAABcBMUOAADARVDsAAAA\nXATFDgAAwEVQ7AAAAFwExQ4AAMBFUOwAAABchMbXilUUZfny5Vu2bDGbze3atRs6dKibm1sF\n7gMAAACNi93nn3++YcOGsWPHuru7v/feeyIyYsSICtwHAAAAWu6KNZlMGzZsGDJkSJs2be69\n994RI0Z8++23ubm55b0PAAAARNtid+7cuZSUlKioqIKbUVFR2dnZp06dKu99AAAAINruik1O\nTtbpdEFBQQU3/fz8vLy8UlJSynufGxRFycvLq9TMLkNRFDZ8WsNsNouIyWTiZbRGfn6+TqfT\nOoVzK/hR5K3PGoqi8JZoJZPJJCL5+fm8jNbIz88v+Gks+246nc7Ly6u072pZ7DIyMry8vPT6\nm1sNfXx80tPTy3ufGxRFyczMrKS0rofXynpGo9FoNGqdwulRSqzHr7P1eA2tx1ui9dS8gG5u\nbg5a7Hx9ffPy8hRFufGRPScnx8/Pr7z3uUGn0/n7+1dqZpeRmZlZ2ssINcxmc1ZWloeHh7e3\nt9ZZnFheXp5er/fw8NA6iBPLycnJz8/nrc8aiqJkZ2f7+vpqHcSJmUym7OxsT0/PMgoHbknl\nW2LZOzq0LHaBgYGKoqSmpgYGBopITk5OXl5ewdflus8NZW+cRGFZWVm8VtYwmUxZWVllf2zC\nLZlMJr1ez2tojYLtnZ6enuzUrjBFUXJycvg5tEbBdibeEq2Un5/v7u5u5Wuo5ckTjRs3rlGj\nxt69ewtuJiYm+vj4NGvWrLz3AQAAgGi7xc7Nza1Hjx5Lly5t2LChXq9ftGhR165dC4rq999/\nbzAYunfvXsZ9AAAAUJjGCxTHxsYajca3337bbDa3bdt2+PDhBfOtW7dmZWV17969jPsAAACg\nMN0tz6qFS0pOTr6xiAwqwGQypaSkeHt7cw6KNbKzs/V6PSegWCM9Pd1gMAQHB3OMXYUVPpIb\nFWM0GtPS0nx8fDgHxRpZWVnOfYwdAAD/397dB0VVvXEAP3d3YWV3dTEoKAEpdFnSHVegUUZe\ng1EQlxxmieWlGHVkIk2wMjKZpilsmlhxGyudakp6cXhZSjIXptp0LHzJQRBSKXzBgAoHWXkX\nyL2/P+50fxsoXEBZ9vb9/LXnnHvPfWAOx8d77rkLAHcREjsAAAAAnkBiBwAAAMATSOwAAAAA\neMLOu2LBXvCu/ymiKMrJyUkoFNo7EMcmFArxyP8UiUQi7IGbIoqiRCL8azglmBLvCqFQaPsd\nqpODXbEAAAAAPIGlWAAAAACeQGIHAAAAwBNI7AAAAAB4AokdAAAAAE8gsQMAAADgCSR2AAAA\nADyBxA4AAACAJ/BKRoBxfPnll/v372eLQqHwq6++GnEMTdMHDhw4cuSI1WoNDQ3NyMjAizrh\nLjp+/Phbb701ojI6Ojo7O9u2hstYBZiKoqKi5OTkWbNmMUUuUx+mx2mGxA5gHO3t7YGBgQkJ\nCUzxtt+UUFpaajKZNm/eLBKJ3n33XULI+vXrpzVK4LVHH330tddeY4tWq9VgMCxZsmTEYVzG\nKsCkXbhwoby8PDExkU3suEx9mB6nGRI7gHG0t7crlcrAwMA7HXDr1i2TyfT000+HhIQQQtav\nX793797U1FR27gOYIldXV9sR+O233/r5+UVGRo44bNyxCjA5dXV1VVVVp0+ftq3kMvVhepx+\neMYOYBzt7e2enp43b97s6em57QGtra0WiyUoKIgpBgUF9ff3X758eRpjhP+QgYGB0tLSZ599\ndnTTuGMVYHLEYrFSqYyNjbWt5DL1YXqcfrhjBzAWmqbb29u/+eab3bt30zTt7e393HPPKZVK\n22M6OzspirrvvvuYokwmE4vFFovFHvEC/5WVlT322GMeHh4j6rmMVYDJCQgICAgIuHjx4qFD\nh9hKLlMfpsfphzt2AGPp7OwUCAQBAQFFRUUff/yxr6/vG2+80dXVZXtMT0+PWCwWCP7/1+Ti\n4tLd3T3twQL/Xbt2zWQyabXa0U1cxirAXcRl6sP0OP2Q2AGMxc3NzWg0btiwwdXV1d3dfcuW\nLcPDwzU1NbbHSKXSwcFBmqbZmoGBAZlMNu3BAv8Zjcbg4GA3N7fRTVzGKsBdxGXqw/Q4/ZDY\nAUyAWCy+//77b9y4YVs5d+5cmqbZyoGBgcHBwblz59ojQOCzoaGhY8eOjd4zcVu3HasAdxGX\nqQ/T4/RDYgcwlpqamk2bNrELB/39/deuXfPx8bE9Zv78+XK5vLa2linW1dW5uLgsXLhwumMF\nvmP2JC5duvS2rVzGKsBdxGXqw/Q4/bB5AmAsKpWqt7e3sLBw7dq1Tk5OxcXF3t7ezOskzGbz\n0NBQXFycUChcvXr1559/7uXlJRAIPvnkk5UrV4rFYnvHDnxTV1enVCpHvNyVHYdjjFWAe2GM\nqQ/Tox0hsQMYi7Ozs16v/+ijj3bt2iUUCgMDA1966SXmQeCjR4/29fXFxcURQnQ63fDw8Ntv\nv221WlesWLFu3Tp7Bw48VF9fP3odlh2HY4xVgHvkTlMfpkc7omwfaQQAAAAAx4X/zAEAAADw\nBBI7AAAAAJ5AYgcAAADAE0jsAAAAAHgCiR0AAAAATyCxAwAAAOAJJHYAAAAAPIHEDgAcQ0tL\ni0AgoChqz549dgkgLCwsJCRkdH1VVRVFUbm5uaObGhsbKYpKT0+fXM8AABOFxA4AHENpaSnz\nQvXS0lLuZ1VVVa1bt663t/eexUViYmLc3NzKyspGN1VUVBBCtFrtvbs6AIAtJHYA4BhKSkpm\nz54dFxdXXV3d1tbG8axz587t379/cHDw3gUmEom0Wu2VK1fOnDkzoungwYMymSw2NvbeXR0A\nwBYSOwBwAFeuXDl9+nRCQoJOp6Np2mg02juif0lOTiaEjLhp9+eff546dUqj0cyaNctOcQHA\nfw4SOwBwACUlJYSQpKSk+Ph4oVA4et3z5MmTsbGx7u7uCoViw4YN169fJ4RERUW9+OKLhBB3\nd/ennnqKELJ06VKNRmN7okajUalUbNFkMkVGRnp4eMyZMycwMPDDDz/kEl5ERMSDDz44Iqqv\nv/6apumkpKQJ9TxuhM3NzSkpKQ8//LBcLo+IiDh8+DDb1NPTs3379oULF0okEj8/v23btvX1\n9XGJHwB4A4kdADgAZh121apVbm5uoaGhx48fb21tZVsPHz4cHh7e2tq6efNmjUZjNBqDg4Mt\nFovBYMjKyiKEVFRU7NixY9yrFBUVxcfHd3Z2ZmRkZGVlWa3WzMxMLo/0CQSCpKSkS5cu1dXV\nsZW267CT7nmEhoYGtVr9448/6nS6F154oaurS6PRfPDBB0xrenq6Xq9Xq9WvvPLKokWL9Hr9\nli1bJnoJAHBsNADAzPbrr78SQtLS0phiYWEhIWT37t1McXh4WKFQqFSq3t5epua7774jhBgM\nBpqm9Xo9IaSjo4NpUqvVa9asse18zZo1ixcvZj6vXLnSx8dncHCQKQ4ODs6ZM2fjxo1MMTQ0\ndPny5XcKsrq6mhCyY8cOptjd3e3s7MwsHE+o57EjjIqKmj9/vsViYX/2iIgIqVTa3d1948YN\niqJycnLYE9PT01Uq1Z0CBgBewh07AJjpiouLCSHsmuYTTzxBbB5oO3PmzG+//ZadnS2VSpma\nmJiY999/X61WT/RC5eXl58+fd3Z2ZoodHR1///33wMAAl3NDQkJ8fHzYqCorK4eGhtiYp9Iz\ny2KxHDlyZOPGja6urkyNSCR65pln+vr6Tp48KRKJBAKB2Wxm8mBCyGeffVZfXz+hSwCAoxPZ\nOwAAgHEwS5ZNTU3vvfceU+Pq6nrixImWlhZvb++LFy8SQhYtWmR7CrMCO1Eymay2tra6uvrs\n2bO1tbV1dXW3bt3ieC5FUcnJyQUFBQ0NDSqV6uDBg1KplN0PO5WeWUzGlpeXl5eXN6Kpo6ND\nKpXq9frc3FylUqlWq8PCwhISEqKjoymKmuiFAMBxIbEDgBntl19+OXfuHCFk27ZtI5qMRuPW\nrVuHhoYIISLRJGcz2wQrPz//1Vdf9fHxWbt27csvvxwcHBwZGcm9K51OV1BQUFZWplQqTSZT\nfHy8RCKZes9shMwNv7y8vJiYmBHH+Pv7E0JycnKefPLJioqK77///osvvtizZ090dHRlZaWT\nkxP3nwIAHBqWYgFgRmP2wx44cMD2IZILFy6Qf+7kLViwgBDS2Nhoe1ZOTs7evXtv2yFN07bF\n5uZm5kNPT8/rr7+emZnZ3NxsMBi0Wq2vr++E7qsFBgYuWLCgrKzshx9+6OrqYtdhJ9rznSL0\n8/MjhIhEoggbnp6era2ts2fPvn79ek1NjUwmy8rKKi8v/+OPP3Jycsxms8lk4v4jAICjQ2IH\nADNaSUmJRCJJSEiwrVQqlUuWLDl16tTvv/8eFBT00EMPGQwG9i3E1dXV77zzju23TVitVuaD\ni4tLY2Mjm1QdPXqUyREJIVevXh0eHlYqlexZP/30E/c3ITN0Ol1jY+Obb74pkUhWr149iZ7H\niFAul8fExOzbt+/y5ctMzdDQUEZGxvbt2yUSSUNDQ3BwsMFgYJrEYnF4eDiZwr1MAHBE+IMH\ngJmrtra2qakpNTWV3RjB0ul0Z8+eNRqNzz///K5du9LS0pYvX67Vavv7+/ft2+fj45OZmUkI\nYVYhDQZDXFxcaGjo448/vnPnzsTExMTExEuXLhUWFrI9+/v7+/r65ufn//XXXwqF4ueffy4v\nL/fw8Dhx4oTZbI6OjuYSsE6ny8/PP3bsWFJSErsOO6Gex4iQEFJQUBAeHr5ixYqUlBRPT8+y\nsrKampri4mKKopYtW6ZQKHbu3NnW1qZQKOrr6ysqKvz9/SMiIib76wcAB2SfzbgAABzk5uYS\nQg4dOjS6iblrtWzZMqZoNpujoqJcXV3nzZuXmpp69epVpr65uTkyMlIikWzatImm6Zs3b27d\nunXevHnMloK0tLTs7Gz2ZSLnz5+PjY2Vy+Xe3t4pKSktLS2ffvrpAw88sGrVKnq8152wFi9e\nTP75ZlsW957HjpCm6aampsTERC8vL7lcHhYWVllZafs7SUtL8/LyEovFjzzySFZWVltbG9ff\nNQDwAkX/+2EOAID/gr6+voGBAXd3d3sHckczP0IAmIGQ2AEAAADwBDZPAAAAAPAEEjsAAAAA\nnkBiBwAAAMATSOwAAAAAeAKJHQAAAABPILEDAAAA4AkkdgAAAAA8gcQOAAAAgCeQ2AEAAADw\nBBI7AAAAAJ5AYgcAAADAE0jsAAAAAHjif+71+HLc5sCDAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "library(limma)\n",
    "library(pROC)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_predictions <- list()\n",
    "all_performances <- vector()\n",
    "all_top_markers <- list()\n",
    "all_auc <- numeric(k)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "     # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Check dimensions\n",
    "    print(dim(design_train))  # Dimensions of design matrix\n",
    "    print(dim(methylation_data_no_child_pugh))  # Dimensions of filtered data\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Store the top 100 markers for the current fold\n",
    "    all_top_markers[[i]] <- top_100_markers\n",
    "    # Export top 100 markers for the current fold to a CSV file\n",
    "    write.csv(top_100_markers, file = paste0(\"ChildPugh_top_100_markers_fold_\", i, \".csv\"), row.names = FALSE)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Store the predictions\n",
    "    all_predictions[[i]] <- data.frame(\n",
    "    PlasmaAlias = testing_set$plasma_alias,\n",
    "    Actual = testing_set$child_pugh_score,\n",
    "    Predicted = predictions\n",
    "    )\n",
    "\n",
    "    # Calculate and store the performance metrics for the current fold\n",
    "    performance <- postResample(predictions, testing_set$child_pugh_score)\n",
    "    all_performances <- rbind(all_performances, performance)\n",
    "\n",
    "    # Calculate and store the AUC for the current fold\n",
    "    roc_curve <- roc(testing_set$child_pugh_score, predictions)\n",
    "    auc_value <- auc(roc_curve)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Print AUC for the current fold\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    # Plot the actual vs predicted values\n",
    "    plot_data <- data.frame(\n",
    "        Actual = testing_set$child_pugh_score,\n",
    "        Predicted = predictions\n",
    "    )\n",
    "    # Calculate Pearson Correlation Coefficient R and R²\n",
    "    r_value <- cor(plot_data$Actual, plot_data$Predicted)\n",
    "    r_squared <- r_value^2\n",
    "    pearson_corr <- cor(plot_data$Actual, plot_data$Predicted, method = \"pearson\")\n",
    "\n",
    "    # Create the plot with y = x line and Pearson correlation\n",
    "    p <- ggplot(plot_data, aes(x = Actual, y = Predicted)) +\n",
    "        geom_point(color = \"blue\") +\n",
    "        # Add the y = x line\n",
    "        geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n",
    "        labs(title = paste(\"Fold\", i, \"Actual vs Predicted\"),\n",
    "             subtitle = paste(\"R =\", round(r_value, 2), \"| R² =\", round(r_squared, 2), \"| Pearson =\", round(pearson_corr, 2)),\n",
    "             x = \"Actual Values\",\n",
    "             y = \"Predicted Values\") +\n",
    "        theme_minimal()\n",
    "\n",
    "    # Display the plot\n",
    "    print(p)\n",
    "}\n",
    "\n",
    "# Print AUC values for each fold\n",
    "for (i in 1:k) {\n",
    "  cat(paste(\"AUC for fold\", i, \":\", all_auc[i], \"\\n\"))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd6d7f",
   "metadata": {},
   "source": [
    "# 100 Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5db9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "library(limma)\n",
    "library(pROC)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_predictions <- list()\n",
    "all_performances <- vector()\n",
    "all_top_markers <- list()\n",
    "all_auc <- numeric(k)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "     # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Check dimensions\n",
    "    print(dim(design_train))  # Dimensions of design matrix\n",
    "    print(dim(methylation_data_no_child_pugh))  # Dimensions of filtered data\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 50)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Store the top 100 markers for the current fold\n",
    "    all_top_markers[[i]] <- top_100_markers\n",
    "    # Export top 100 markers for the current fold to a CSV file\n",
    "    write.csv(top_100_markers, file = paste0(\"ChildPugh_top_100_markers_fold_\", i, \".csv\"), row.names = FALSE)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Store the predictions\n",
    "    all_predictions[[i]] <- data.frame(\n",
    "    PlasmaAlias = testing_set$plasma_alias,\n",
    "    Actual = testing_set$child_pugh_score,\n",
    "    Predicted = predictions\n",
    "    )\n",
    "\n",
    "    # Calculate and store the performance metrics for the current fold\n",
    "    performance <- postResample(predictions, testing_set$child_pugh_score)\n",
    "    all_performances <- rbind(all_performances, performance)\n",
    "\n",
    "    # Calculate and store the AUC for the current fold\n",
    "    roc_curve <- roc(testing_set$child_pugh_score, predictions)\n",
    "    auc_value <- auc(roc_curve)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Print AUC for the current fold\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    # Plot the actual vs predicted values\n",
    "    plot_data <- data.frame(\n",
    "        Actual = testing_set$child_pugh_score,\n",
    "        Predicted = predictions\n",
    "    )\n",
    "    # Calculate Pearson Correlation Coefficient R and R²\n",
    "    r_value <- cor(plot_data$Actual, plot_data$Predicted)\n",
    "    r_squared <- r_value^2\n",
    "    pearson_corr <- cor(plot_data$Actual, plot_data$Predicted, method = \"pearson\")\n",
    "\n",
    "    # Create the plot with y = x line and Pearson correlation\n",
    "    p <- ggplot(plot_data, aes(x = Actual, y = Predicted)) +\n",
    "        geom_point(color = \"blue\") +\n",
    "        # Add the y = x line\n",
    "        geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n",
    "        labs(title = paste(\"Fold\", i, \"Actual vs Predicted\"),\n",
    "             subtitle = paste(\"R =\", round(r_value, 2), \"| R² =\", round(r_squared, 2), \"| Pearson =\", round(pearson_corr, 2)),\n",
    "             x = \"Actual Values\",\n",
    "             y = \"Predicted Values\") +\n",
    "        theme_minimal()\n",
    "\n",
    "    # Display the plot\n",
    "    print(p)\n",
    "}\n",
    "\n",
    "# Print AUC values for each fold\n",
    "for (i in 1:k) {\n",
    "  cat(paste(\"AUC for fold\", i, \":\", all_auc[i], \"\\n\"))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac28d305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 AUC: 0.975609756097561 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 AUC: 0.632478632478632 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls > cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 AUC: 0.585365853658537 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 AUC: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Warning message in roc.default(testing_set$child_pugh_score, predictions):\n",
      "“'response' has more than two levels. Consider setting 'levels' explicitly or using 'multiclass.roc' instead”\n",
      "Setting levels: control = 3, case = 4\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 AUC: 0.752136752136752 \n",
      "AUC for fold 1 : 0.975609756097561 \n",
      "AUC for fold 2 : 0.632478632478632 \n",
      "AUC for fold 3 : 0.585365853658537 \n",
      "AUC for fold 4 : 1 \n",
      "AUC for fold 5 : 0.752136752136752 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nTWpISoZhAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "library(limma)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_predictions <- list()\n",
    "all_performances <- vector()\n",
    "all_top_markers <- list()\n",
    "all_auc <- numeric(k)\n",
    "aggregated_data <- data.frame()  # Empty dataframe for aggregated results\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "    \n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 25\n",
    "                               )\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Store the top 100 markers for the current fold\n",
    "    all_top_markers[[i]] <- top_100_markers\n",
    "    # Export top 100 markers for the current fold to a CSV file\n",
    "    write.csv(top_100_markers, file = paste0(\"ChildPugh_top_100_markers_fold_\", i, \".csv\"), row.names = FALSE)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Store the predictions\n",
    "    fold_predictions <- data.frame(\n",
    "        PlasmaAlias = testing_set$plasma_alias,\n",
    "        Actual = testing_set$child_pugh_score,\n",
    "        Predicted = predictions,\n",
    "        Fold = i  # Track the fold number\n",
    "    )\n",
    "    \n",
    "    all_predictions[[i]] <- fold_predictions\n",
    "    \n",
    "    # Append the fold predictions to the aggregated data\n",
    "    aggregated_data <- rbind(aggregated_data, fold_predictions)\n",
    "\n",
    "    # Calculate and store the performance metrics for the current fold\n",
    "    performance <- postResample(predictions, testing_set$child_pugh_score)\n",
    "    all_performances <- rbind(all_performances, performance)\n",
    "\n",
    "    # Calculate and store the AUC for the current fold\n",
    "    roc_curve <- roc(testing_set$child_pugh_score, predictions)\n",
    "    auc_value <- auc(roc_curve)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Print AUC for the current fold\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "}\n",
    "\n",
    "p_aggregated_single_color <- ggplot(aggregated_data, aes(x = Actual, y = Predicted)) +\n",
    "    geom_point(color = \"blue\") +  # Set a single color for all points\n",
    "    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +\n",
    "    labs(title = \"Child Pugh Actual vs Predicted Values (25 Markers)\",\n",
    "         #subtitle = \"All points in the same color across 5 folds\",\n",
    "         x = \"Actual Values\",\n",
    "         y = \"Predicted Values\") +\n",
    "    theme_minimal()\n",
    "\n",
    "# Display the aggregated plot\n",
    "print(p_aggregated_single_color)\n",
    "\n",
    "# Print AUC values for each fold\n",
    "for (i in 1:k) {\n",
    "  cat(paste(\"AUC for fold\", i, \":\", all_auc[i], \"\\n\"))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea3bfa5",
   "metadata": {},
   "source": [
    "# Pick up from here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744eec1a",
   "metadata": {},
   "source": [
    "Do the clasification from here -> find out if I can do rounding values e.g. 4.5=5 for the classification or would i have to do the thresholds myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15578683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35  9  2  1\n",
      "         2  6  9  6  1\n",
      "         3  1  6  4  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.1120          \n",
      "                                          \n",
      "                  Kappa : 0.3143          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.5524          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.3750  0.33333  0.00000\n",
      "Specificity            0.7000   0.7759  0.87143  1.00000\n",
      "Pos Pred Value         0.7447   0.4091  0.30769      NaN\n",
      "Neg Pred Value         0.8000   0.7500  0.88406  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4268   0.1098  0.04878  0.00000\n",
      "Detection Prevalence   0.5732   0.2683  0.15854  0.00000\n",
      "Balanced Accuracy      0.7667   0.5754  0.60238  0.50000\n",
      "Fold 1 Accuracy: 0.585365853658537 \n",
      "Fold 1 AUC: 0.698164682539682 \n",
      "Fold 1 Overall Sensitivity: 0.385416666666667 \n",
      "Fold 1 Overall Specificity: 0.836822660098522 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted\n",
      "1       6  8.586404\n",
      "2       3  2.238850\n",
      "3       3  4.546350\n",
      "4       5  5.171442\n",
      "5       5  7.095618\n",
      "6       4  6.954157\n",
      "7       3  4.838752\n",
      "8       5  6.169339\n",
      "9       5  4.908542\n",
      "10      3  3.866497\n",
      "11      3  3.266065\n",
      "12      3  3.055424\n",
      "13      5  4.556442\n",
      "14     10  8.090156\n",
      "15      3  3.068069\n",
      "16      3  2.265068\n",
      "17      6  7.987336\n",
      "18      3  4.114586\n",
      "19      3  3.405042\n",
      "20      3  3.191306\n",
      "21      3  4.466749\n",
      "22      3  3.038775\n",
      "23      3  1.092566\n",
      "24      8  5.676230\n",
      "25      3  3.483192\n",
      "26      3  3.770425\n",
      "27      3  5.011883\n",
      "28      3  3.755493\n",
      "29      3  3.143744\n",
      "30      3  4.250678\n",
      "31      5  1.865943\n",
      "32      3  5.110044\n",
      "33      3  1.948479\n",
      "34      3  2.387621\n",
      "35      5  7.703621\n",
      "36      3  7.627630\n",
      "37      6  5.819073\n",
      "38      6  7.377768\n",
      "39      3  1.657355\n",
      "40      5  3.469572\n",
      "41      5  6.138008\n",
      "42      6  2.961655\n",
      "43      3  1.263724\n",
      "44      7  5.094025\n",
      "45      7  2.717765\n",
      "46      3  5.014835\n",
      "47     11  6.884615\n",
      "48      9  6.386113\n",
      "49      3  5.901272\n",
      "50      3  0.942686\n",
      "51      3  3.676910\n",
      "52      8  8.123538\n",
      "53      6  2.076911\n",
      "54      6  6.890224\n",
      "55      5  6.448359\n",
      "56      5  3.095921\n",
      "57      7  7.869065\n",
      "58      3  3.497546\n",
      "59      3  2.587999\n",
      "60      9  4.559393\n",
      "61      9  7.308262\n",
      "62      9  6.819449\n",
      "63      3  4.098310\n",
      "64      9  7.152524\n",
      "65     10  4.457664\n",
      "66      6  6.572504\n",
      "67      3  2.397025\n",
      "68      5  6.651625\n",
      "69      5  2.197169\n",
      "70      3  4.152147\n",
      "71      6  6.319567\n",
      "72      5  7.818113\n",
      "73      8  6.919941\n",
      "74      5  3.288485\n",
      "75      3  5.377862\n",
      "76      3  3.394215\n",
      "77      7  5.770257\n",
      "78      3  2.665008\n",
      "79      3  4.851649\n",
      "80     11  8.702833\n",
      "81      3  2.450381\n",
      "82      3  4.689708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  6  0  0\n",
      "         2  6 14  6  1\n",
      "         3  2  4  7  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6707          \n",
      "                 95% CI : (0.5581, 0.7706)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.002659        \n",
      "                                          \n",
      "                  Kappa : 0.473           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.5833  0.53846  0.00000\n",
      "Specificity            0.8500   0.7759  0.88406  1.00000\n",
      "Pos Pred Value         0.8500   0.5185  0.46667      NaN\n",
      "Neg Pred Value         0.8095   0.8182  0.91045  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4146   0.1707  0.08537  0.00000\n",
      "Detection Prevalence   0.4878   0.3293  0.18293  0.00000\n",
      "Balanced Accuracy      0.8298   0.6796  0.71126  0.50000\n",
      "Fold 2 Accuracy: 0.670731707317073 \n",
      "Fold 2 AUC: 0.792302604802605 \n",
      "Fold 2 Overall Sensitivity: 0.48282967032967 \n",
      "Fold 2 Overall Specificity: 0.877480009995003 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted\n",
      "1       3 4.5146078\n",
      "2       4 6.5764557\n",
      "3       3 4.3547911\n",
      "4       3 3.6126528\n",
      "5       3 3.1822957\n",
      "6       3 2.4790334\n",
      "7       5 3.8549240\n",
      "8       3 4.0952849\n",
      "9       5 7.8454826\n",
      "10      4 3.0421551\n",
      "11      3 2.4874124\n",
      "12      3 7.3111463\n",
      "13      3 5.1260709\n",
      "14      6 6.5915240\n",
      "15     12 7.1777228\n",
      "16      6 6.9909734\n",
      "17      3 6.4162452\n",
      "18      5 5.4761906\n",
      "19      3 4.5145026\n",
      "20      3 5.1344562\n",
      "21      3 2.1543875\n",
      "22      3 1.7454996\n",
      "23      3 4.1347221\n",
      "24      3 3.5327682\n",
      "25      3 4.6657006\n",
      "26      3 5.2510298\n",
      "27      3 2.0169221\n",
      "28      3 3.9319198\n",
      "29      5 4.0551993\n",
      "30      4 8.5180038\n",
      "31      3 4.9308689\n",
      "32      3 2.6336685\n",
      "33      3 6.2549305\n",
      "34      3 3.4313455\n",
      "35      8 9.5316380\n",
      "36      6 8.6012766\n",
      "37      5 5.0966883\n",
      "38      3 4.1197139\n",
      "39      9 7.0972086\n",
      "40     12 8.3011047\n",
      "41      5 6.2593110\n",
      "42      7 6.2029282\n",
      "43      6 7.5729416\n",
      "44      5 3.9680996\n",
      "45      8 7.8730234\n",
      "46      5 1.9825954\n",
      "47      7 8.3276036\n",
      "48      5 5.4651266\n",
      "49      5 8.6925477\n",
      "50      3 4.1334983\n",
      "51      5 6.1417759\n",
      "52      8 6.3959460\n",
      "53      3 2.1716187\n",
      "54      5 6.1364556\n",
      "55      6 6.7552340\n",
      "56      6 6.6952572\n",
      "57      5 6.8816225\n",
      "58      6 6.2416929\n",
      "59      5 3.1793913\n",
      "60     11 5.1880504\n",
      "61      3 4.5190545\n",
      "62      3 1.6603956\n",
      "63      5 4.3563132\n",
      "64      3 2.0322043\n",
      "65      3 4.4804004\n",
      "66      7 6.2447147\n",
      "67      3 1.4879344\n",
      "68      5 5.5519188\n",
      "69      8 7.4637686\n",
      "70      8 7.6589666\n",
      "71      3 4.4718586\n",
      "72      3 3.3614448\n",
      "73      7 6.3274582\n",
      "74      3 3.0815158\n",
      "75      7 5.1266766\n",
      "76      3 4.3435823\n",
      "77      3 4.0671098\n",
      "78      3 3.2930426\n",
      "79      3 0.4839879\n",
      "80      5 5.1431591\n",
      "81      8 5.3196648\n",
      "82      8 8.9210976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37 13  6  0\n",
      "         2  5  9  4  3\n",
      "         3  0  2  2  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.112           \n",
      "                                          \n",
      "                  Kappa : 0.2679          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810   0.3750  0.16667  0.00000\n",
      "Specificity            0.5250   0.7931  0.95714  1.00000\n",
      "Pos Pred Value         0.6607   0.4286  0.40000      NaN\n",
      "Neg Pred Value         0.8077   0.7541  0.87013  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4512   0.1098  0.02439  0.00000\n",
      "Detection Prevalence   0.6829   0.2561  0.06098  0.00000\n",
      "Balanced Accuracy      0.7030   0.5841  0.56190  0.50000\n",
      "Fold 3 Accuracy: 0.585365853658537 \n",
      "Fold 3 AUC: 0.730696097883598 \n",
      "Fold 3 Overall Sensitivity: 0.355654761904762 \n",
      "Fold 3 Overall Specificity: 0.81881157635468 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual   Predicted\n",
      "1       6  1.53612000\n",
      "2       3  5.91794724\n",
      "3       3  2.48202340\n",
      "4       3  3.93087133\n",
      "5       3  5.27295866\n",
      "6       7  9.06291592\n",
      "7       5  4.04973943\n",
      "8       3  3.87102097\n",
      "9       3  6.26525546\n",
      "10      3  3.72160259\n",
      "11      4  3.93258938\n",
      "12      3  4.07982976\n",
      "13      3  4.46999884\n",
      "14      3 -0.06942872\n",
      "15      8  3.33364575\n",
      "16      3  0.64319724\n",
      "17      3  5.63598142\n",
      "18      5  4.78657479\n",
      "19      3  2.31141101\n",
      "20      7  5.26517194\n",
      "21      3  2.80168552\n",
      "22      3  2.40047014\n",
      "23      6  6.18106992\n",
      "24      8  4.60809576\n",
      "25      3  1.99428260\n",
      "26     10  6.20201558\n",
      "27      3  1.74433091\n",
      "28      3  3.21519635\n",
      "29      3  3.96750444\n",
      "30      3  2.29398033\n",
      "31      3  1.86419054\n",
      "32      6  6.78882902\n",
      "33      6  6.32569766\n",
      "34      7  4.00684707\n",
      "35      5  7.96836775\n",
      "36      3  3.53115944\n",
      "37      5  3.12045493\n",
      "38      5  1.96484848\n",
      "39      3  1.45387563\n",
      "40      6  6.16093802\n",
      "41      3  4.81000996\n",
      "42      8  4.55035707\n",
      "43      5  4.65617304\n",
      "44      5  5.79768265\n",
      "45     12  6.23325302\n",
      "46      5  6.56090919\n",
      "47      6  4.24203156\n",
      "48      5  5.87554763\n",
      "49      6  4.41295968\n",
      "50      3  2.69438560\n",
      "51      5  4.11446285\n",
      "52      5  4.26630966\n",
      "53      3  4.54690171\n",
      "54      7  5.26677694\n",
      "55      8  6.28226803\n",
      "56      3  2.45147769\n",
      "57      3  3.16078603\n",
      "58      5  7.17266743\n",
      "59      7  4.87200139\n",
      "60     10  8.00127483\n",
      "61      5  5.30459771\n",
      "62      3  4.01060016\n",
      "63      3  4.90969726\n",
      "64      5  1.34865828\n",
      "65      5  6.06763476\n",
      "66      3  3.94423731\n",
      "67      5  4.10746653\n",
      "68      3  2.30511496\n",
      "69      8  5.52865083\n",
      "70      3  4.66139887\n",
      "71      9  7.54797224\n",
      "72      3  4.94671851\n",
      "73      3  3.19875872\n",
      "74      3  2.65315050\n",
      "75      3  3.52362356\n",
      "76      3  2.38705521\n",
      "77      3  5.06873818\n",
      "78     10  6.65472719\n",
      "79      8  4.35008255\n",
      "80      3  3.02151817\n",
      "81      5  3.36434542\n",
      "82      3  3.21818583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 33  8  2  0\n",
      "         2  8  9  6  0\n",
      "         3  1  5  4  3\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5802          \n",
      "                 95% CI : (0.4654, 0.6891)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.1585          \n",
      "                                          \n",
      "                  Kappa : 0.322           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7857   0.3913  0.33333  0.25000\n",
      "Specificity            0.7436   0.7586  0.86957  0.98701\n",
      "Pos Pred Value         0.7674   0.3913  0.30769  0.50000\n",
      "Neg Pred Value         0.7632   0.7586  0.88235  0.96203\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4074   0.1111  0.04938  0.01235\n",
      "Detection Prevalence   0.5309   0.2840  0.16049  0.02469\n",
      "Balanced Accuracy      0.7647   0.5750  0.60145  0.61851\n",
      "Fold 4 Accuracy: 0.580246913580247 \n",
      "Fold 4 AUC: 0.817467506326202 \n",
      "Fold 4 Overall Sensitivity: 0.440087991718426 \n",
      "Fold 4 Overall Specificity: 0.839697159412302 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted\n",
      "1       3  4.313330\n",
      "2       3  4.539652\n",
      "3       3  4.631728\n",
      "4       4  6.922639\n",
      "5       3  4.853542\n",
      "6       6  7.316025\n",
      "7       3  4.200451\n",
      "8       4  5.459959\n",
      "9       3  7.343430\n",
      "10      3  5.014862\n",
      "11      3  4.823576\n",
      "12      6  3.563204\n",
      "13      3  2.794440\n",
      "14      3  1.903975\n",
      "15      6  5.386734\n",
      "16      3  3.277842\n",
      "17      5  7.575279\n",
      "18      3  4.450365\n",
      "19      3  2.312539\n",
      "20      9  8.161538\n",
      "21      3  3.192185\n",
      "22      3  3.319337\n",
      "23      3  1.804455\n",
      "24      3  2.840388\n",
      "25      5  5.006015\n",
      "26      3  6.563637\n",
      "27      3  4.786956\n",
      "28      5  6.208354\n",
      "29      5  3.626387\n",
      "30      3  3.749369\n",
      "31      3  5.073185\n",
      "32      3  4.115786\n",
      "33      8  6.742738\n",
      "34      3  4.479225\n",
      "35      3  3.468131\n",
      "36      3  4.230307\n",
      "37      3  3.022107\n",
      "38      3  3.433587\n",
      "39      5  5.077259\n",
      "40      5  8.167677\n",
      "41      6  5.092182\n",
      "42      8  4.535127\n",
      "43      9  5.494329\n",
      "44      6  5.514030\n",
      "45      9  6.428347\n",
      "46      8  6.275537\n",
      "47      3  3.753218\n",
      "48      9  5.259137\n",
      "49      7  8.478213\n",
      "50     11  9.187367\n",
      "51      8  6.264543\n",
      "52      6  7.924666\n",
      "53      5  4.275661\n",
      "54      5  5.023058\n",
      "55      3  3.967514\n",
      "56      3  4.278246\n",
      "57      3  5.384906\n",
      "58      5  6.920882\n",
      "59     12 10.504137\n",
      "60      5  3.108298\n",
      "61      5 10.692194\n",
      "62      5  4.321451\n",
      "63     10  8.294447\n",
      "64      3  3.197316\n",
      "65      5  1.425402\n",
      "66      5  4.759407\n",
      "67     11  8.509217\n",
      "68      5  5.709180\n",
      "69      3  6.544010\n",
      "70      3  4.498166\n",
      "71      3  2.654166\n",
      "72      3  5.115556\n",
      "73      3  3.284409\n",
      "74      3  4.170017\n",
      "75      6  3.123366\n",
      "76      3  3.536088\n",
      "77      7  4.605254\n",
      "78      9  7.907459\n",
      "79      9  8.356541\n",
      "80      6  8.319669\n",
      "81      3  3.273879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36  9  1  0\n",
      "         2  6  5  6  0\n",
      "         3  0  9  5  2\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5875          \n",
      "                 95% CI : (0.4718, 0.6965)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.1568          \n",
      "                                          \n",
      "                  Kappa : 0.3199          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.2174   0.4167   0.3333\n",
      "Specificity            0.7368   0.7895   0.8382   1.0000\n",
      "Pos Pred Value         0.7826   0.2941   0.3125   1.0000\n",
      "Neg Pred Value         0.8235   0.7143   0.8906   0.9747\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4500   0.0625   0.0625   0.0125\n",
      "Detection Prevalence   0.5750   0.2125   0.2000   0.0125\n",
      "Balanced Accuracy      0.7970   0.5034   0.6275   0.6667\n",
      "Fold 5 Accuracy: 0.5875 \n",
      "Fold 5 AUC: 0.834943639291465 \n",
      "Fold 5 Overall Sensitivity: 0.456133540372671 \n",
      "Fold 5 Overall Specificity: 0.841137770897833 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual  Predicted\n",
      "1       3  6.0115671\n",
      "2       3  4.2220313\n",
      "3       3  3.4624508\n",
      "4       3  4.8273377\n",
      "5       3  6.6950447\n",
      "6       3  4.9932358\n",
      "7       5  3.3798212\n",
      "8       5  7.8492929\n",
      "9       4  1.4274760\n",
      "10      3  4.2539116\n",
      "11      7  6.3450229\n",
      "12      3  1.7784142\n",
      "13      7  5.6315160\n",
      "14      3  1.7698328\n",
      "15      3  4.3419335\n",
      "16      8  5.9877573\n",
      "17      3  3.7428955\n",
      "18      5  6.1681106\n",
      "19      3  3.6087007\n",
      "20      3  2.1326893\n",
      "21      3  4.0553741\n",
      "22      3  6.0022415\n",
      "23      5  4.2935866\n",
      "24      3  2.8221544\n",
      "25      6  9.6837416\n",
      "26      6  5.8116334\n",
      "27      5  5.3786511\n",
      "28      5  7.0191108\n",
      "29      5  4.6116597\n",
      "30      3  4.0204475\n",
      "31      3  3.2525126\n",
      "32      4  4.8602159\n",
      "33      7  6.1229599\n",
      "34      3  4.1678666\n",
      "35      3  0.3336202\n",
      "36      5  3.7793994\n",
      "37      5  6.4689309\n",
      "38      7  6.9882447\n",
      "39     10 10.5387493\n",
      "40     10  7.9948061\n",
      "41      3  2.7037541\n",
      "42      3  4.4223567\n",
      "43      8  5.9970951\n",
      "44      3  4.7547385\n",
      "45      7  8.1305426\n",
      "46      3  1.2217954\n",
      "47      7  3.7256029\n",
      "48      6  7.0107744\n",
      "49     11  7.8140070\n",
      "50      3  2.8376885\n",
      "51      6  8.7901585\n",
      "52      6  7.8504806\n",
      "53      3  4.1279211\n",
      "54      3  4.1153760\n",
      "55      9  9.3570316\n",
      "56      3  4.3100292\n",
      "57      6  3.8833026\n",
      "58      3  4.0667764\n",
      "59      5  4.8926072\n",
      "60      5  7.4539351\n",
      "61      3  1.5530113\n",
      "62      3  3.2177917\n",
      "63      3  1.9202919\n",
      "64      9  7.6271800\n",
      "65      6  7.2606816\n",
      "66      5  3.9262762\n",
      "67      7  9.3916246\n",
      "68      3  2.7408500\n",
      "69      4  6.9367162\n",
      "70      3  0.6534420\n",
      "71      5  4.5901975\n",
      "72      6  4.6197961\n",
      "73      8  7.4147288\n",
      "74      6  6.4384773\n",
      "75      3  5.3699272\n",
      "76      3  5.9096986\n",
      "77      3  3.2802899\n",
      "78      3  3.4181398\n",
      "79      5  7.5032575\n",
      "80      3  3.6748416\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 175  45  11   1\n",
      "         2  31  46  28   5\n",
      "         3   4  26  22  10\n",
      "         4   0   1   0   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.602           \n",
      "                 95% CI : (0.5526, 0.6499)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.0002959       \n",
      "                                          \n",
      "                  Kappa : 0.3414          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.0032799       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.3898  0.36066 0.111111\n",
      "Specificity            0.7107   0.7785  0.88439 0.997429\n",
      "Pos Pred Value         0.7543   0.4182  0.35484 0.666667\n",
      "Neg Pred Value         0.8000   0.7576  0.88696 0.960396\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4300   0.1130  0.05405 0.004914\n",
      "Detection Prevalence   0.5700   0.2703  0.15233 0.007371\n",
      "Balanced Accuracy      0.7720   0.5842  0.62252 0.554270\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6018421 \n",
      "Average AUC: 0.7747149 \n",
      "Average Sensitivity: 0.4240245 \n",
      "Average Specificity: 0.8427898 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    \n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc409ef5",
   "metadata": {},
   "source": [
    "# 0.5 cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfd98081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  7  1  1\n",
      "         2  9  8  5  0\n",
      "         3  2  9  6  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5488         \n",
      "                 95% CI : (0.4349, 0.659)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2907         \n",
      "                                         \n",
      "                  Kappa : 0.2905         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381  0.33333  0.50000  0.00000\n",
      "Specificity            0.7750  0.75862  0.80000  1.00000\n",
      "Pos Pred Value         0.7750  0.36364  0.30000      NaN\n",
      "Neg Pred Value         0.7381  0.73333  0.90323  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.3780  0.09756  0.07317  0.00000\n",
      "Detection Prevalence   0.4878  0.26829  0.24390  0.00000\n",
      "Balanced Accuracy      0.7565  0.54598  0.65000  0.50000\n",
      "Fold 1 Accuracy: 0.548780487804878 \n",
      "Fold 1 AUC: 0.713169642857143 \n",
      "Fold 1 Overall Sensitivity: 0.392857142857143 \n",
      "Fold 1 Overall Specificity: 0.833405172413793 \n",
      "Fold 1 Actual vs Predicted values:\n",
      "   Actual Predicted\n",
      "1       6  8.586404\n",
      "2       3  2.238850\n",
      "3       3  4.546350\n",
      "4       5  5.171442\n",
      "5       5  7.095618\n",
      "6       4  6.954157\n",
      "7       3  4.838752\n",
      "8       5  6.169339\n",
      "9       5  4.908542\n",
      "10      3  3.866497\n",
      "11      3  3.266065\n",
      "12      3  3.055424\n",
      "13      5  4.556442\n",
      "14     10  8.090156\n",
      "15      3  3.068069\n",
      "16      3  2.265068\n",
      "17      6  7.987336\n",
      "18      3  4.114586\n",
      "19      3  3.405042\n",
      "20      3  3.191306\n",
      "21      3  4.466749\n",
      "22      3  3.038775\n",
      "23      3  1.092566\n",
      "24      8  5.676230\n",
      "25      3  3.483192\n",
      "26      3  3.770425\n",
      "27      3  5.011883\n",
      "28      3  3.755493\n",
      "29      3  3.143744\n",
      "30      3  4.250678\n",
      "31      5  1.865943\n",
      "32      3  5.110044\n",
      "33      3  1.948479\n",
      "34      3  2.387621\n",
      "35      5  7.703621\n",
      "36      3  7.627630\n",
      "37      6  5.819073\n",
      "38      6  7.377768\n",
      "39      3  1.657355\n",
      "40      5  3.469572\n",
      "41      5  6.138008\n",
      "42      6  2.961655\n",
      "43      3  1.263724\n",
      "44      7  5.094025\n",
      "45      7  2.717765\n",
      "46      3  5.014835\n",
      "47     11  6.884615\n",
      "48      9  6.386113\n",
      "49      3  5.901272\n",
      "50      3  0.942686\n",
      "51      3  3.676910\n",
      "52      8  8.123538\n",
      "53      6  2.076911\n",
      "54      6  6.890224\n",
      "55      5  6.448359\n",
      "56      5  3.095921\n",
      "57      7  7.869065\n",
      "58      3  3.497546\n",
      "59      3  2.587999\n",
      "60      9  4.559393\n",
      "61      9  7.308262\n",
      "62      9  6.819449\n",
      "63      3  4.098310\n",
      "64      9  7.152524\n",
      "65     10  4.457664\n",
      "66      6  6.572504\n",
      "67      3  2.397025\n",
      "68      5  6.651625\n",
      "69      5  2.197169\n",
      "70      3  4.152147\n",
      "71      6  6.319567\n",
      "72      5  7.818113\n",
      "73      8  6.919941\n",
      "74      5  3.288485\n",
      "75      3  5.377862\n",
      "76      3  3.394215\n",
      "77      7  5.770257\n",
      "78      3  2.665008\n",
      "79      3  4.851649\n",
      "80     11  8.702833\n",
      "81      3  2.450381\n",
      "82      3  4.689708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  6  0  0\n",
      "         2 10  9  6  1\n",
      "         3  3  9  6  2\n",
      "         4  0  0  1  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5366         \n",
      "                 95% CI : (0.423, 0.6475)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.3706         \n",
      "                                         \n",
      "                  Kappa : 0.2865         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.3750  0.46154  0.00000\n",
      "Specificity            0.8500   0.7069  0.79710  0.98734\n",
      "Pos Pred Value         0.8286   0.3462  0.30000  0.00000\n",
      "Neg Pred Value         0.7234   0.7321  0.88710  0.96296\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3537   0.1098  0.07317  0.00000\n",
      "Detection Prevalence   0.4268   0.3171  0.24390  0.01220\n",
      "Balanced Accuracy      0.7702   0.5409  0.62932  0.49367\n",
      "Fold 2 Accuracy: 0.536585365853659 \n",
      "Fold 2 AUC: 0.739933099308099 \n",
      "Fold 2 Overall Sensitivity: 0.381753663003663 \n",
      "Fold 2 Overall Specificity: 0.83533494328785 \n",
      "Fold 2 Actual vs Predicted values:\n",
      "   Actual Predicted\n",
      "1       3 4.5146078\n",
      "2       4 6.5764557\n",
      "3       3 4.3547911\n",
      "4       3 3.6126528\n",
      "5       3 3.1822957\n",
      "6       3 2.4790334\n",
      "7       5 3.8549240\n",
      "8       3 4.0952849\n",
      "9       5 7.8454826\n",
      "10      4 3.0421551\n",
      "11      3 2.4874124\n",
      "12      3 7.3111463\n",
      "13      3 5.1260709\n",
      "14      6 6.5915240\n",
      "15     12 7.1777228\n",
      "16      6 6.9909734\n",
      "17      3 6.4162452\n",
      "18      5 5.4761906\n",
      "19      3 4.5145026\n",
      "20      3 5.1344562\n",
      "21      3 2.1543875\n",
      "22      3 1.7454996\n",
      "23      3 4.1347221\n",
      "24      3 3.5327682\n",
      "25      3 4.6657006\n",
      "26      3 5.2510298\n",
      "27      3 2.0169221\n",
      "28      3 3.9319198\n",
      "29      5 4.0551993\n",
      "30      4 8.5180038\n",
      "31      3 4.9308689\n",
      "32      3 2.6336685\n",
      "33      3 6.2549305\n",
      "34      3 3.4313455\n",
      "35      8 9.5316380\n",
      "36      6 8.6012766\n",
      "37      5 5.0966883\n",
      "38      3 4.1197139\n",
      "39      9 7.0972086\n",
      "40     12 8.3011047\n",
      "41      5 6.2593110\n",
      "42      7 6.2029282\n",
      "43      6 7.5729416\n",
      "44      5 3.9680996\n",
      "45      8 7.8730234\n",
      "46      5 1.9825954\n",
      "47      7 8.3276036\n",
      "48      5 5.4651266\n",
      "49      5 8.6925477\n",
      "50      3 4.1334983\n",
      "51      5 6.1417759\n",
      "52      8 6.3959460\n",
      "53      3 2.1716187\n",
      "54      5 6.1364556\n",
      "55      6 6.7552340\n",
      "56      6 6.6952572\n",
      "57      5 6.8816225\n",
      "58      6 6.2416929\n",
      "59      5 3.1793913\n",
      "60     11 5.1880504\n",
      "61      3 4.5190545\n",
      "62      3 1.6603956\n",
      "63      5 4.3563132\n",
      "64      3 2.0322043\n",
      "65      3 4.4804004\n",
      "66      7 6.2447147\n",
      "67      3 1.4879344\n",
      "68      5 5.5519188\n",
      "69      8 7.4637686\n",
      "70      8 7.6589666\n",
      "71      3 4.4718586\n",
      "72      3 3.3614448\n",
      "73      7 6.3274582\n",
      "74      3 3.0815158\n",
      "75      7 5.1266766\n",
      "76      3 4.3435823\n",
      "77      3 4.0671098\n",
      "78      3 3.2930426\n",
      "79      3 0.4839879\n",
      "80      5 5.1431591\n",
      "81      8 5.3196648\n",
      "82      8 8.9210976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32 11  3  0\n",
      "         2 10  9  7  2\n",
      "         3  0  4  2  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5244          \n",
      "                 95% CI : (0.4111, 0.6359)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.4564          \n",
      "                                          \n",
      "                  Kappa : 0.2053          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3750  0.16667  0.00000\n",
      "Specificity            0.6500   0.6724  0.91429  1.00000\n",
      "Pos Pred Value         0.6957   0.3214  0.25000      NaN\n",
      "Neg Pred Value         0.7222   0.7222  0.86486  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.3902   0.1098  0.02439  0.00000\n",
      "Detection Prevalence   0.5610   0.3415  0.09756  0.00000\n",
      "Balanced Accuracy      0.7060   0.5237  0.54048  0.50000\n",
      "Fold 3 Accuracy: 0.524390243902439 \n",
      "Fold 3 AUC: 0.747519841269841 \n",
      "Fold 3 Overall Sensitivity: 0.325892857142857 \n",
      "Fold 3 Overall Specificity: 0.809174876847291 \n",
      "Fold 3 Actual vs Predicted values:\n",
      "   Actual   Predicted\n",
      "1       6  1.53612000\n",
      "2       3  5.91794724\n",
      "3       3  2.48202340\n",
      "4       3  3.93087133\n",
      "5       3  5.27295866\n",
      "6       7  9.06291592\n",
      "7       5  4.04973943\n",
      "8       3  3.87102097\n",
      "9       3  6.26525546\n",
      "10      3  3.72160259\n",
      "11      4  3.93258938\n",
      "12      3  4.07982976\n",
      "13      3  4.46999884\n",
      "14      3 -0.06942872\n",
      "15      8  3.33364575\n",
      "16      3  0.64319724\n",
      "17      3  5.63598142\n",
      "18      5  4.78657479\n",
      "19      3  2.31141101\n",
      "20      7  5.26517194\n",
      "21      3  2.80168552\n",
      "22      3  2.40047014\n",
      "23      6  6.18106992\n",
      "24      8  4.60809576\n",
      "25      3  1.99428260\n",
      "26     10  6.20201558\n",
      "27      3  1.74433091\n",
      "28      3  3.21519635\n",
      "29      3  3.96750444\n",
      "30      3  2.29398033\n",
      "31      3  1.86419054\n",
      "32      6  6.78882902\n",
      "33      6  6.32569766\n",
      "34      7  4.00684707\n",
      "35      5  7.96836775\n",
      "36      3  3.53115944\n",
      "37      5  3.12045493\n",
      "38      5  1.96484848\n",
      "39      3  1.45387563\n",
      "40      6  6.16093802\n",
      "41      3  4.81000996\n",
      "42      8  4.55035707\n",
      "43      5  4.65617304\n",
      "44      5  5.79768265\n",
      "45     12  6.23325302\n",
      "46      5  6.56090919\n",
      "47      6  4.24203156\n",
      "48      5  5.87554763\n",
      "49      6  4.41295968\n",
      "50      3  2.69438560\n",
      "51      5  4.11446285\n",
      "52      5  4.26630966\n",
      "53      3  4.54690171\n",
      "54      7  5.26677694\n",
      "55      8  6.28226803\n",
      "56      3  2.45147769\n",
      "57      3  3.16078603\n",
      "58      5  7.17266743\n",
      "59      7  4.87200139\n",
      "60     10  8.00127483\n",
      "61      5  5.30459771\n",
      "62      3  4.01060016\n",
      "63      3  4.90969726\n",
      "64      5  1.34865828\n",
      "65      5  6.06763476\n",
      "66      3  3.94423731\n",
      "67      5  4.10746653\n",
      "68      3  2.30511496\n",
      "69      8  5.52865083\n",
      "70      3  4.66139887\n",
      "71      9  7.54797224\n",
      "72      3  4.94671851\n",
      "73      3  3.19875872\n",
      "74      3  2.65315050\n",
      "75      3  3.52362356\n",
      "76      3  2.38705521\n",
      "77      3  5.06873818\n",
      "78     10  6.65472719\n",
      "79      8  4.35008255\n",
      "80      3  3.02151817\n",
      "81      5  3.36434542\n",
      "82      3  3.21818583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  7  0  0\n",
      "         2 10  9  7  0\n",
      "         3  4  6  5  3\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5309          \n",
      "                 95% CI : (0.4167, 0.6427)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.4563          \n",
      "                                          \n",
      "                  Kappa : 0.279           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.3913  0.41667  0.25000\n",
      "Specificity            0.8205   0.7069  0.81159  0.98701\n",
      "Pos Pred Value         0.8000   0.3462  0.27778  0.50000\n",
      "Neg Pred Value         0.6957   0.7455  0.88889  0.96203\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3457   0.1111  0.06173  0.01235\n",
      "Detection Prevalence   0.4321   0.3210  0.22222  0.02469\n",
      "Balanced Accuracy      0.7436   0.5491  0.61413  0.61851\n",
      "Fold 4 Accuracy: 0.530864197530864 \n",
      "Fold 4 AUC: 0.81067043363239 \n",
      "Fold 4 Overall Sensitivity: 0.431159420289855 \n",
      "Fold 4 Overall Specificity: 0.831504140537124 \n",
      "Fold 4 Actual vs Predicted values:\n",
      "   Actual Predicted\n",
      "1       3  4.313330\n",
      "2       3  4.539652\n",
      "3       3  4.631728\n",
      "4       4  6.922639\n",
      "5       3  4.853542\n",
      "6       6  7.316025\n",
      "7       3  4.200451\n",
      "8       4  5.459959\n",
      "9       3  7.343430\n",
      "10      3  5.014862\n",
      "11      3  4.823576\n",
      "12      6  3.563204\n",
      "13      3  2.794440\n",
      "14      3  1.903975\n",
      "15      6  5.386734\n",
      "16      3  3.277842\n",
      "17      5  7.575279\n",
      "18      3  4.450365\n",
      "19      3  2.312539\n",
      "20      9  8.161538\n",
      "21      3  3.192185\n",
      "22      3  3.319337\n",
      "23      3  1.804455\n",
      "24      3  2.840388\n",
      "25      5  5.006015\n",
      "26      3  6.563637\n",
      "27      3  4.786956\n",
      "28      5  6.208354\n",
      "29      5  3.626387\n",
      "30      3  3.749369\n",
      "31      3  5.073185\n",
      "32      3  4.115786\n",
      "33      8  6.742738\n",
      "34      3  4.479225\n",
      "35      3  3.468131\n",
      "36      3  4.230307\n",
      "37      3  3.022107\n",
      "38      3  3.433587\n",
      "39      5  5.077259\n",
      "40      5  8.167677\n",
      "41      6  5.092182\n",
      "42      8  4.535127\n",
      "43      9  5.494329\n",
      "44      6  5.514030\n",
      "45      9  6.428347\n",
      "46      8  6.275537\n",
      "47      3  3.753218\n",
      "48      9  5.259137\n",
      "49      7  8.478213\n",
      "50     11  9.187367\n",
      "51      8  6.264543\n",
      "52      6  7.924666\n",
      "53      5  4.275661\n",
      "54      5  5.023058\n",
      "55      3  3.967514\n",
      "56      3  4.278246\n",
      "57      3  5.384906\n",
      "58      5  6.920882\n",
      "59     12 10.504137\n",
      "60      5  3.108298\n",
      "61      5 10.692194\n",
      "62      5  4.321451\n",
      "63     10  8.294447\n",
      "64      3  3.197316\n",
      "65      5  1.425402\n",
      "66      5  4.759407\n",
      "67     11  8.509217\n",
      "68      5  5.709180\n",
      "69      3  6.544010\n",
      "70      3  4.498166\n",
      "71      3  2.654166\n",
      "72      3  5.115556\n",
      "73      3  3.284409\n",
      "74      3  4.170017\n",
      "75      6  3.123366\n",
      "76      3  3.536088\n",
      "77      7  4.605254\n",
      "78      9  7.907459\n",
      "79      9  8.356541\n",
      "80      6  8.319669\n",
      "81      3  3.273879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32  5  1  0\n",
      "         2  8  9  5  0\n",
      "         3  2  8  6  2\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6            \n",
      "                 95% CI : (0.4844, 0.708)\n",
      "    No Information Rate : 0.525          \n",
      "    P-Value [Acc > NIR] : 0.1088         \n",
      "                                         \n",
      "                  Kappa : 0.3719         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3913   0.5000   0.3333\n",
      "Specificity            0.8421   0.7719   0.8235   0.9870\n",
      "Pos Pred Value         0.8421   0.4091   0.3333   0.5000\n",
      "Neg Pred Value         0.7619   0.7586   0.9032   0.9744\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4000   0.1125   0.0750   0.0125\n",
      "Detection Prevalence   0.4750   0.2750   0.2250   0.0250\n",
      "Balanced Accuracy      0.8020   0.5816   0.6618   0.6602\n",
      "Fold 5 Accuracy: 0.6 \n",
      "Fold 5 AUC: 0.819329422590292 \n",
      "Fold 5 Overall Sensitivity: 0.496635610766046 \n",
      "Fold 5 Overall Specificity: 0.856144371624248 \n",
      "Fold 5 Actual vs Predicted values:\n",
      "   Actual  Predicted\n",
      "1       3  6.0115671\n",
      "2       3  4.2220313\n",
      "3       3  3.4624508\n",
      "4       3  4.8273377\n",
      "5       3  6.6950447\n",
      "6       3  4.9932358\n",
      "7       5  3.3798212\n",
      "8       5  7.8492929\n",
      "9       4  1.4274760\n",
      "10      3  4.2539116\n",
      "11      7  6.3450229\n",
      "12      3  1.7784142\n",
      "13      7  5.6315160\n",
      "14      3  1.7698328\n",
      "15      3  4.3419335\n",
      "16      8  5.9877573\n",
      "17      3  3.7428955\n",
      "18      5  6.1681106\n",
      "19      3  3.6087007\n",
      "20      3  2.1326893\n",
      "21      3  4.0553741\n",
      "22      3  6.0022415\n",
      "23      5  4.2935866\n",
      "24      3  2.8221544\n",
      "25      6  9.6837416\n",
      "26      6  5.8116334\n",
      "27      5  5.3786511\n",
      "28      5  7.0191108\n",
      "29      5  4.6116597\n",
      "30      3  4.0204475\n",
      "31      3  3.2525126\n",
      "32      4  4.8602159\n",
      "33      7  6.1229599\n",
      "34      3  4.1678666\n",
      "35      3  0.3336202\n",
      "36      5  3.7793994\n",
      "37      5  6.4689309\n",
      "38      7  6.9882447\n",
      "39     10 10.5387493\n",
      "40     10  7.9948061\n",
      "41      3  2.7037541\n",
      "42      3  4.4223567\n",
      "43      8  5.9970951\n",
      "44      3  4.7547385\n",
      "45      7  8.1305426\n",
      "46      3  1.2217954\n",
      "47      7  3.7256029\n",
      "48      6  7.0107744\n",
      "49     11  7.8140070\n",
      "50      3  2.8376885\n",
      "51      6  8.7901585\n",
      "52      6  7.8504806\n",
      "53      3  4.1279211\n",
      "54      3  4.1153760\n",
      "55      9  9.3570316\n",
      "56      3  4.3100292\n",
      "57      6  3.8833026\n",
      "58      3  4.0667764\n",
      "59      5  4.8926072\n",
      "60      5  7.4539351\n",
      "61      3  1.5530113\n",
      "62      3  3.2177917\n",
      "63      3  1.9202919\n",
      "64      9  7.6271800\n",
      "65      6  7.2606816\n",
      "66      5  3.9262762\n",
      "67      7  9.3916246\n",
      "68      3  2.7408500\n",
      "69      4  6.9367162\n",
      "70      3  0.6534420\n",
      "71      5  4.5901975\n",
      "72      6  4.6197961\n",
      "73      8  7.4147288\n",
      "74      6  6.4384773\n",
      "75      3  5.3699272\n",
      "76      3  5.9096986\n",
      "77      3  3.2802899\n",
      "78      3  3.4181398\n",
      "79      5  7.5032575\n",
      "80      3  3.6748416\n",
      "Overall Confusion Matrix across all folds:\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 152  36   5   1\n",
      "         2  47  44  30   3\n",
      "         3  11  36  25  12\n",
      "         4   0   2   1   2\n",
      "Overall Accuracy across all folds: 0.5479115 \n",
      "Overall Sensitivity across all folds:\n",
      "        1         2         3         4 \n",
      "0.7835052 0.3548387 0.2976190 0.4000000 \n",
      "Overall Specificity across all folds:\n",
      "        1         2         3         4 \n",
      "0.7238095 0.3728814 0.4098361 0.1111111 \n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5481241 \n",
      "Average AUC: 0.7661245 \n",
      "Average Sensitivity: 0.4056597 \n",
      "Average Specificity: 0.8331127 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 4.5) {\n",
    "    return(1)\n",
    "  } else if (value >= 4.5 & value < 6.5) {\n",
    "    return(2)\n",
    "  } else if (value >= 6.5 & value < 9.5) {\n",
    "    return(3)\n",
    "  } else if (value >= 9.5 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    \n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "}\n",
    "\n",
    "# Initialize an empty matrix for accumulating confusion matrix counts\n",
    "overall_conf_matrix <- matrix(0, nrow = 4, ncol = 4)\n",
    "\n",
    "# Loop over all confusion matrices from each fold and sum the table counts\n",
    "for (i in 1:k) {\n",
    "  # Extract confusion matrix for this fold as a table\n",
    "  fold_conf_matrix <- as.table(all_conf_matrices[[i]]$table)\n",
    "  \n",
    "  # Sum the counts into the overall confusion matrix\n",
    "  overall_conf_matrix <- overall_conf_matrix + fold_conf_matrix\n",
    "}\n",
    "\n",
    "# Convert the overall matrix to a table for clearer output\n",
    "overall_conf_matrix <- as.table(overall_conf_matrix)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Optionally, calculate overall accuracy, sensitivity, specificity based on the accumulated matrix\n",
    "overall_accuracy <- sum(diag(overall_conf_matrix)) / sum(overall_conf_matrix)\n",
    "cat(\"Overall Accuracy across all folds:\", overall_accuracy, \"\\n\")\n",
    "\n",
    "# Calculate sensitivity and specificity for each class\n",
    "overall_sensitivity <- diag(overall_conf_matrix) / rowSums(overall_conf_matrix)\n",
    "overall_specificity <- diag(overall_conf_matrix) / colSums(overall_conf_matrix)\n",
    "\n",
    "cat(\"Overall Sensitivity across all folds:\\n\")\n",
    "print(overall_sensitivity)\n",
    "\n",
    "cat(\"Overall Specificity across all folds:\\n\")\n",
    "print(overall_specificity)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062a3ba",
   "metadata": {},
   "source": [
    "# Rounding Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "290f03b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  7  1  1\n",
      "         2  9  8  5  0\n",
      "         3  2  9  6  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5488         \n",
      "                 95% CI : (0.4349, 0.659)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2907         \n",
      "                                         \n",
      "                  Kappa : 0.2905         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381  0.33333  0.50000  0.00000\n",
      "Specificity            0.7750  0.75862  0.80000  1.00000\n",
      "Pos Pred Value         0.7750  0.36364  0.30000      NaN\n",
      "Neg Pred Value         0.7381  0.73333  0.90323  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.3780  0.09756  0.07317  0.00000\n",
      "Detection Prevalence   0.4878  0.26829  0.24390  0.00000\n",
      "Balanced Accuracy      0.7565  0.54598  0.65000  0.50000\n",
      "Fold 1 Accuracy: 0.548780487804878 \n",
      "Fold 1 AUC: 0.713169642857143 \n",
      "Fold 1 Overall Sensitivity: 0.392857142857143 \n",
      "Fold 1 Overall Specificity: 0.833405172413793 \n",
      "Fold 1 Pearson Correlation: 0.585850562863264 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  8.586404                 9\n",
      "2       3  2.238850                 2\n",
      "3       3  4.546350                 5\n",
      "4       5  5.171442                 5\n",
      "5       5  7.095618                 7\n",
      "6       4  6.954157                 7\n",
      "7       3  4.838752                 5\n",
      "8       5  6.169339                 6\n",
      "9       5  4.908542                 5\n",
      "10      3  3.866497                 4\n",
      "11      3  3.266065                 3\n",
      "12      3  3.055424                 3\n",
      "13      5  4.556442                 5\n",
      "14     10  8.090156                 8\n",
      "15      3  3.068069                 3\n",
      "16      3  2.265068                 2\n",
      "17      6  7.987336                 8\n",
      "18      3  4.114586                 4\n",
      "19      3  3.405042                 3\n",
      "20      3  3.191306                 3\n",
      "21      3  4.466749                 4\n",
      "22      3  3.038775                 3\n",
      "23      3  1.092566                 1\n",
      "24      8  5.676230                 6\n",
      "25      3  3.483192                 3\n",
      "26      3  3.770425                 4\n",
      "27      3  5.011883                 5\n",
      "28      3  3.755493                 4\n",
      "29      3  3.143744                 3\n",
      "30      3  4.250678                 4\n",
      "31      5  1.865943                 2\n",
      "32      3  5.110044                 5\n",
      "33      3  1.948479                 2\n",
      "34      3  2.387621                 2\n",
      "35      5  7.703621                 8\n",
      "36      3  7.627630                 8\n",
      "37      6  5.819073                 6\n",
      "38      6  7.377768                 7\n",
      "39      3  1.657355                 2\n",
      "40      5  3.469572                 3\n",
      "41      5  6.138008                 6\n",
      "42      6  2.961655                 3\n",
      "43      3  1.263724                 1\n",
      "44      7  5.094025                 5\n",
      "45      7  2.717765                 3\n",
      "46      3  5.014835                 5\n",
      "47     11  6.884615                 7\n",
      "48      9  6.386113                 6\n",
      "49      3  5.901272                 6\n",
      "50      3  0.942686                 1\n",
      "51      3  3.676910                 4\n",
      "52      8  8.123538                 8\n",
      "53      6  2.076911                 2\n",
      "54      6  6.890224                 7\n",
      "55      5  6.448359                 6\n",
      "56      5  3.095921                 3\n",
      "57      7  7.869065                 8\n",
      "58      3  3.497546                 3\n",
      "59      3  2.587999                 3\n",
      "60      9  4.559393                 5\n",
      "61      9  7.308262                 7\n",
      "62      9  6.819449                 7\n",
      "63      3  4.098310                 4\n",
      "64      9  7.152524                 7\n",
      "65     10  4.457664                 4\n",
      "66      6  6.572504                 7\n",
      "67      3  2.397025                 2\n",
      "68      5  6.651625                 7\n",
      "69      5  2.197169                 2\n",
      "70      3  4.152147                 4\n",
      "71      6  6.319567                 6\n",
      "72      5  7.818113                 8\n",
      "73      8  6.919941                 7\n",
      "74      5  3.288485                 3\n",
      "75      3  5.377862                 5\n",
      "76      3  3.394215                 3\n",
      "77      7  5.770257                 6\n",
      "78      3  2.665008                 3\n",
      "79      3  4.851649                 5\n",
      "80     11  8.702833                 9\n",
      "81      3  2.450381                 2\n",
      "82      3  4.689708                 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  6  0  0\n",
      "         2 10  9  6  1\n",
      "         3  3  9  6  2\n",
      "         4  0  0  1  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5366         \n",
      "                 95% CI : (0.423, 0.6475)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.3706         \n",
      "                                         \n",
      "                  Kappa : 0.2865         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.3750  0.46154  0.00000\n",
      "Specificity            0.8500   0.7069  0.79710  0.98734\n",
      "Pos Pred Value         0.8286   0.3462  0.30000  0.00000\n",
      "Neg Pred Value         0.7234   0.7321  0.88710  0.96296\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3537   0.1098  0.07317  0.00000\n",
      "Detection Prevalence   0.4268   0.3171  0.24390  0.01220\n",
      "Balanced Accuracy      0.7702   0.5409  0.62932  0.49367\n",
      "Fold 2 Accuracy: 0.536585365853659 \n",
      "Fold 2 AUC: 0.739933099308099 \n",
      "Fold 2 Overall Sensitivity: 0.381753663003663 \n",
      "Fold 2 Overall Specificity: 0.83533494328785 \n",
      "Fold 2 Pearson Correlation: 0.62780241562379 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3 4.5146078                 5\n",
      "2       4 6.5764557                 7\n",
      "3       3 4.3547911                 4\n",
      "4       3 3.6126528                 4\n",
      "5       3 3.1822957                 3\n",
      "6       3 2.4790334                 2\n",
      "7       5 3.8549240                 4\n",
      "8       3 4.0952849                 4\n",
      "9       5 7.8454826                 8\n",
      "10      4 3.0421551                 3\n",
      "11      3 2.4874124                 2\n",
      "12      3 7.3111463                 7\n",
      "13      3 5.1260709                 5\n",
      "14      6 6.5915240                 7\n",
      "15     12 7.1777228                 7\n",
      "16      6 6.9909734                 7\n",
      "17      3 6.4162452                 6\n",
      "18      5 5.4761906                 5\n",
      "19      3 4.5145026                 5\n",
      "20      3 5.1344562                 5\n",
      "21      3 2.1543875                 2\n",
      "22      3 1.7454996                 2\n",
      "23      3 4.1347221                 4\n",
      "24      3 3.5327682                 4\n",
      "25      3 4.6657006                 5\n",
      "26      3 5.2510298                 5\n",
      "27      3 2.0169221                 2\n",
      "28      3 3.9319198                 4\n",
      "29      5 4.0551993                 4\n",
      "30      4 8.5180038                 9\n",
      "31      3 4.9308689                 5\n",
      "32      3 2.6336685                 3\n",
      "33      3 6.2549305                 6\n",
      "34      3 3.4313455                 3\n",
      "35      8 9.5316380                10\n",
      "36      6 8.6012766                 9\n",
      "37      5 5.0966883                 5\n",
      "38      3 4.1197139                 4\n",
      "39      9 7.0972086                 7\n",
      "40     12 8.3011047                 8\n",
      "41      5 6.2593110                 6\n",
      "42      7 6.2029282                 6\n",
      "43      6 7.5729416                 8\n",
      "44      5 3.9680996                 4\n",
      "45      8 7.8730234                 8\n",
      "46      5 1.9825954                 2\n",
      "47      7 8.3276036                 8\n",
      "48      5 5.4651266                 5\n",
      "49      5 8.6925477                 9\n",
      "50      3 4.1334983                 4\n",
      "51      5 6.1417759                 6\n",
      "52      8 6.3959460                 6\n",
      "53      3 2.1716187                 2\n",
      "54      5 6.1364556                 6\n",
      "55      6 6.7552340                 7\n",
      "56      6 6.6952572                 7\n",
      "57      5 6.8816225                 7\n",
      "58      6 6.2416929                 6\n",
      "59      5 3.1793913                 3\n",
      "60     11 5.1880504                 5\n",
      "61      3 4.5190545                 5\n",
      "62      3 1.6603956                 2\n",
      "63      5 4.3563132                 4\n",
      "64      3 2.0322043                 2\n",
      "65      3 4.4804004                 4\n",
      "66      7 6.2447147                 6\n",
      "67      3 1.4879344                 1\n",
      "68      5 5.5519188                 6\n",
      "69      8 7.4637686                 7\n",
      "70      8 7.6589666                 8\n",
      "71      3 4.4718586                 4\n",
      "72      3 3.3614448                 3\n",
      "73      7 6.3274582                 6\n",
      "74      3 3.0815158                 3\n",
      "75      7 5.1266766                 5\n",
      "76      3 4.3435823                 4\n",
      "77      3 4.0671098                 4\n",
      "78      3 3.2930426                 3\n",
      "79      3 0.4839879                 0\n",
      "80      5 5.1431591                 5\n",
      "81      8 5.3196648                 5\n",
      "82      8 8.9210976                 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32 11  3  0\n",
      "         2 10  9  7  2\n",
      "         3  0  4  2  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5244          \n",
      "                 95% CI : (0.4111, 0.6359)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.4564          \n",
      "                                          \n",
      "                  Kappa : 0.2053          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3750  0.16667  0.00000\n",
      "Specificity            0.6500   0.6724  0.91429  1.00000\n",
      "Pos Pred Value         0.6957   0.3214  0.25000      NaN\n",
      "Neg Pred Value         0.7222   0.7222  0.86486  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.3902   0.1098  0.02439  0.00000\n",
      "Detection Prevalence   0.5610   0.3415  0.09756  0.00000\n",
      "Balanced Accuracy      0.7060   0.5237  0.54048  0.50000\n",
      "Fold 3 Accuracy: 0.524390243902439 \n",
      "Fold 3 AUC: 0.747519841269841 \n",
      "Fold 3 Overall Sensitivity: 0.325892857142857 \n",
      "Fold 3 Overall Specificity: 0.809174876847291 \n",
      "Fold 3 Pearson Correlation: 0.537613515812769 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual   Predicted Rounded_Predicted\n",
      "1       6  1.53612000                 2\n",
      "2       3  5.91794724                 6\n",
      "3       3  2.48202340                 2\n",
      "4       3  3.93087133                 4\n",
      "5       3  5.27295866                 5\n",
      "6       7  9.06291592                 9\n",
      "7       5  4.04973943                 4\n",
      "8       3  3.87102097                 4\n",
      "9       3  6.26525546                 6\n",
      "10      3  3.72160259                 4\n",
      "11      4  3.93258938                 4\n",
      "12      3  4.07982976                 4\n",
      "13      3  4.46999884                 4\n",
      "14      3 -0.06942872                 0\n",
      "15      8  3.33364575                 3\n",
      "16      3  0.64319724                 1\n",
      "17      3  5.63598142                 6\n",
      "18      5  4.78657479                 5\n",
      "19      3  2.31141101                 2\n",
      "20      7  5.26517194                 5\n",
      "21      3  2.80168552                 3\n",
      "22      3  2.40047014                 2\n",
      "23      6  6.18106992                 6\n",
      "24      8  4.60809576                 5\n",
      "25      3  1.99428260                 2\n",
      "26     10  6.20201558                 6\n",
      "27      3  1.74433091                 2\n",
      "28      3  3.21519635                 3\n",
      "29      3  3.96750444                 4\n",
      "30      3  2.29398033                 2\n",
      "31      3  1.86419054                 2\n",
      "32      6  6.78882902                 7\n",
      "33      6  6.32569766                 6\n",
      "34      7  4.00684707                 4\n",
      "35      5  7.96836775                 8\n",
      "36      3  3.53115944                 4\n",
      "37      5  3.12045493                 3\n",
      "38      5  1.96484848                 2\n",
      "39      3  1.45387563                 1\n",
      "40      6  6.16093802                 6\n",
      "41      3  4.81000996                 5\n",
      "42      8  4.55035707                 5\n",
      "43      5  4.65617304                 5\n",
      "44      5  5.79768265                 6\n",
      "45     12  6.23325302                 6\n",
      "46      5  6.56090919                 7\n",
      "47      6  4.24203156                 4\n",
      "48      5  5.87554763                 6\n",
      "49      6  4.41295968                 4\n",
      "50      3  2.69438560                 3\n",
      "51      5  4.11446285                 4\n",
      "52      5  4.26630966                 4\n",
      "53      3  4.54690171                 5\n",
      "54      7  5.26677694                 5\n",
      "55      8  6.28226803                 6\n",
      "56      3  2.45147769                 2\n",
      "57      3  3.16078603                 3\n",
      "58      5  7.17266743                 7\n",
      "59      7  4.87200139                 5\n",
      "60     10  8.00127483                 8\n",
      "61      5  5.30459771                 5\n",
      "62      3  4.01060016                 4\n",
      "63      3  4.90969726                 5\n",
      "64      5  1.34865828                 1\n",
      "65      5  6.06763476                 6\n",
      "66      3  3.94423731                 4\n",
      "67      5  4.10746653                 4\n",
      "68      3  2.30511496                 2\n",
      "69      8  5.52865083                 6\n",
      "70      3  4.66139887                 5\n",
      "71      9  7.54797224                 8\n",
      "72      3  4.94671851                 5\n",
      "73      3  3.19875872                 3\n",
      "74      3  2.65315050                 3\n",
      "75      3  3.52362356                 4\n",
      "76      3  2.38705521                 2\n",
      "77      3  5.06873818                 5\n",
      "78     10  6.65472719                 7\n",
      "79      8  4.35008255                 4\n",
      "80      3  3.02151817                 3\n",
      "81      5  3.36434542                 3\n",
      "82      3  3.21818583                 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  7  0  0\n",
      "         2 10  9  7  0\n",
      "         3  4  6  5  3\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5309          \n",
      "                 95% CI : (0.4167, 0.6427)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.4563          \n",
      "                                          \n",
      "                  Kappa : 0.279           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.3913  0.41667  0.25000\n",
      "Specificity            0.8205   0.7069  0.81159  0.98701\n",
      "Pos Pred Value         0.8000   0.3462  0.27778  0.50000\n",
      "Neg Pred Value         0.6957   0.7455  0.88889  0.96203\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3457   0.1111  0.06173  0.01235\n",
      "Detection Prevalence   0.4321   0.3210  0.22222  0.02469\n",
      "Balanced Accuracy      0.7436   0.5491  0.61413  0.61851\n",
      "Fold 4 Accuracy: 0.530864197530864 \n",
      "Fold 4 AUC: 0.81067043363239 \n",
      "Fold 4 Overall Sensitivity: 0.431159420289855 \n",
      "Fold 4 Overall Specificity: 0.831504140537124 \n",
      "Fold 4 Pearson Correlation: 0.646123683883944 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.313330                 4\n",
      "2       3  4.539652                 5\n",
      "3       3  4.631728                 5\n",
      "4       4  6.922639                 7\n",
      "5       3  4.853542                 5\n",
      "6       6  7.316025                 7\n",
      "7       3  4.200451                 4\n",
      "8       4  5.459959                 5\n",
      "9       3  7.343430                 7\n",
      "10      3  5.014862                 5\n",
      "11      3  4.823576                 5\n",
      "12      6  3.563204                 4\n",
      "13      3  2.794440                 3\n",
      "14      3  1.903975                 2\n",
      "15      6  5.386734                 5\n",
      "16      3  3.277842                 3\n",
      "17      5  7.575279                 8\n",
      "18      3  4.450365                 4\n",
      "19      3  2.312539                 2\n",
      "20      9  8.161538                 8\n",
      "21      3  3.192185                 3\n",
      "22      3  3.319337                 3\n",
      "23      3  1.804455                 2\n",
      "24      3  2.840388                 3\n",
      "25      5  5.006015                 5\n",
      "26      3  6.563637                 7\n",
      "27      3  4.786956                 5\n",
      "28      5  6.208354                 6\n",
      "29      5  3.626387                 4\n",
      "30      3  3.749369                 4\n",
      "31      3  5.073185                 5\n",
      "32      3  4.115786                 4\n",
      "33      8  6.742738                 7\n",
      "34      3  4.479225                 4\n",
      "35      3  3.468131                 3\n",
      "36      3  4.230307                 4\n",
      "37      3  3.022107                 3\n",
      "38      3  3.433587                 3\n",
      "39      5  5.077259                 5\n",
      "40      5  8.167677                 8\n",
      "41      6  5.092182                 5\n",
      "42      8  4.535127                 5\n",
      "43      9  5.494329                 5\n",
      "44      6  5.514030                 6\n",
      "45      9  6.428347                 6\n",
      "46      8  6.275537                 6\n",
      "47      3  3.753218                 4\n",
      "48      9  5.259137                 5\n",
      "49      7  8.478213                 8\n",
      "50     11  9.187367                 9\n",
      "51      8  6.264543                 6\n",
      "52      6  7.924666                 8\n",
      "53      5  4.275661                 4\n",
      "54      5  5.023058                 5\n",
      "55      3  3.967514                 4\n",
      "56      3  4.278246                 4\n",
      "57      3  5.384906                 5\n",
      "58      5  6.920882                 7\n",
      "59     12 10.504137                11\n",
      "60      5  3.108298                 3\n",
      "61      5 10.692194                11\n",
      "62      5  4.321451                 4\n",
      "63     10  8.294447                 8\n",
      "64      3  3.197316                 3\n",
      "65      5  1.425402                 1\n",
      "66      5  4.759407                 5\n",
      "67     11  8.509217                 9\n",
      "68      5  5.709180                 6\n",
      "69      3  6.544010                 7\n",
      "70      3  4.498166                 4\n",
      "71      3  2.654166                 3\n",
      "72      3  5.115556                 5\n",
      "73      3  3.284409                 3\n",
      "74      3  4.170017                 4\n",
      "75      6  3.123366                 3\n",
      "76      3  3.536088                 4\n",
      "77      7  4.605254                 5\n",
      "78      9  7.907459                 8\n",
      "79      9  8.356541                 8\n",
      "80      6  8.319669                 8\n",
      "81      3  3.273879                 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32  5  1  0\n",
      "         2  8  9  5  0\n",
      "         3  2  8  6  2\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6            \n",
      "                 95% CI : (0.4844, 0.708)\n",
      "    No Information Rate : 0.525          \n",
      "    P-Value [Acc > NIR] : 0.1088         \n",
      "                                         \n",
      "                  Kappa : 0.3719         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3913   0.5000   0.3333\n",
      "Specificity            0.8421   0.7719   0.8235   0.9870\n",
      "Pos Pred Value         0.8421   0.4091   0.3333   0.5000\n",
      "Neg Pred Value         0.7619   0.7586   0.9032   0.9744\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4000   0.1125   0.0750   0.0125\n",
      "Detection Prevalence   0.4750   0.2750   0.2250   0.0250\n",
      "Balanced Accuracy      0.8020   0.5816   0.6618   0.6602\n",
      "Fold 5 Accuracy: 0.6 \n",
      "Fold 5 AUC: 0.819329422590292 \n",
      "Fold 5 Overall Sensitivity: 0.496635610766046 \n",
      "Fold 5 Overall Specificity: 0.856144371624248 \n",
      "Fold 5 Pearson Correlation: 0.698701250270758 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual  Predicted Rounded_Predicted\n",
      "1       3  6.0115671                 6\n",
      "2       3  4.2220313                 4\n",
      "3       3  3.4624508                 3\n",
      "4       3  4.8273377                 5\n",
      "5       3  6.6950447                 7\n",
      "6       3  4.9932358                 5\n",
      "7       5  3.3798212                 3\n",
      "8       5  7.8492929                 8\n",
      "9       4  1.4274760                 1\n",
      "10      3  4.2539116                 4\n",
      "11      7  6.3450229                 6\n",
      "12      3  1.7784142                 2\n",
      "13      7  5.6315160                 6\n",
      "14      3  1.7698328                 2\n",
      "15      3  4.3419335                 4\n",
      "16      8  5.9877573                 6\n",
      "17      3  3.7428955                 4\n",
      "18      5  6.1681106                 6\n",
      "19      3  3.6087007                 4\n",
      "20      3  2.1326893                 2\n",
      "21      3  4.0553741                 4\n",
      "22      3  6.0022415                 6\n",
      "23      5  4.2935866                 4\n",
      "24      3  2.8221544                 3\n",
      "25      6  9.6837416                10\n",
      "26      6  5.8116334                 6\n",
      "27      5  5.3786511                 5\n",
      "28      5  7.0191108                 7\n",
      "29      5  4.6116597                 5\n",
      "30      3  4.0204475                 4\n",
      "31      3  3.2525126                 3\n",
      "32      4  4.8602159                 5\n",
      "33      7  6.1229599                 6\n",
      "34      3  4.1678666                 4\n",
      "35      3  0.3336202                 0\n",
      "36      5  3.7793994                 4\n",
      "37      5  6.4689309                 6\n",
      "38      7  6.9882447                 7\n",
      "39     10 10.5387493                11\n",
      "40     10  7.9948061                 8\n",
      "41      3  2.7037541                 3\n",
      "42      3  4.4223567                 4\n",
      "43      8  5.9970951                 6\n",
      "44      3  4.7547385                 5\n",
      "45      7  8.1305426                 8\n",
      "46      3  1.2217954                 1\n",
      "47      7  3.7256029                 4\n",
      "48      6  7.0107744                 7\n",
      "49     11  7.8140070                 8\n",
      "50      3  2.8376885                 3\n",
      "51      6  8.7901585                 9\n",
      "52      6  7.8504806                 8\n",
      "53      3  4.1279211                 4\n",
      "54      3  4.1153760                 4\n",
      "55      9  9.3570316                 9\n",
      "56      3  4.3100292                 4\n",
      "57      6  3.8833026                 4\n",
      "58      3  4.0667764                 4\n",
      "59      5  4.8926072                 5\n",
      "60      5  7.4539351                 7\n",
      "61      3  1.5530113                 2\n",
      "62      3  3.2177917                 3\n",
      "63      3  1.9202919                 2\n",
      "64      9  7.6271800                 8\n",
      "65      6  7.2606816                 7\n",
      "66      5  3.9262762                 4\n",
      "67      7  9.3916246                 9\n",
      "68      3  2.7408500                 3\n",
      "69      4  6.9367162                 7\n",
      "70      3  0.6534420                 1\n",
      "71      5  4.5901975                 5\n",
      "72      6  4.6197961                 5\n",
      "73      8  7.4147288                 7\n",
      "74      6  6.4384773                 6\n",
      "75      3  5.3699272                 5\n",
      "76      3  5.9096986                 6\n",
      "77      3  3.2802899                 3\n",
      "78      3  3.4181398                 3\n",
      "79      5  7.5032575                 8\n",
      "80      3  3.6748416                 4\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 152  36   5   1\n",
      "         2  47  44  30   3\n",
      "         3  11  36  25  12\n",
      "         4   0   2   1   2\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5479         \n",
      "                 95% CI : (0.4981, 0.597)\n",
      "    No Information Rate : 0.516          \n",
      "    P-Value [Acc > NIR] : 0.1074         \n",
      "                                         \n",
      "                  Kappa : 0.2872         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 0.0222         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7238   0.3729  0.40984 0.111111\n",
      "Specificity            0.7868   0.7232  0.82948 0.992288\n",
      "Pos Pred Value         0.7835   0.3548  0.29762 0.400000\n",
      "Neg Pred Value         0.7277   0.7385  0.88854 0.960199\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.3735   0.1081  0.06143 0.004914\n",
      "Detection Prevalence   0.4767   0.3047  0.20639 0.012285\n",
      "Balanced Accuracy      0.7553   0.5480  0.61966 0.551700\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5481241 \n",
      "Average AUC: 0.7661245 \n",
      "Average Sensitivity: 0.4056597 \n",
      "Average Specificity: 0.8331127 \n",
      "Average Pearson Correlation: 0.6192183 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  \n",
    "all_pearson_correlations <- numeric(k)\n",
    "\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "# Perform k-fold cross-validation manually\n",
    "# Initialize a vector to store Pearson correlation coefficients\n",
    "all_pearson_correlations <- numeric(k)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Calculate Pearson correlation coefficient for this fold\n",
    "    pearson_correlation <- cor(testing_set$child_pugh_score[valid_indices], predictions[valid_indices], method = \"pearson\")\n",
    "    all_pearson_correlations[i] <- pearson_correlation\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation:\", pearson_correlation, \"\\n\"))\n",
    "    \n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(all_pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d33c67fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "\u001b[1m\u001b[22m`geom_smooth()` using formula = 'y ~ x'\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  7  1  1\n",
      "         2  9  8  5  0\n",
      "         3  2  9  6  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5488         \n",
      "                 95% CI : (0.4349, 0.659)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2907         \n",
      "                                         \n",
      "                  Kappa : 0.2905         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381  0.33333  0.50000  0.00000\n",
      "Specificity            0.7750  0.75862  0.80000  1.00000\n",
      "Pos Pred Value         0.7750  0.36364  0.30000      NaN\n",
      "Neg Pred Value         0.7381  0.73333  0.90323  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.3780  0.09756  0.07317  0.00000\n",
      "Detection Prevalence   0.4878  0.26829  0.24390  0.00000\n",
      "Balanced Accuracy      0.7565  0.54598  0.65000  0.50000\n",
      "Fold 1 Accuracy: 0.548780487804878 \n",
      "Fold 1 AUC: 0.713169642857143 \n",
      "Fold 1 Overall Sensitivity: 0.392857142857143 \n",
      "Fold 1 Overall Specificity: 0.833405172413793 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "\u001b[1m\u001b[22m`geom_smooth()` using formula = 'y ~ x'\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  6  0  0\n",
      "         2 10  9  6  1\n",
      "         3  3  9  6  2\n",
      "         4  0  0  1  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5366         \n",
      "                 95% CI : (0.423, 0.6475)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.3706         \n",
      "                                         \n",
      "                  Kappa : 0.2865         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.3750  0.46154  0.00000\n",
      "Specificity            0.8500   0.7069  0.79710  0.98734\n",
      "Pos Pred Value         0.8286   0.3462  0.30000  0.00000\n",
      "Neg Pred Value         0.7234   0.7321  0.88710  0.96296\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3537   0.1098  0.07317  0.00000\n",
      "Detection Prevalence   0.4268   0.3171  0.24390  0.01220\n",
      "Balanced Accuracy      0.7702   0.5409  0.62932  0.49367\n",
      "Fold 2 Accuracy: 0.536585365853659 \n",
      "Fold 2 AUC: 0.739933099308099 \n",
      "Fold 2 Overall Sensitivity: 0.381753663003663 \n",
      "Fold 2 Overall Specificity: 0.83533494328785 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nSUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m`geom_smooth()` using formula = 'y ~ x'\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32 11  3  0\n",
      "         2 10  9  7  2\n",
      "         3  0  4  2  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5244          \n",
      "                 95% CI : (0.4111, 0.6359)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.4564          \n",
      "                                          \n",
      "                  Kappa : 0.2053          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3750  0.16667  0.00000\n",
      "Specificity            0.6500   0.6724  0.91429  1.00000\n",
      "Pos Pred Value         0.6957   0.3214  0.25000      NaN\n",
      "Neg Pred Value         0.7222   0.7222  0.86486  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.3902   0.1098  0.02439  0.00000\n",
      "Detection Prevalence   0.5610   0.3415  0.09756  0.00000\n",
      "Balanced Accuracy      0.7060   0.5237  0.54048  0.50000\n",
      "Fold 3 Accuracy: 0.524390243902439 \n",
      "Fold 3 AUC: 0.747519841269841 \n",
      "Fold 3 Overall Sensitivity: 0.325892857142857 \n",
      "Fold 3 Overall Specificity: 0.809174876847291 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\ngg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m`geom_smooth()` using formula = 'y ~ x'\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  7  0  0\n",
      "         2 10  9  7  0\n",
      "         3  4  6  5  3\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5309          \n",
      "                 95% CI : (0.4167, 0.6427)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.4563          \n",
      "                                          \n",
      "                  Kappa : 0.279           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.3913  0.41667  0.25000\n",
      "Specificity            0.8205   0.7069  0.81159  0.98701\n",
      "Pos Pred Value         0.8000   0.3462  0.27778  0.50000\n",
      "Neg Pred Value         0.6957   0.7455  0.88889  0.96203\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3457   0.1111  0.06173  0.01235\n",
      "Detection Prevalence   0.4321   0.3210  0.22222  0.02469\n",
      "Balanced Accuracy      0.7436   0.5491  0.61413  0.61851\n",
      "Fold 4 Accuracy: 0.530864197530864 \n",
      "Fold 4 AUC: 0.81067043363239 \n",
      "Fold 4 Overall Sensitivity: 0.431159420289855 \n",
      "Fold 4 Overall Specificity: 0.831504140537124 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nAIAkCHYAAACS+B8GmuuVUd8oigAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[22m`geom_smooth()` using formula = 'y ~ x'\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nAAAAAIMEEjsAAACAQQKJHQAAAMAg8R9i/G+3U1SnKgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32  5  1  0\n",
      "         2  8  9  5  0\n",
      "         3  2  8  6  2\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6            \n",
      "                 95% CI : (0.4844, 0.708)\n",
      "    No Information Rate : 0.525          \n",
      "    P-Value [Acc > NIR] : 0.1088         \n",
      "                                         \n",
      "                  Kappa : 0.3719         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3913   0.5000   0.3333\n",
      "Specificity            0.8421   0.7719   0.8235   0.9870\n",
      "Pos Pred Value         0.8421   0.4091   0.3333   0.5000\n",
      "Neg Pred Value         0.7619   0.7586   0.9032   0.9744\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4000   0.1125   0.0750   0.0125\n",
      "Detection Prevalence   0.4750   0.2750   0.2250   0.0250\n",
      "Balanced Accuracy      0.8020   0.5816   0.6618   0.6602\n",
      "Fold 5 Accuracy: 0.6 \n",
      "Fold 5 AUC: 0.819329422590292 \n",
      "Fold 5 Overall Sensitivity: 0.496635610766046 \n",
      "Fold 5 Overall Specificity: 0.856144371624248 \n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 152  36   5   1\n",
      "         2  47  44  30   3\n",
      "         3  11  36  25  12\n",
      "         4   0   2   1   2\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5479         \n",
      "                 95% CI : (0.4981, 0.597)\n",
      "    No Information Rate : 0.516          \n",
      "    P-Value [Acc > NIR] : 0.1074         \n",
      "                                         \n",
      "                  Kappa : 0.2872         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 0.0222         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7238   0.3729  0.40984 0.111111\n",
      "Specificity            0.7868   0.7232  0.82948 0.992288\n",
      "Pos Pred Value         0.7835   0.3548  0.29762 0.400000\n",
      "Neg Pred Value         0.7277   0.7385  0.88854 0.960199\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.3735   0.1081  0.06143 0.004914\n",
      "Detection Prevalence   0.4767   0.3047  0.20639 0.012285\n",
      "Balanced Accuracy      0.7553   0.5480  0.61966 0.551700\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5481241 \n",
      "Average AUC: 0.7661245 \n",
      "Average Sensitivity: 0.4056597 \n",
      "Average Specificity: 0.8331127 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\ndgAAADrx/zDCiWuiOkO2AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 4.5) {\n",
    "    return(1)\n",
    "  } else if (value >= 4.5 & value < 6.5) {\n",
    "    return(2)\n",
    "  } else if (value >= 6.5 & value < 9.5) {\n",
    "    return(3)\n",
    "  } else if (value >= 9.5 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- vector(\"list\", k)\n",
    "all_classified_predictions <- vector()\n",
    "all_classified_actuals <- vector()\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Calculate Pearson correlation\n",
    "    pearson_correlation <- cor(all_predictions_vs_actual[[i]]$Actual, all_predictions_vs_actual[[i]]$Predicted)\n",
    "\n",
    "    # Create the plot\n",
    "    plot <- ggplot(all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"blue\") +\n",
    "      geom_smooth(method = \"lm\", color = \"red\") +  \n",
    "      xlab(\"Actual Child-Pugh Score\") +\n",
    "      ylab(\"Predicted Child-Pugh Score\") +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values (Pearson Correlation:\", round(pearson_correlation, 2), \")\")) +\n",
    "      theme_minimal()\n",
    "\n",
    "    # Print the plot\n",
    "    print(plot)\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(factor(all_classified_predictions, levels = 1:4), \n",
    "                                       factor(all_classified_actuals, levels = 1:4))\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a083a5",
   "metadata": {},
   "source": [
    "# Truncating 100 Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "601af731",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35  9  2  1\n",
      "         2  6  9  6  1\n",
      "         3  1  6  4  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.1120          \n",
      "                                          \n",
      "                  Kappa : 0.3143          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.5524          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.3750  0.33333  0.00000\n",
      "Specificity            0.7000   0.7759  0.87143  1.00000\n",
      "Pos Pred Value         0.7447   0.4091  0.30769      NaN\n",
      "Neg Pred Value         0.8000   0.7500  0.88406  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4268   0.1098  0.04878  0.00000\n",
      "Detection Prevalence   0.5732   0.2683  0.15854  0.00000\n",
      "Balanced Accuracy      0.7667   0.5754  0.60238  0.50000\n",
      "Fold 1 Accuracy: 0.585365853658537 \n",
      "Fold 1 AUC: 0.698164682539682 \n",
      "Fold 1 Overall Sensitivity: 0.385416666666667 \n",
      "Fold 1 Overall Specificity: 0.836822660098522 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.585850562863264 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  8.586404                   8\n",
      "2       3  2.238850                   2\n",
      "3       3  4.546350                   4\n",
      "4       5  5.171442                   5\n",
      "5       5  7.095618                   7\n",
      "6       4  6.954157                   6\n",
      "7       3  4.838752                   4\n",
      "8       5  6.169339                   6\n",
      "9       5  4.908542                   4\n",
      "10      3  3.866497                   3\n",
      "11      3  3.266065                   3\n",
      "12      3  3.055424                   3\n",
      "13      5  4.556442                   4\n",
      "14     10  8.090156                   8\n",
      "15      3  3.068069                   3\n",
      "16      3  2.265068                   2\n",
      "17      6  7.987336                   7\n",
      "18      3  4.114586                   4\n",
      "19      3  3.405042                   3\n",
      "20      3  3.191306                   3\n",
      "21      3  4.466749                   4\n",
      "22      3  3.038775                   3\n",
      "23      3  1.092566                   1\n",
      "24      8  5.676230                   5\n",
      "25      3  3.483192                   3\n",
      "26      3  3.770425                   3\n",
      "27      3  5.011883                   5\n",
      "28      3  3.755493                   3\n",
      "29      3  3.143744                   3\n",
      "30      3  4.250678                   4\n",
      "31      5  1.865943                   1\n",
      "32      3  5.110044                   5\n",
      "33      3  1.948479                   1\n",
      "34      3  2.387621                   2\n",
      "35      5  7.703621                   7\n",
      "36      3  7.627630                   7\n",
      "37      6  5.819073                   5\n",
      "38      6  7.377768                   7\n",
      "39      3  1.657355                   1\n",
      "40      5  3.469572                   3\n",
      "41      5  6.138008                   6\n",
      "42      6  2.961655                   2\n",
      "43      3  1.263724                   1\n",
      "44      7  5.094025                   5\n",
      "45      7  2.717765                   2\n",
      "46      3  5.014835                   5\n",
      "47     11  6.884615                   6\n",
      "48      9  6.386113                   6\n",
      "49      3  5.901272                   5\n",
      "50      3  0.942686                   0\n",
      "51      3  3.676910                   3\n",
      "52      8  8.123538                   8\n",
      "53      6  2.076911                   2\n",
      "54      6  6.890224                   6\n",
      "55      5  6.448359                   6\n",
      "56      5  3.095921                   3\n",
      "57      7  7.869065                   7\n",
      "58      3  3.497546                   3\n",
      "59      3  2.587999                   2\n",
      "60      9  4.559393                   4\n",
      "61      9  7.308262                   7\n",
      "62      9  6.819449                   6\n",
      "63      3  4.098310                   4\n",
      "64      9  7.152524                   7\n",
      "65     10  4.457664                   4\n",
      "66      6  6.572504                   6\n",
      "67      3  2.397025                   2\n",
      "68      5  6.651625                   6\n",
      "69      5  2.197169                   2\n",
      "70      3  4.152147                   4\n",
      "71      6  6.319567                   6\n",
      "72      5  7.818113                   7\n",
      "73      8  6.919941                   6\n",
      "74      5  3.288485                   3\n",
      "75      3  5.377862                   5\n",
      "76      3  3.394215                   3\n",
      "77      7  5.770257                   5\n",
      "78      3  2.665008                   2\n",
      "79      3  4.851649                   4\n",
      "80     11  8.702833                   8\n",
      "81      3  2.450381                   2\n",
      "82      3  4.689708                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  6  0  0\n",
      "         2  6 14  6  1\n",
      "         3  2  4  7  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6707          \n",
      "                 95% CI : (0.5581, 0.7706)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.002659        \n",
      "                                          \n",
      "                  Kappa : 0.473           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.5833  0.53846  0.00000\n",
      "Specificity            0.8500   0.7759  0.88406  1.00000\n",
      "Pos Pred Value         0.8500   0.5185  0.46667      NaN\n",
      "Neg Pred Value         0.8095   0.8182  0.91045  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4146   0.1707  0.08537  0.00000\n",
      "Detection Prevalence   0.4878   0.3293  0.18293  0.00000\n",
      "Balanced Accuracy      0.8298   0.6796  0.71126  0.50000\n",
      "Fold 2 Accuracy: 0.670731707317073 \n",
      "Fold 2 AUC: 0.792302604802605 \n",
      "Fold 2 Overall Sensitivity: 0.48282967032967 \n",
      "Fold 2 Overall Specificity: 0.877480009995003 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.62780241562379 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3 4.5146078                   4\n",
      "2       4 6.5764557                   6\n",
      "3       3 4.3547911                   4\n",
      "4       3 3.6126528                   3\n",
      "5       3 3.1822957                   3\n",
      "6       3 2.4790334                   2\n",
      "7       5 3.8549240                   3\n",
      "8       3 4.0952849                   4\n",
      "9       5 7.8454826                   7\n",
      "10      4 3.0421551                   3\n",
      "11      3 2.4874124                   2\n",
      "12      3 7.3111463                   7\n",
      "13      3 5.1260709                   5\n",
      "14      6 6.5915240                   6\n",
      "15     12 7.1777228                   7\n",
      "16      6 6.9909734                   6\n",
      "17      3 6.4162452                   6\n",
      "18      5 5.4761906                   5\n",
      "19      3 4.5145026                   4\n",
      "20      3 5.1344562                   5\n",
      "21      3 2.1543875                   2\n",
      "22      3 1.7454996                   1\n",
      "23      3 4.1347221                   4\n",
      "24      3 3.5327682                   3\n",
      "25      3 4.6657006                   4\n",
      "26      3 5.2510298                   5\n",
      "27      3 2.0169221                   2\n",
      "28      3 3.9319198                   3\n",
      "29      5 4.0551993                   4\n",
      "30      4 8.5180038                   8\n",
      "31      3 4.9308689                   4\n",
      "32      3 2.6336685                   2\n",
      "33      3 6.2549305                   6\n",
      "34      3 3.4313455                   3\n",
      "35      8 9.5316380                   9\n",
      "36      6 8.6012766                   8\n",
      "37      5 5.0966883                   5\n",
      "38      3 4.1197139                   4\n",
      "39      9 7.0972086                   7\n",
      "40     12 8.3011047                   8\n",
      "41      5 6.2593110                   6\n",
      "42      7 6.2029282                   6\n",
      "43      6 7.5729416                   7\n",
      "44      5 3.9680996                   3\n",
      "45      8 7.8730234                   7\n",
      "46      5 1.9825954                   1\n",
      "47      7 8.3276036                   8\n",
      "48      5 5.4651266                   5\n",
      "49      5 8.6925477                   8\n",
      "50      3 4.1334983                   4\n",
      "51      5 6.1417759                   6\n",
      "52      8 6.3959460                   6\n",
      "53      3 2.1716187                   2\n",
      "54      5 6.1364556                   6\n",
      "55      6 6.7552340                   6\n",
      "56      6 6.6952572                   6\n",
      "57      5 6.8816225                   6\n",
      "58      6 6.2416929                   6\n",
      "59      5 3.1793913                   3\n",
      "60     11 5.1880504                   5\n",
      "61      3 4.5190545                   4\n",
      "62      3 1.6603956                   1\n",
      "63      5 4.3563132                   4\n",
      "64      3 2.0322043                   2\n",
      "65      3 4.4804004                   4\n",
      "66      7 6.2447147                   6\n",
      "67      3 1.4879344                   1\n",
      "68      5 5.5519188                   5\n",
      "69      8 7.4637686                   7\n",
      "70      8 7.6589666                   7\n",
      "71      3 4.4718586                   4\n",
      "72      3 3.3614448                   3\n",
      "73      7 6.3274582                   6\n",
      "74      3 3.0815158                   3\n",
      "75      7 5.1266766                   5\n",
      "76      3 4.3435823                   4\n",
      "77      3 4.0671098                   4\n",
      "78      3 3.2930426                   3\n",
      "79      3 0.4839879                   0\n",
      "80      5 5.1431591                   5\n",
      "81      8 5.3196648                   5\n",
      "82      8 8.9210976                   8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37 13  6  0\n",
      "         2  5  9  4  3\n",
      "         3  0  2  2  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.112           \n",
      "                                          \n",
      "                  Kappa : 0.2679          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810   0.3750  0.16667  0.00000\n",
      "Specificity            0.5250   0.7931  0.95714  1.00000\n",
      "Pos Pred Value         0.6607   0.4286  0.40000      NaN\n",
      "Neg Pred Value         0.8077   0.7541  0.87013  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4512   0.1098  0.02439  0.00000\n",
      "Detection Prevalence   0.6829   0.2561  0.06098  0.00000\n",
      "Balanced Accuracy      0.7030   0.5841  0.56190  0.50000\n",
      "Fold 3 Accuracy: 0.585365853658537 \n",
      "Fold 3 AUC: 0.730696097883598 \n",
      "Fold 3 Overall Sensitivity: 0.355654761904762 \n",
      "Fold 3 Overall Specificity: 0.81881157635468 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.537613515812769 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual   Predicted Truncated_Predicted\n",
      "1       6  1.53612000                   1\n",
      "2       3  5.91794724                   5\n",
      "3       3  2.48202340                   2\n",
      "4       3  3.93087133                   3\n",
      "5       3  5.27295866                   5\n",
      "6       7  9.06291592                   9\n",
      "7       5  4.04973943                   4\n",
      "8       3  3.87102097                   3\n",
      "9       3  6.26525546                   6\n",
      "10      3  3.72160259                   3\n",
      "11      4  3.93258938                   3\n",
      "12      3  4.07982976                   4\n",
      "13      3  4.46999884                   4\n",
      "14      3 -0.06942872                  -1\n",
      "15      8  3.33364575                   3\n",
      "16      3  0.64319724                   0\n",
      "17      3  5.63598142                   5\n",
      "18      5  4.78657479                   4\n",
      "19      3  2.31141101                   2\n",
      "20      7  5.26517194                   5\n",
      "21      3  2.80168552                   2\n",
      "22      3  2.40047014                   2\n",
      "23      6  6.18106992                   6\n",
      "24      8  4.60809576                   4\n",
      "25      3  1.99428260                   1\n",
      "26     10  6.20201558                   6\n",
      "27      3  1.74433091                   1\n",
      "28      3  3.21519635                   3\n",
      "29      3  3.96750444                   3\n",
      "30      3  2.29398033                   2\n",
      "31      3  1.86419054                   1\n",
      "32      6  6.78882902                   6\n",
      "33      6  6.32569766                   6\n",
      "34      7  4.00684707                   4\n",
      "35      5  7.96836775                   7\n",
      "36      3  3.53115944                   3\n",
      "37      5  3.12045493                   3\n",
      "38      5  1.96484848                   1\n",
      "39      3  1.45387563                   1\n",
      "40      6  6.16093802                   6\n",
      "41      3  4.81000996                   4\n",
      "42      8  4.55035707                   4\n",
      "43      5  4.65617304                   4\n",
      "44      5  5.79768265                   5\n",
      "45     12  6.23325302                   6\n",
      "46      5  6.56090919                   6\n",
      "47      6  4.24203156                   4\n",
      "48      5  5.87554763                   5\n",
      "49      6  4.41295968                   4\n",
      "50      3  2.69438560                   2\n",
      "51      5  4.11446285                   4\n",
      "52      5  4.26630966                   4\n",
      "53      3  4.54690171                   4\n",
      "54      7  5.26677694                   5\n",
      "55      8  6.28226803                   6\n",
      "56      3  2.45147769                   2\n",
      "57      3  3.16078603                   3\n",
      "58      5  7.17266743                   7\n",
      "59      7  4.87200139                   4\n",
      "60     10  8.00127483                   8\n",
      "61      5  5.30459771                   5\n",
      "62      3  4.01060016                   4\n",
      "63      3  4.90969726                   4\n",
      "64      5  1.34865828                   1\n",
      "65      5  6.06763476                   6\n",
      "66      3  3.94423731                   3\n",
      "67      5  4.10746653                   4\n",
      "68      3  2.30511496                   2\n",
      "69      8  5.52865083                   5\n",
      "70      3  4.66139887                   4\n",
      "71      9  7.54797224                   7\n",
      "72      3  4.94671851                   4\n",
      "73      3  3.19875872                   3\n",
      "74      3  2.65315050                   2\n",
      "75      3  3.52362356                   3\n",
      "76      3  2.38705521                   2\n",
      "77      3  5.06873818                   5\n",
      "78     10  6.65472719                   6\n",
      "79      8  4.35008255                   4\n",
      "80      3  3.02151817                   3\n",
      "81      5  3.36434542                   3\n",
      "82      3  3.21818583                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 33  8  2  0\n",
      "         2  8  9  6  0\n",
      "         3  1  5  4  3\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5802          \n",
      "                 95% CI : (0.4654, 0.6891)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.1585          \n",
      "                                          \n",
      "                  Kappa : 0.322           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7857   0.3913  0.33333  0.25000\n",
      "Specificity            0.7436   0.7586  0.86957  0.98701\n",
      "Pos Pred Value         0.7674   0.3913  0.30769  0.50000\n",
      "Neg Pred Value         0.7632   0.7586  0.88235  0.96203\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4074   0.1111  0.04938  0.01235\n",
      "Detection Prevalence   0.5309   0.2840  0.16049  0.02469\n",
      "Balanced Accuracy      0.7647   0.5750  0.60145  0.61851\n",
      "Fold 4 Accuracy: 0.580246913580247 \n",
      "Fold 4 AUC: 0.817467506326202 \n",
      "Fold 4 Overall Sensitivity: 0.440087991718426 \n",
      "Fold 4 Overall Specificity: 0.839697159412302 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.646123683883944 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.313330                   4\n",
      "2       3  4.539652                   4\n",
      "3       3  4.631728                   4\n",
      "4       4  6.922639                   6\n",
      "5       3  4.853542                   4\n",
      "6       6  7.316025                   7\n",
      "7       3  4.200451                   4\n",
      "8       4  5.459959                   5\n",
      "9       3  7.343430                   7\n",
      "10      3  5.014862                   5\n",
      "11      3  4.823576                   4\n",
      "12      6  3.563204                   3\n",
      "13      3  2.794440                   2\n",
      "14      3  1.903975                   1\n",
      "15      6  5.386734                   5\n",
      "16      3  3.277842                   3\n",
      "17      5  7.575279                   7\n",
      "18      3  4.450365                   4\n",
      "19      3  2.312539                   2\n",
      "20      9  8.161538                   8\n",
      "21      3  3.192185                   3\n",
      "22      3  3.319337                   3\n",
      "23      3  1.804455                   1\n",
      "24      3  2.840388                   2\n",
      "25      5  5.006015                   5\n",
      "26      3  6.563637                   6\n",
      "27      3  4.786956                   4\n",
      "28      5  6.208354                   6\n",
      "29      5  3.626387                   3\n",
      "30      3  3.749369                   3\n",
      "31      3  5.073185                   5\n",
      "32      3  4.115786                   4\n",
      "33      8  6.742738                   6\n",
      "34      3  4.479225                   4\n",
      "35      3  3.468131                   3\n",
      "36      3  4.230307                   4\n",
      "37      3  3.022107                   3\n",
      "38      3  3.433587                   3\n",
      "39      5  5.077259                   5\n",
      "40      5  8.167677                   8\n",
      "41      6  5.092182                   5\n",
      "42      8  4.535127                   4\n",
      "43      9  5.494329                   5\n",
      "44      6  5.514030                   5\n",
      "45      9  6.428347                   6\n",
      "46      8  6.275537                   6\n",
      "47      3  3.753218                   3\n",
      "48      9  5.259137                   5\n",
      "49      7  8.478213                   8\n",
      "50     11  9.187367                   9\n",
      "51      8  6.264543                   6\n",
      "52      6  7.924666                   7\n",
      "53      5  4.275661                   4\n",
      "54      5  5.023058                   5\n",
      "55      3  3.967514                   3\n",
      "56      3  4.278246                   4\n",
      "57      3  5.384906                   5\n",
      "58      5  6.920882                   6\n",
      "59     12 10.504137                  10\n",
      "60      5  3.108298                   3\n",
      "61      5 10.692194                  10\n",
      "62      5  4.321451                   4\n",
      "63     10  8.294447                   8\n",
      "64      3  3.197316                   3\n",
      "65      5  1.425402                   1\n",
      "66      5  4.759407                   4\n",
      "67     11  8.509217                   8\n",
      "68      5  5.709180                   5\n",
      "69      3  6.544010                   6\n",
      "70      3  4.498166                   4\n",
      "71      3  2.654166                   2\n",
      "72      3  5.115556                   5\n",
      "73      3  3.284409                   3\n",
      "74      3  4.170017                   4\n",
      "75      6  3.123366                   3\n",
      "76      3  3.536088                   3\n",
      "77      7  4.605254                   4\n",
      "78      9  7.907459                   7\n",
      "79      9  8.356541                   8\n",
      "80      6  8.319669                   8\n",
      "81      3  3.273879                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36  9  1  0\n",
      "         2  6  5  6  0\n",
      "         3  0  9  5  2\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5875          \n",
      "                 95% CI : (0.4718, 0.6965)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.1568          \n",
      "                                          \n",
      "                  Kappa : 0.3199          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.2174   0.4167   0.3333\n",
      "Specificity            0.7368   0.7895   0.8382   1.0000\n",
      "Pos Pred Value         0.7826   0.2941   0.3125   1.0000\n",
      "Neg Pred Value         0.8235   0.7143   0.8906   0.9747\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4500   0.0625   0.0625   0.0125\n",
      "Detection Prevalence   0.5750   0.2125   0.2000   0.0125\n",
      "Balanced Accuracy      0.7970   0.5034   0.6275   0.6667\n",
      "Fold 5 Accuracy: 0.5875 \n",
      "Fold 5 AUC: 0.834943639291465 \n",
      "Fold 5 Overall Sensitivity: 0.456133540372671 \n",
      "Fold 5 Overall Specificity: 0.841137770897833 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.698701250270758 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual  Predicted Truncated_Predicted\n",
      "1       3  6.0115671                   6\n",
      "2       3  4.2220313                   4\n",
      "3       3  3.4624508                   3\n",
      "4       3  4.8273377                   4\n",
      "5       3  6.6950447                   6\n",
      "6       3  4.9932358                   4\n",
      "7       5  3.3798212                   3\n",
      "8       5  7.8492929                   7\n",
      "9       4  1.4274760                   1\n",
      "10      3  4.2539116                   4\n",
      "11      7  6.3450229                   6\n",
      "12      3  1.7784142                   1\n",
      "13      7  5.6315160                   5\n",
      "14      3  1.7698328                   1\n",
      "15      3  4.3419335                   4\n",
      "16      8  5.9877573                   5\n",
      "17      3  3.7428955                   3\n",
      "18      5  6.1681106                   6\n",
      "19      3  3.6087007                   3\n",
      "20      3  2.1326893                   2\n",
      "21      3  4.0553741                   4\n",
      "22      3  6.0022415                   6\n",
      "23      5  4.2935866                   4\n",
      "24      3  2.8221544                   2\n",
      "25      6  9.6837416                   9\n",
      "26      6  5.8116334                   5\n",
      "27      5  5.3786511                   5\n",
      "28      5  7.0191108                   7\n",
      "29      5  4.6116597                   4\n",
      "30      3  4.0204475                   4\n",
      "31      3  3.2525126                   3\n",
      "32      4  4.8602159                   4\n",
      "33      7  6.1229599                   6\n",
      "34      3  4.1678666                   4\n",
      "35      3  0.3336202                   0\n",
      "36      5  3.7793994                   3\n",
      "37      5  6.4689309                   6\n",
      "38      7  6.9882447                   6\n",
      "39     10 10.5387493                  10\n",
      "40     10  7.9948061                   7\n",
      "41      3  2.7037541                   2\n",
      "42      3  4.4223567                   4\n",
      "43      8  5.9970951                   5\n",
      "44      3  4.7547385                   4\n",
      "45      7  8.1305426                   8\n",
      "46      3  1.2217954                   1\n",
      "47      7  3.7256029                   3\n",
      "48      6  7.0107744                   7\n",
      "49     11  7.8140070                   7\n",
      "50      3  2.8376885                   2\n",
      "51      6  8.7901585                   8\n",
      "52      6  7.8504806                   7\n",
      "53      3  4.1279211                   4\n",
      "54      3  4.1153760                   4\n",
      "55      9  9.3570316                   9\n",
      "56      3  4.3100292                   4\n",
      "57      6  3.8833026                   3\n",
      "58      3  4.0667764                   4\n",
      "59      5  4.8926072                   4\n",
      "60      5  7.4539351                   7\n",
      "61      3  1.5530113                   1\n",
      "62      3  3.2177917                   3\n",
      "63      3  1.9202919                   1\n",
      "64      9  7.6271800                   7\n",
      "65      6  7.2606816                   7\n",
      "66      5  3.9262762                   3\n",
      "67      7  9.3916246                   9\n",
      "68      3  2.7408500                   2\n",
      "69      4  6.9367162                   6\n",
      "70      3  0.6534420                   0\n",
      "71      5  4.5901975                   4\n",
      "72      6  4.6197961                   4\n",
      "73      8  7.4147288                   7\n",
      "74      6  6.4384773                   6\n",
      "75      3  5.3699272                   5\n",
      "76      3  5.9096986                   5\n",
      "77      3  3.2802899                   3\n",
      "78      3  3.4181398                   3\n",
      "79      5  7.5032575                   7\n",
      "80      3  3.6748416                   3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 175  45  11   1\n",
      "         2  31  46  28   5\n",
      "         3   4  26  22  10\n",
      "         4   0   1   0   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.602           \n",
      "                 95% CI : (0.5526, 0.6499)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.0002959       \n",
      "                                          \n",
      "                  Kappa : 0.3414          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.0032799       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.3898  0.36066 0.111111\n",
      "Specificity            0.7107   0.7785  0.88439 0.997429\n",
      "Pos Pred Value         0.7543   0.4182  0.35484 0.666667\n",
      "Neg Pred Value         0.8000   0.7576  0.88696 0.960396\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4300   0.1130  0.05405 0.004914\n",
      "Detection Prevalence   0.5700   0.2703  0.15233 0.007371\n",
      "Balanced Accuracy      0.7720   0.5842  0.62252 0.554270\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6018421 \n",
      "Average AUC: 0.7747149 \n",
      "Average Sensitivity: 0.4240245 \n",
      "Average Specificity: 0.8427898 \n",
      "Average Pearson Correlation: 0.6192183 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Truncate the predicted values before classification\n",
    "    truncated_predictions <- floor(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(truncated_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Truncated_Predicted = truncated_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a68f20",
   "metadata": {},
   "source": [
    "# 50 markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14eae362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35  7  2  0\n",
      "         2  7 14  4  1\n",
      "         3  0  3  6  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6707          \n",
      "                 95% CI : (0.5581, 0.7706)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.002659        \n",
      "                                          \n",
      "                  Kappa : 0.4611          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.5833  0.50000  0.00000\n",
      "Specificity            0.7750   0.7931  0.91429  1.00000\n",
      "Pos Pred Value         0.7955   0.5385  0.50000      NaN\n",
      "Neg Pred Value         0.8158   0.8214  0.91429  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4268   0.1707  0.07317  0.00000\n",
      "Detection Prevalence   0.5366   0.3171  0.14634  0.00000\n",
      "Balanced Accuracy      0.8042   0.6882  0.70714  0.50000\n",
      "Fold 1 Accuracy: 0.670731707317073 \n",
      "Fold 1 AUC: 0.802951388888889 \n",
      "Fold 1 Overall Sensitivity: 0.479166666666667 \n",
      "Fold 1 Overall Specificity: 0.870597290640394 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.67608308817058 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  8.355219                 8\n",
      "2       3  2.552506                 3\n",
      "3       3  3.971837                 4\n",
      "4       5  4.047165                 4\n",
      "5       5  5.715963                 6\n",
      "6       4  5.531800                 6\n",
      "7       3  4.851384                 5\n",
      "8       5  4.925355                 5\n",
      "9       5  4.791590                 5\n",
      "10      3  3.140588                 3\n",
      "11      3  4.385533                 4\n",
      "12      3  2.646068                 3\n",
      "13      5  5.108160                 5\n",
      "14     10  8.852836                 9\n",
      "15      3  1.571414                 2\n",
      "16      3  2.335469                 2\n",
      "17      6  6.852273                 7\n",
      "18      3  4.188200                 4\n",
      "19      3  3.733021                 4\n",
      "20      3  3.895255                 4\n",
      "21      3  4.900089                 5\n",
      "22      3  3.768994                 4\n",
      "23      3  2.911027                 3\n",
      "24      8  6.466215                 6\n",
      "25      3  3.170091                 3\n",
      "26      3  2.474034                 2\n",
      "27      3  4.249412                 4\n",
      "28      3  4.491310                 4\n",
      "29      3  3.110863                 3\n",
      "30      3  4.477243                 4\n",
      "31      5  2.644215                 3\n",
      "32      3  2.749413                 3\n",
      "33      3  2.617773                 3\n",
      "34      3  1.606416                 2\n",
      "35      5  7.361410                 7\n",
      "36      3  5.871697                 6\n",
      "37      6  5.780382                 6\n",
      "38      6  6.408621                 6\n",
      "39      3  3.326129                 3\n",
      "40      5  3.793904                 4\n",
      "41      5  5.509568                 6\n",
      "42      6  3.660851                 4\n",
      "43      3  2.411438                 2\n",
      "44      7  4.442031                 4\n",
      "45      7  3.505932                 4\n",
      "46      3  3.388502                 3\n",
      "47     11  7.223587                 7\n",
      "48      9  6.073567                 6\n",
      "49      3  5.431765                 5\n",
      "50      3  1.365499                 1\n",
      "51      3  3.641555                 4\n",
      "52      8  7.699729                 8\n",
      "53      6  1.930132                 2\n",
      "54      6  5.980690                 6\n",
      "55      5  6.066861                 6\n",
      "56      5  3.050886                 3\n",
      "57      7  7.731044                 8\n",
      "58      3  3.990033                 4\n",
      "59      3  1.984130                 2\n",
      "60      9  6.878078                 7\n",
      "61      9  5.099333                 5\n",
      "62      9  7.040222                 7\n",
      "63      3  4.030961                 4\n",
      "64      9  6.662983                 7\n",
      "65     10  4.770753                 5\n",
      "66      6  6.156579                 6\n",
      "67      3  2.908100                 3\n",
      "68      5  5.876471                 6\n",
      "69      5  2.255296                 2\n",
      "70      3  3.318804                 3\n",
      "71      6  6.166486                 6\n",
      "72      5  6.353470                 6\n",
      "73      8  7.234769                 7\n",
      "74      5  5.221834                 5\n",
      "75      3  5.631041                 6\n",
      "76      3  2.993922                 3\n",
      "77      7  5.113066                 5\n",
      "78      3  2.926179                 3\n",
      "79      3  4.841184                 5\n",
      "80     11  7.551931                 8\n",
      "81      3  2.073997                 2\n",
      "82      3  3.899639                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 26  5  0  0\n",
      "         2 15 12  5  1\n",
      "         3  1  7  8  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.561          \n",
      "                 95% CI : (0.447, 0.6704)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2199         \n",
      "                                         \n",
      "                  Kappa : 0.3285         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6190   0.5000  0.61538  0.00000\n",
      "Specificity            0.8750   0.6379  0.85507  1.00000\n",
      "Pos Pred Value         0.8387   0.3636  0.44444      NaN\n",
      "Neg Pred Value         0.6863   0.7551  0.92187  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3171   0.1463  0.09756  0.00000\n",
      "Detection Prevalence   0.3780   0.4024  0.21951  0.00000\n",
      "Balanced Accuracy      0.7470   0.5690  0.73523  0.50000\n",
      "Fold 2 Accuracy: 0.560975609756098 \n",
      "Fold 2 AUC: 0.75678545991046 \n",
      "Fold 2 Overall Sensitivity: 0.433608058608059 \n",
      "Fold 2 Overall Specificity: 0.842000874562719 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.677829161408002 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  5.536720                 6\n",
      "2       4  4.990893                 5\n",
      "3       3  4.784804                 5\n",
      "4       3  5.250340                 5\n",
      "5       3  2.757282                 3\n",
      "6       3  2.400030                 2\n",
      "7       5  4.010764                 4\n",
      "8       3  4.501186                 5\n",
      "9       5  7.634701                 8\n",
      "10      4  3.645534                 4\n",
      "11      3  2.061372                 2\n",
      "12      3  6.844030                 7\n",
      "13      3  4.054041                 4\n",
      "14      6  6.707063                 7\n",
      "15     12  7.514332                 8\n",
      "16      6  7.071489                 7\n",
      "17      3  5.340854                 5\n",
      "18      5  6.183279                 6\n",
      "19      3  5.681596                 6\n",
      "20      3  4.531930                 5\n",
      "21      3  2.108646                 2\n",
      "22      3  2.255714                 2\n",
      "23      3  3.032333                 3\n",
      "24      3  5.112438                 5\n",
      "25      3  4.662848                 5\n",
      "26      3  5.973764                 6\n",
      "27      3  3.079866                 3\n",
      "28      3  4.110055                 4\n",
      "29      5  4.432333                 4\n",
      "30      4  6.050577                 6\n",
      "31      3  4.548656                 5\n",
      "32      3  2.321619                 2\n",
      "33      3  4.642031                 5\n",
      "34      3  3.604722                 4\n",
      "35      8  8.252862                 8\n",
      "36      6  7.302976                 7\n",
      "37      5  4.511998                 5\n",
      "38      3  4.446259                 4\n",
      "39      9  7.168859                 7\n",
      "40     12  8.110483                 8\n",
      "41      5  6.042304                 6\n",
      "42      7  5.918954                 6\n",
      "43      6  7.544809                 8\n",
      "44      5  4.084262                 4\n",
      "45      8  7.802884                 8\n",
      "46      5  3.004058                 3\n",
      "47      7  7.667095                 8\n",
      "48      5  4.796822                 5\n",
      "49      5  7.705185                 8\n",
      "50      3  3.487646                 3\n",
      "51      5  6.789013                 7\n",
      "52      8  6.099561                 6\n",
      "53      3  3.243745                 3\n",
      "54      5  6.075504                 6\n",
      "55      6  6.022777                 6\n",
      "56      6  6.434508                 6\n",
      "57      5  5.647480                 6\n",
      "58      6  5.374883                 5\n",
      "59      5  3.362615                 3\n",
      "60     11  5.162582                 5\n",
      "61      3  3.978232                 4\n",
      "62      3  2.436534                 2\n",
      "63      5  4.638247                 5\n",
      "64      3  3.189077                 3\n",
      "65      3  4.838620                 5\n",
      "66      7  6.439727                 6\n",
      "67      3  1.852054                 2\n",
      "68      5  5.779978                 6\n",
      "69      8  7.870840                 8\n",
      "70      8  7.458970                 7\n",
      "71      3  4.161026                 4\n",
      "72      3  4.120566                 4\n",
      "73      7  7.728878                 8\n",
      "74      3  3.126823                 3\n",
      "75      7  5.138813                 5\n",
      "76      3  3.716699                 4\n",
      "77      3  3.767859                 4\n",
      "78      3  3.848858                 4\n",
      "79      3  2.444648                 2\n",
      "80      5  6.116311                 6\n",
      "81      8  4.615721                 5\n",
      "82      8  8.938896                 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34 11  2  0\n",
      "         2  8 11  9  3\n",
      "         3  0  2  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.561          \n",
      "                 95% CI : (0.447, 0.6704)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2199         \n",
      "                                         \n",
      "                  Kappa : 0.2542         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.4583  0.08333  0.00000\n",
      "Specificity            0.6750   0.6552  0.95714  1.00000\n",
      "Pos Pred Value         0.7234   0.3548  0.25000      NaN\n",
      "Neg Pred Value         0.7714   0.7451  0.85897  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4146   0.1341  0.01220  0.00000\n",
      "Detection Prevalence   0.5732   0.3780  0.04878  0.00000\n",
      "Balanced Accuracy      0.7423   0.5568  0.52024  0.50000\n",
      "Fold 3 Accuracy: 0.560975609756098 \n",
      "Fold 3 AUC: 0.746031746031746 \n",
      "Fold 3 Overall Sensitivity: 0.337797619047619 \n",
      "Fold 3 Overall Specificity: 0.82182881773399 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.522683254687344 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  2.927635                 3\n",
      "2       3  5.851833                 6\n",
      "3       3  2.780637                 3\n",
      "4       3  5.607973                 6\n",
      "5       3  4.165954                 4\n",
      "6       7  7.554651                 8\n",
      "7       5  4.104438                 4\n",
      "8       3  4.594627                 5\n",
      "9       3  5.253706                 5\n",
      "10      3  4.325527                 4\n",
      "11      4  3.833473                 4\n",
      "12      3  3.827366                 4\n",
      "13      3  4.624935                 5\n",
      "14      3  2.270082                 2\n",
      "15      8  4.343783                 4\n",
      "16      3  2.979238                 3\n",
      "17      3  6.084467                 6\n",
      "18      5  5.720698                 6\n",
      "19      3  3.803621                 4\n",
      "20      7  5.944984                 6\n",
      "21      3  3.002694                 3\n",
      "22      3  3.087525                 3\n",
      "23      6  5.254243                 5\n",
      "24      8  5.334676                 5\n",
      "25      3  3.249155                 3\n",
      "26     10  5.361047                 5\n",
      "27      3  2.403059                 2\n",
      "28      3  3.782043                 4\n",
      "29      3  3.681497                 4\n",
      "30      3  3.420102                 3\n",
      "31      3  3.060903                 3\n",
      "32      6  4.695303                 5\n",
      "33      6  6.024976                 6\n",
      "34      7  5.252788                 5\n",
      "35      5  6.956231                 7\n",
      "36      3  3.463653                 3\n",
      "37      5  3.471513                 3\n",
      "38      5  2.610052                 3\n",
      "39      3  3.102165                 3\n",
      "40      6  4.907959                 5\n",
      "41      3  3.924035                 4\n",
      "42      8  4.952889                 5\n",
      "43      5  4.206722                 4\n",
      "44      5  4.596172                 5\n",
      "45     12  5.039719                 5\n",
      "46      5  6.066982                 6\n",
      "47      6  4.119785                 4\n",
      "48      5  4.826599                 5\n",
      "49      6  6.147954                 6\n",
      "50      3  3.863712                 4\n",
      "51      5  4.496204                 4\n",
      "52      5  3.229697                 3\n",
      "53      3  4.192466                 4\n",
      "54      7  5.434759                 5\n",
      "55      8  5.709370                 6\n",
      "56      3  3.925098                 4\n",
      "57      3  3.800528                 4\n",
      "58      5  7.602438                 8\n",
      "59      7  5.224808                 5\n",
      "60     10  7.426393                 7\n",
      "61      5  5.029300                 5\n",
      "62      3  5.148014                 5\n",
      "63      3  3.949312                 4\n",
      "64      5  3.800449                 4\n",
      "65      5  4.147367                 4\n",
      "66      3  4.335673                 4\n",
      "67      5  5.125150                 5\n",
      "68      3  4.488036                 4\n",
      "69      8  5.305572                 5\n",
      "70      3  4.373288                 4\n",
      "71      9  6.184268                 6\n",
      "72      3  3.067324                 3\n",
      "73      3  3.308143                 3\n",
      "74      3  2.652920                 3\n",
      "75      3  4.573738                 5\n",
      "76      3  3.130503                 3\n",
      "77      3  3.541897                 4\n",
      "78     10  4.879563                 5\n",
      "79      8  4.401692                 4\n",
      "80      3  3.973299                 4\n",
      "81      5  3.375410                 3\n",
      "82      3  2.776324                 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  7  0  0\n",
      "         2 12 11  5  0\n",
      "         3  1  5  7  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5926          \n",
      "                 95% CI : (0.4777, 0.7005)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.1104          \n",
      "                                          \n",
      "                  Kappa : 0.3649          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.4783  0.58333  0.25000\n",
      "Specificity            0.8205   0.7069  0.86957  1.00000\n",
      "Pos Pred Value         0.8056   0.3929  0.43750  1.00000\n",
      "Neg Pred Value         0.7111   0.7736  0.92308  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3580   0.1358  0.08642  0.01235\n",
      "Detection Prevalence   0.4444   0.3457  0.19753  0.01235\n",
      "Balanced Accuracy      0.7555   0.5926  0.72645  0.62500\n",
      "Fold 4 Accuracy: 0.592592592592593 \n",
      "Fold 4 AUC: 0.846535685530251 \n",
      "Fold 4 Overall Sensitivity: 0.500517598343685 \n",
      "Fold 4 Overall Specificity: 0.849243647407066 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.700361278852242 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  3.940743                 4\n",
      "2       3  3.500501                 4\n",
      "3       3  3.212838                 3\n",
      "4       4  6.303505                 6\n",
      "5       3  3.623339                 4\n",
      "6       6  6.600154                 7\n",
      "7       3  3.194509                 3\n",
      "8       4  6.349317                 6\n",
      "9       3  4.581515                 5\n",
      "10      3  5.280218                 5\n",
      "11      3  4.068276                 4\n",
      "12      6  1.732168                 2\n",
      "13      3  2.862790                 3\n",
      "14      3  1.678720                 2\n",
      "15      6  6.034075                 6\n",
      "16      3  5.286310                 5\n",
      "17      5  7.619360                 8\n",
      "18      3  4.194273                 4\n",
      "19      3  1.357081                 1\n",
      "20      9  7.914094                 8\n",
      "21      3  4.094258                 4\n",
      "22      3  4.405724                 4\n",
      "23      3  4.324405                 4\n",
      "24      3  3.126112                 3\n",
      "25      5  4.985934                 5\n",
      "26      3  4.962079                 5\n",
      "27      3  5.007037                 5\n",
      "28      5  4.612306                 5\n",
      "29      5  3.479020                 3\n",
      "30      3  4.886849                 5\n",
      "31      3  6.914840                 7\n",
      "32      3  3.730569                 4\n",
      "33      8  6.690518                 7\n",
      "34      3  3.513719                 4\n",
      "35      3  2.966014                 3\n",
      "36      3  4.368794                 4\n",
      "37      3  3.970894                 4\n",
      "38      3  4.381377                 4\n",
      "39      5  5.583975                 6\n",
      "40      5  5.754918                 6\n",
      "41      6  6.263327                 6\n",
      "42      8  4.791349                 5\n",
      "43      9  6.222026                 6\n",
      "44      6  4.077885                 4\n",
      "45      9  6.862304                 7\n",
      "46      8  4.965841                 5\n",
      "47      3  3.519778                 4\n",
      "48      9  5.607808                 6\n",
      "49      7  7.437852                 7\n",
      "50     11  8.228305                 8\n",
      "51      8  6.781349                 7\n",
      "52      6  6.817774                 7\n",
      "53      5  4.027704                 4\n",
      "54      5  5.190410                 5\n",
      "55      3  3.906196                 4\n",
      "56      3  3.072035                 3\n",
      "57      3  4.179571                 4\n",
      "58      5  5.142365                 5\n",
      "59     12 10.008720                10\n",
      "60      5  4.726857                 5\n",
      "61      5  8.340183                 8\n",
      "62      5  3.650482                 4\n",
      "63     10  7.862220                 8\n",
      "64      3  2.949609                 3\n",
      "65      5  2.229536                 2\n",
      "66      5  4.182921                 4\n",
      "67     11  8.823892                 9\n",
      "68      5  5.464884                 5\n",
      "69      3  4.481966                 4\n",
      "70      3  5.102807                 5\n",
      "71      3  3.668469                 4\n",
      "72      3  5.785622                 6\n",
      "73      3  5.089523                 5\n",
      "74      3  5.634932                 6\n",
      "75      6  4.597961                 5\n",
      "76      3  3.616171                 4\n",
      "77      7  5.132748                 5\n",
      "78      9  7.831452                 8\n",
      "79      9  8.307667                 8\n",
      "80      6  7.347266                 7\n",
      "81      3  4.268189                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32  4  1  0\n",
      "         2  8 10  6  0\n",
      "         3  2  8  3  2\n",
      "         4  0  1  2  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.575           \n",
      "                 95% CI : (0.4594, 0.6849)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.2169          \n",
      "                                          \n",
      "                  Kappa : 0.3369          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.4348   0.2500   0.3333\n",
      "Specificity            0.8684   0.7544   0.8235   0.9610\n",
      "Pos Pred Value         0.8649   0.4167   0.2000   0.2500\n",
      "Neg Pred Value         0.7674   0.7679   0.8615   0.9737\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4000   0.1250   0.0375   0.0125\n",
      "Detection Prevalence   0.4625   0.3000   0.1875   0.0500\n",
      "Balanced Accuracy      0.8152   0.5946   0.5368   0.6472\n",
      "Fold 5 Accuracy: 0.575 \n",
      "Fold 5 AUC: 0.81071716126064 \n",
      "Fold 5 Overall Sensitivity: 0.445005175983437 \n",
      "Fold 5 Overall Specificity: 0.851843847586882 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.681239333429125 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.888190                 5\n",
      "2       3  3.619607                 4\n",
      "3       3  3.173468                 3\n",
      "4       3  4.244653                 4\n",
      "5       3  7.589254                 8\n",
      "6       3  4.444737                 4\n",
      "7       5  2.831465                 3\n",
      "8       5  6.858144                 7\n",
      "9       4  2.657165                 3\n",
      "10      3  3.802239                 4\n",
      "11      7  6.372094                 6\n",
      "12      3  3.167264                 3\n",
      "13      7  5.504203                 6\n",
      "14      3  2.515861                 3\n",
      "15      3  4.027984                 4\n",
      "16      8  5.085882                 5\n",
      "17      3  3.255781                 3\n",
      "18      5  5.402979                 5\n",
      "19      3  2.655805                 3\n",
      "20      3  3.260161                 3\n",
      "21      3  3.669251                 4\n",
      "22      3  6.028371                 6\n",
      "23      5  5.784831                 6\n",
      "24      3  1.842110                 2\n",
      "25      6  9.853986                10\n",
      "26      6  5.745640                 6\n",
      "27      5  5.737039                 6\n",
      "28      5  5.473216                 5\n",
      "29      5  4.423691                 4\n",
      "30      3  3.411625                 3\n",
      "31      3  4.773614                 5\n",
      "32      4  3.949865                 4\n",
      "33      7  4.558104                 5\n",
      "34      3  4.208660                 4\n",
      "35      3  1.500931                 2\n",
      "36      5  4.175013                 4\n",
      "37      5  5.338469                 5\n",
      "38      7  7.844913                 8\n",
      "39     10 11.433015                11\n",
      "40     10  7.032033                 7\n",
      "41      3  3.494391                 3\n",
      "42      3  4.215021                 4\n",
      "43      8  6.148222                 6\n",
      "44      3  4.786902                 5\n",
      "45      7  7.305283                 7\n",
      "46      3  2.207216                 2\n",
      "47      7  3.897067                 4\n",
      "48      6  6.993716                 7\n",
      "49     11  7.244677                 7\n",
      "50      3  2.964675                 3\n",
      "51      6  9.350143                 9\n",
      "52      6  7.226266                 7\n",
      "53      3  4.484979                 4\n",
      "54      3  3.457639                 3\n",
      "55      9  9.644150                10\n",
      "56      3  3.697540                 4\n",
      "57      6  4.844964                 5\n",
      "58      3  4.817179                 5\n",
      "59      5  5.008414                 5\n",
      "60      5  7.519083                 8\n",
      "61      3  2.062263                 2\n",
      "62      3  2.250895                 2\n",
      "63      3  2.145406                 2\n",
      "64      9  6.187768                 6\n",
      "65      6  6.819069                 7\n",
      "66      5  3.579810                 4\n",
      "67      7  9.676678                10\n",
      "68      3  3.273075                 3\n",
      "69      4  8.478849                 8\n",
      "70      3  3.427553                 3\n",
      "71      5  5.653348                 6\n",
      "72      6  7.507606                 8\n",
      "73      8  7.920406                 8\n",
      "74      6  5.909501                 6\n",
      "75      3  5.217429                 5\n",
      "76      3  5.489933                 5\n",
      "77      3  3.679861                 4\n",
      "78      3  4.918876                 5\n",
      "79      5  6.964171                 7\n",
      "80      3  3.380598                 3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 156  34   5   0\n",
      "         2  50  58  29   5\n",
      "         3   4  25  25  11\n",
      "         4   0   1   2   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5921          \n",
      "                 95% CI : (0.5426, 0.6403)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.001204        \n",
      "                                          \n",
      "                  Kappa : 0.3497          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7429   0.4915  0.40984 0.111111\n",
      "Specificity            0.8020   0.7093  0.88439 0.992288\n",
      "Pos Pred Value         0.8000   0.4085  0.38462 0.400000\n",
      "Neg Pred Value         0.7453   0.7736  0.89474 0.960199\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.3833   0.1425  0.06143 0.004914\n",
      "Detection Prevalence   0.4791   0.3489  0.15971 0.012285\n",
      "Balanced Accuracy      0.7724   0.6004  0.64711 0.551700\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5920551 \n",
      "Average AUC: 0.7926043 \n",
      "Average Sensitivity: 0.439219 \n",
      "Average Specificity: 0.8471029 \n",
      "Average Pearson Correlation: 0.6516392 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 50)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be102020",
   "metadata": {},
   "source": [
    "# 50 markers Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90543bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38  9  2  1\n",
      "         2  4 13  6  0\n",
      "         3  0  2  4  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6707          \n",
      "                 95% CI : (0.5581, 0.7706)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.002659        \n",
      "                                          \n",
      "                  Kappa : 0.4415          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.5417  0.33333  0.00000\n",
      "Specificity            0.7000   0.8276  0.92857  1.00000\n",
      "Pos Pred Value         0.7600   0.5652  0.44444      NaN\n",
      "Neg Pred Value         0.8750   0.8136  0.89041  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4634   0.1585  0.04878  0.00000\n",
      "Detection Prevalence   0.6098   0.2805  0.10976  0.00000\n",
      "Balanced Accuracy      0.8024   0.6846  0.63095  0.50000\n",
      "Fold 1 Accuracy: 0.670731707317073 \n",
      "Fold 1 AUC: 0.766989087301587 \n",
      "Fold 1 Overall Sensitivity: 0.444940476190476 \n",
      "Fold 1 Overall Specificity: 0.864039408866995 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.67608308817058 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  8.355219                   8\n",
      "2       3  2.552506                   2\n",
      "3       3  3.971837                   3\n",
      "4       5  4.047165                   4\n",
      "5       5  5.715963                   5\n",
      "6       4  5.531800                   5\n",
      "7       3  4.851384                   4\n",
      "8       5  4.925355                   4\n",
      "9       5  4.791590                   4\n",
      "10      3  3.140588                   3\n",
      "11      3  4.385533                   4\n",
      "12      3  2.646068                   2\n",
      "13      5  5.108160                   5\n",
      "14     10  8.852836                   8\n",
      "15      3  1.571414                   1\n",
      "16      3  2.335469                   2\n",
      "17      6  6.852273                   6\n",
      "18      3  4.188200                   4\n",
      "19      3  3.733021                   3\n",
      "20      3  3.895255                   3\n",
      "21      3  4.900089                   4\n",
      "22      3  3.768994                   3\n",
      "23      3  2.911027                   2\n",
      "24      8  6.466215                   6\n",
      "25      3  3.170091                   3\n",
      "26      3  2.474034                   2\n",
      "27      3  4.249412                   4\n",
      "28      3  4.491310                   4\n",
      "29      3  3.110863                   3\n",
      "30      3  4.477243                   4\n",
      "31      5  2.644215                   2\n",
      "32      3  2.749413                   2\n",
      "33      3  2.617773                   2\n",
      "34      3  1.606416                   1\n",
      "35      5  7.361410                   7\n",
      "36      3  5.871697                   5\n",
      "37      6  5.780382                   5\n",
      "38      6  6.408621                   6\n",
      "39      3  3.326129                   3\n",
      "40      5  3.793904                   3\n",
      "41      5  5.509568                   5\n",
      "42      6  3.660851                   3\n",
      "43      3  2.411438                   2\n",
      "44      7  4.442031                   4\n",
      "45      7  3.505932                   3\n",
      "46      3  3.388502                   3\n",
      "47     11  7.223587                   7\n",
      "48      9  6.073567                   6\n",
      "49      3  5.431765                   5\n",
      "50      3  1.365499                   1\n",
      "51      3  3.641555                   3\n",
      "52      8  7.699729                   7\n",
      "53      6  1.930132                   1\n",
      "54      6  5.980690                   5\n",
      "55      5  6.066861                   6\n",
      "56      5  3.050886                   3\n",
      "57      7  7.731044                   7\n",
      "58      3  3.990033                   3\n",
      "59      3  1.984130                   1\n",
      "60      9  6.878078                   6\n",
      "61      9  5.099333                   5\n",
      "62      9  7.040222                   7\n",
      "63      3  4.030961                   4\n",
      "64      9  6.662983                   6\n",
      "65     10  4.770753                   4\n",
      "66      6  6.156579                   6\n",
      "67      3  2.908100                   2\n",
      "68      5  5.876471                   5\n",
      "69      5  2.255296                   2\n",
      "70      3  3.318804                   3\n",
      "71      6  6.166486                   6\n",
      "72      5  6.353470                   6\n",
      "73      8  7.234769                   7\n",
      "74      5  5.221834                   5\n",
      "75      3  5.631041                   5\n",
      "76      3  2.993922                   2\n",
      "77      7  5.113066                   5\n",
      "78      3  2.926179                   2\n",
      "79      3  4.841184                   4\n",
      "80     11  7.551931                   7\n",
      "81      3  2.073997                   2\n",
      "82      3  3.899639                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  8  1  0\n",
      "         2  8 11  4  1\n",
      "         3  0  5  8  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6463         \n",
      "                 95% CI : (0.533, 0.7488)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.009787       \n",
      "                                         \n",
      "                  Kappa : 0.4266         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.4583  0.61538  0.00000\n",
      "Specificity            0.7750   0.7759  0.89855  1.00000\n",
      "Pos Pred Value         0.7907   0.4583  0.53333      NaN\n",
      "Neg Pred Value         0.7949   0.7759  0.92537  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4146   0.1341  0.09756  0.00000\n",
      "Detection Prevalence   0.5244   0.2927  0.18293  0.00000\n",
      "Balanced Accuracy      0.7923   0.6171  0.75697  0.50000\n",
      "Fold 2 Accuracy: 0.646341463414634 \n",
      "Fold 2 AUC: 0.785243691493692 \n",
      "Fold 2 Overall Sensitivity: 0.47081043956044 \n",
      "Fold 2 Overall Specificity: 0.8623531984008 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.677829161408002 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  5.536720                   5\n",
      "2       4  4.990893                   4\n",
      "3       3  4.784804                   4\n",
      "4       3  5.250340                   5\n",
      "5       3  2.757282                   2\n",
      "6       3  2.400030                   2\n",
      "7       5  4.010764                   4\n",
      "8       3  4.501186                   4\n",
      "9       5  7.634701                   7\n",
      "10      4  3.645534                   3\n",
      "11      3  2.061372                   2\n",
      "12      3  6.844030                   6\n",
      "13      3  4.054041                   4\n",
      "14      6  6.707063                   6\n",
      "15     12  7.514332                   7\n",
      "16      6  7.071489                   7\n",
      "17      3  5.340854                   5\n",
      "18      5  6.183279                   6\n",
      "19      3  5.681596                   5\n",
      "20      3  4.531930                   4\n",
      "21      3  2.108646                   2\n",
      "22      3  2.255714                   2\n",
      "23      3  3.032333                   3\n",
      "24      3  5.112438                   5\n",
      "25      3  4.662848                   4\n",
      "26      3  5.973764                   5\n",
      "27      3  3.079866                   3\n",
      "28      3  4.110055                   4\n",
      "29      5  4.432333                   4\n",
      "30      4  6.050577                   6\n",
      "31      3  4.548656                   4\n",
      "32      3  2.321619                   2\n",
      "33      3  4.642031                   4\n",
      "34      3  3.604722                   3\n",
      "35      8  8.252862                   8\n",
      "36      6  7.302976                   7\n",
      "37      5  4.511998                   4\n",
      "38      3  4.446259                   4\n",
      "39      9  7.168859                   7\n",
      "40     12  8.110483                   8\n",
      "41      5  6.042304                   6\n",
      "42      7  5.918954                   5\n",
      "43      6  7.544809                   7\n",
      "44      5  4.084262                   4\n",
      "45      8  7.802884                   7\n",
      "46      5  3.004058                   3\n",
      "47      7  7.667095                   7\n",
      "48      5  4.796822                   4\n",
      "49      5  7.705185                   7\n",
      "50      3  3.487646                   3\n",
      "51      5  6.789013                   6\n",
      "52      8  6.099561                   6\n",
      "53      3  3.243745                   3\n",
      "54      5  6.075504                   6\n",
      "55      6  6.022777                   6\n",
      "56      6  6.434508                   6\n",
      "57      5  5.647480                   5\n",
      "58      6  5.374883                   5\n",
      "59      5  3.362615                   3\n",
      "60     11  5.162582                   5\n",
      "61      3  3.978232                   3\n",
      "62      3  2.436534                   2\n",
      "63      5  4.638247                   4\n",
      "64      3  3.189077                   3\n",
      "65      3  4.838620                   4\n",
      "66      7  6.439727                   6\n",
      "67      3  1.852054                   1\n",
      "68      5  5.779978                   5\n",
      "69      8  7.870840                   7\n",
      "70      8  7.458970                   7\n",
      "71      3  4.161026                   4\n",
      "72      3  4.120566                   4\n",
      "73      7  7.728878                   7\n",
      "74      3  3.126823                   3\n",
      "75      7  5.138813                   5\n",
      "76      3  3.716699                   3\n",
      "77      3  3.767859                   3\n",
      "78      3  3.848858                   3\n",
      "79      3  2.444648                   2\n",
      "80      5  6.116311                   6\n",
      "81      8  4.615721                   4\n",
      "82      8  8.938896                   8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37 15  3  1\n",
      "         2  5  8  8  2\n",
      "         3  0  1  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.561          \n",
      "                 95% CI : (0.447, 0.6704)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.219893       \n",
      "                                         \n",
      "                  Kappa : 0.2199         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 0.007781       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810  0.33333  0.08333  0.00000\n",
      "Specificity            0.5250  0.74138  0.97143  1.00000\n",
      "Pos Pred Value         0.6607  0.34783  0.33333      NaN\n",
      "Neg Pred Value         0.8077  0.72881  0.86076  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4512  0.09756  0.01220  0.00000\n",
      "Detection Prevalence   0.6829  0.28049  0.03659  0.00000\n",
      "Balanced Accuracy      0.7030  0.53736  0.52738  0.50000\n",
      "Fold 3 Accuracy: 0.560975609756098 \n",
      "Fold 3 AUC: 0.708333333333333 \n",
      "Fold 3 Overall Sensitivity: 0.324404761904762 \n",
      "Fold 3 Overall Specificity: 0.80945197044335 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.522683254687344 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  2.927635                   2\n",
      "2       3  5.851833                   5\n",
      "3       3  2.780637                   2\n",
      "4       3  5.607973                   5\n",
      "5       3  4.165954                   4\n",
      "6       7  7.554651                   7\n",
      "7       5  4.104438                   4\n",
      "8       3  4.594627                   4\n",
      "9       3  5.253706                   5\n",
      "10      3  4.325527                   4\n",
      "11      4  3.833473                   3\n",
      "12      3  3.827366                   3\n",
      "13      3  4.624935                   4\n",
      "14      3  2.270082                   2\n",
      "15      8  4.343783                   4\n",
      "16      3  2.979238                   2\n",
      "17      3  6.084467                   6\n",
      "18      5  5.720698                   5\n",
      "19      3  3.803621                   3\n",
      "20      7  5.944984                   5\n",
      "21      3  3.002694                   3\n",
      "22      3  3.087525                   3\n",
      "23      6  5.254243                   5\n",
      "24      8  5.334676                   5\n",
      "25      3  3.249155                   3\n",
      "26     10  5.361047                   5\n",
      "27      3  2.403059                   2\n",
      "28      3  3.782043                   3\n",
      "29      3  3.681497                   3\n",
      "30      3  3.420102                   3\n",
      "31      3  3.060903                   3\n",
      "32      6  4.695303                   4\n",
      "33      6  6.024976                   6\n",
      "34      7  5.252788                   5\n",
      "35      5  6.956231                   6\n",
      "36      3  3.463653                   3\n",
      "37      5  3.471513                   3\n",
      "38      5  2.610052                   2\n",
      "39      3  3.102165                   3\n",
      "40      6  4.907959                   4\n",
      "41      3  3.924035                   3\n",
      "42      8  4.952889                   4\n",
      "43      5  4.206722                   4\n",
      "44      5  4.596172                   4\n",
      "45     12  5.039719                   5\n",
      "46      5  6.066982                   6\n",
      "47      6  4.119785                   4\n",
      "48      5  4.826599                   4\n",
      "49      6  6.147954                   6\n",
      "50      3  3.863712                   3\n",
      "51      5  4.496204                   4\n",
      "52      5  3.229697                   3\n",
      "53      3  4.192466                   4\n",
      "54      7  5.434759                   5\n",
      "55      8  5.709370                   5\n",
      "56      3  3.925098                   3\n",
      "57      3  3.800528                   3\n",
      "58      5  7.602438                   7\n",
      "59      7  5.224808                   5\n",
      "60     10  7.426393                   7\n",
      "61      5  5.029300                   5\n",
      "62      3  5.148014                   5\n",
      "63      3  3.949312                   3\n",
      "64      5  3.800449                   3\n",
      "65      5  4.147367                   4\n",
      "66      3  4.335673                   4\n",
      "67      5  5.125150                   5\n",
      "68      3  4.488036                   4\n",
      "69      8  5.305572                   5\n",
      "70      3  4.373288                   4\n",
      "71      9  6.184268                   6\n",
      "72      3  3.067324                   3\n",
      "73      3  3.308143                   3\n",
      "74      3  2.652920                   2\n",
      "75      3  4.573738                   4\n",
      "76      3  3.130503                   3\n",
      "77      3  3.541897                   3\n",
      "78     10  4.879563                   4\n",
      "79      8  4.401692                   4\n",
      "80      3  3.973299                   3\n",
      "81      5  3.375410                   3\n",
      "82      3  2.776324                   2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32 11  2  0\n",
      "         2 10  9  6  0\n",
      "         3  0  3  4  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5679          \n",
      "                 95% CI : (0.4531, 0.6776)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.2185          \n",
      "                                          \n",
      "                  Kappa : 0.2863          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3913  0.33333  0.25000\n",
      "Specificity            0.6667   0.7241  0.91304  1.00000\n",
      "Pos Pred Value         0.7111   0.3600  0.40000  1.00000\n",
      "Neg Pred Value         0.7222   0.7500  0.88732  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3951   0.1111  0.04938  0.01235\n",
      "Detection Prevalence   0.5556   0.3086  0.12346  0.01235\n",
      "Balanced Accuracy      0.7143   0.5577  0.62319  0.62500\n",
      "Fold 4 Accuracy: 0.567901234567901 \n",
      "Fold 4 AUC: 0.834857372900851 \n",
      "Fold 4 Overall Sensitivity: 0.434135610766046 \n",
      "Fold 4 Overall Specificity: 0.825962018990505 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.700361278852242 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  3.940743                   3\n",
      "2       3  3.500501                   3\n",
      "3       3  3.212838                   3\n",
      "4       4  6.303505                   6\n",
      "5       3  3.623339                   3\n",
      "6       6  6.600154                   6\n",
      "7       3  3.194509                   3\n",
      "8       4  6.349317                   6\n",
      "9       3  4.581515                   4\n",
      "10      3  5.280218                   5\n",
      "11      3  4.068276                   4\n",
      "12      6  1.732168                   1\n",
      "13      3  2.862790                   2\n",
      "14      3  1.678720                   1\n",
      "15      6  6.034075                   6\n",
      "16      3  5.286310                   5\n",
      "17      5  7.619360                   7\n",
      "18      3  4.194273                   4\n",
      "19      3  1.357081                   1\n",
      "20      9  7.914094                   7\n",
      "21      3  4.094258                   4\n",
      "22      3  4.405724                   4\n",
      "23      3  4.324405                   4\n",
      "24      3  3.126112                   3\n",
      "25      5  4.985934                   4\n",
      "26      3  4.962079                   4\n",
      "27      3  5.007037                   5\n",
      "28      5  4.612306                   4\n",
      "29      5  3.479020                   3\n",
      "30      3  4.886849                   4\n",
      "31      3  6.914840                   6\n",
      "32      3  3.730569                   3\n",
      "33      8  6.690518                   6\n",
      "34      3  3.513719                   3\n",
      "35      3  2.966014                   2\n",
      "36      3  4.368794                   4\n",
      "37      3  3.970894                   3\n",
      "38      3  4.381377                   4\n",
      "39      5  5.583975                   5\n",
      "40      5  5.754918                   5\n",
      "41      6  6.263327                   6\n",
      "42      8  4.791349                   4\n",
      "43      9  6.222026                   6\n",
      "44      6  4.077885                   4\n",
      "45      9  6.862304                   6\n",
      "46      8  4.965841                   4\n",
      "47      3  3.519778                   3\n",
      "48      9  5.607808                   5\n",
      "49      7  7.437852                   7\n",
      "50     11  8.228305                   8\n",
      "51      8  6.781349                   6\n",
      "52      6  6.817774                   6\n",
      "53      5  4.027704                   4\n",
      "54      5  5.190410                   5\n",
      "55      3  3.906196                   3\n",
      "56      3  3.072035                   3\n",
      "57      3  4.179571                   4\n",
      "58      5  5.142365                   5\n",
      "59     12 10.008720                  10\n",
      "60      5  4.726857                   4\n",
      "61      5  8.340183                   8\n",
      "62      5  3.650482                   3\n",
      "63     10  7.862220                   7\n",
      "64      3  2.949609                   2\n",
      "65      5  2.229536                   2\n",
      "66      5  4.182921                   4\n",
      "67     11  8.823892                   8\n",
      "68      5  5.464884                   5\n",
      "69      3  4.481966                   4\n",
      "70      3  5.102807                   5\n",
      "71      3  3.668469                   3\n",
      "72      3  5.785622                   5\n",
      "73      3  5.089523                   5\n",
      "74      3  5.634932                   5\n",
      "75      6  4.597961                   4\n",
      "76      3  3.616171                   3\n",
      "77      7  5.132748                   5\n",
      "78      9  7.831452                   7\n",
      "79      9  8.307667                   8\n",
      "80      6  7.347266                   7\n",
      "81      3  4.268189                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37  5  2  0\n",
      "         2  3 13  5  0\n",
      "         3  2  5  5  2\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.7             \n",
      "                 95% CI : (0.5872, 0.7974)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.001086        \n",
      "                                          \n",
      "                  Kappa : 0.5074          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810   0.5652   0.4167   0.3333\n",
      "Specificity            0.8158   0.8596   0.8676   1.0000\n",
      "Pos Pred Value         0.8409   0.6190   0.3571   1.0000\n",
      "Neg Pred Value         0.8611   0.8305   0.8939   0.9747\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4625   0.1625   0.0625   0.0125\n",
      "Detection Prevalence   0.5500   0.2625   0.1750   0.0125\n",
      "Balanced Accuracy      0.8484   0.7124   0.6422   0.6667\n",
      "Fold 5 Accuracy: 0.7 \n",
      "Fold 5 AUC: 0.843131757533932 \n",
      "Fold 5 Overall Sensitivity: 0.549042443064182 \n",
      "Fold 5 Overall Specificity: 0.885771413828689 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.681239333429125 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.888190                   4\n",
      "2       3  3.619607                   3\n",
      "3       3  3.173468                   3\n",
      "4       3  4.244653                   4\n",
      "5       3  7.589254                   7\n",
      "6       3  4.444737                   4\n",
      "7       5  2.831465                   2\n",
      "8       5  6.858144                   6\n",
      "9       4  2.657165                   2\n",
      "10      3  3.802239                   3\n",
      "11      7  6.372094                   6\n",
      "12      3  3.167264                   3\n",
      "13      7  5.504203                   5\n",
      "14      3  2.515861                   2\n",
      "15      3  4.027984                   4\n",
      "16      8  5.085882                   5\n",
      "17      3  3.255781                   3\n",
      "18      5  5.402979                   5\n",
      "19      3  2.655805                   2\n",
      "20      3  3.260161                   3\n",
      "21      3  3.669251                   3\n",
      "22      3  6.028371                   6\n",
      "23      5  5.784831                   5\n",
      "24      3  1.842110                   1\n",
      "25      6  9.853986                   9\n",
      "26      6  5.745640                   5\n",
      "27      5  5.737039                   5\n",
      "28      5  5.473216                   5\n",
      "29      5  4.423691                   4\n",
      "30      3  3.411625                   3\n",
      "31      3  4.773614                   4\n",
      "32      4  3.949865                   3\n",
      "33      7  4.558104                   4\n",
      "34      3  4.208660                   4\n",
      "35      3  1.500931                   1\n",
      "36      5  4.175013                   4\n",
      "37      5  5.338469                   5\n",
      "38      7  7.844913                   7\n",
      "39     10 11.433015                  11\n",
      "40     10  7.032033                   7\n",
      "41      3  3.494391                   3\n",
      "42      3  4.215021                   4\n",
      "43      8  6.148222                   6\n",
      "44      3  4.786902                   4\n",
      "45      7  7.305283                   7\n",
      "46      3  2.207216                   2\n",
      "47      7  3.897067                   3\n",
      "48      6  6.993716                   6\n",
      "49     11  7.244677                   7\n",
      "50      3  2.964675                   2\n",
      "51      6  9.350143                   9\n",
      "52      6  7.226266                   7\n",
      "53      3  4.484979                   4\n",
      "54      3  3.457639                   3\n",
      "55      9  9.644150                   9\n",
      "56      3  3.697540                   3\n",
      "57      6  4.844964                   4\n",
      "58      3  4.817179                   4\n",
      "59      5  5.008414                   5\n",
      "60      5  7.519083                   7\n",
      "61      3  2.062263                   2\n",
      "62      3  2.250895                   2\n",
      "63      3  2.145406                   2\n",
      "64      9  6.187768                   6\n",
      "65      6  6.819069                   6\n",
      "66      5  3.579810                   3\n",
      "67      7  9.676678                   9\n",
      "68      3  3.273075                   3\n",
      "69      4  8.478849                   8\n",
      "70      3  3.427553                   3\n",
      "71      5  5.653348                   5\n",
      "72      6  7.507606                   7\n",
      "73      8  7.920406                   7\n",
      "74      6  5.909501                   5\n",
      "75      3  5.217429                   5\n",
      "76      3  5.489933                   5\n",
      "77      3  3.679861                   3\n",
      "78      3  4.918876                   4\n",
      "79      5  6.964171                   6\n",
      "80      3  3.380598                   3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 178  48  10   2\n",
      "         2  30  54  29   3\n",
      "         3   2  16  22  11\n",
      "         4   0   0   0   2\n",
      "\n",
      "Overall Statistics\n",
      "                                        \n",
      "               Accuracy : 0.629         \n",
      "                 95% CI : (0.58, 0.6761)\n",
      "    No Information Rate : 0.516         \n",
      "    P-Value [Acc > NIR] : 2.789e-06     \n",
      "                                        \n",
      "                  Kappa : 0.3782        \n",
      "                                        \n",
      " Mcnemar's Test P-Value : 5.473e-05     \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8476   0.4576  0.36066 0.111111\n",
      "Specificity            0.6954   0.7855  0.91618 1.000000\n",
      "Pos Pred Value         0.7479   0.4655  0.43137 1.000000\n",
      "Neg Pred Value         0.8107   0.7801  0.89045 0.960494\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4373   0.1327  0.05405 0.004914\n",
      "Detection Prevalence   0.5848   0.2850  0.12531 0.004914\n",
      "Balanced Accuracy      0.7715   0.6215  0.63842 0.555556\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.62919 \n",
      "Average AUC: 0.787711 \n",
      "Average Sensitivity: 0.4446667 \n",
      "Average Specificity: 0.8495156 \n",
      "Average Pearson Correlation: 0.6516392 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 50)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Truncate the predicted values before classification\n",
    "    truncated_predictions <- floor(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(truncated_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Truncated_Predicted = truncated_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8f94b",
   "metadata": {},
   "source": [
    "# 25 Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e83d76ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  7  0  0\n",
      "         2  8 13  7  1\n",
      "         3  0  4  5  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6341          \n",
      "                 95% CI : (0.5205, 0.7378)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.01746         \n",
      "                                          \n",
      "                  Kappa : 0.4089          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.5417  0.41667  0.00000\n",
      "Specificity            0.8250   0.7241  0.90000  1.00000\n",
      "Pos Pred Value         0.8293   0.4483  0.41667      NaN\n",
      "Neg Pred Value         0.8049   0.7925  0.90000  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4146   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.5000   0.3537  0.14634  0.00000\n",
      "Balanced Accuracy      0.8173   0.6329  0.65833  0.50000\n",
      "Fold 1 Accuracy: 0.634146341463415 \n",
      "Fold 1 AUC: 0.81671626984127 \n",
      "Fold 1 Overall Sensitivity: 0.441964285714286 \n",
      "Fold 1 Overall Specificity: 0.862284482758621 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.710321857592923 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  7.775430                 8\n",
      "2       3  2.978218                 3\n",
      "3       3  3.749069                 4\n",
      "4       5  3.943935                 4\n",
      "5       5  5.976147                 6\n",
      "6       4  5.996992                 6\n",
      "7       3  4.942150                 5\n",
      "8       5  4.555329                 5\n",
      "9       5  4.613303                 5\n",
      "10      3  3.717765                 4\n",
      "11      3  4.816445                 5\n",
      "12      3  3.433309                 3\n",
      "13      5  5.377219                 5\n",
      "14     10  9.414483                 9\n",
      "15      3  3.318553                 3\n",
      "16      3  3.021815                 3\n",
      "17      6  7.217221                 7\n",
      "18      3  4.309497                 4\n",
      "19      3  2.501869                 3\n",
      "20      3  3.695681                 4\n",
      "21      3  5.350545                 5\n",
      "22      3  3.102687                 3\n",
      "23      3  2.866556                 3\n",
      "24      8  7.261834                 7\n",
      "25      3  3.400767                 3\n",
      "26      3  2.808754                 3\n",
      "27      3  4.409124                 4\n",
      "28      3  4.330890                 4\n",
      "29      3  4.485528                 4\n",
      "30      3  3.667582                 4\n",
      "31      5  2.720271                 3\n",
      "32      3  3.139868                 3\n",
      "33      3  3.194654                 3\n",
      "34      3  1.887526                 2\n",
      "35      5  6.141328                 6\n",
      "36      3  5.257840                 5\n",
      "37      6  5.776188                 6\n",
      "38      6  6.559079                 7\n",
      "39      3  4.155356                 4\n",
      "40      5  3.432640                 3\n",
      "41      5  5.373151                 5\n",
      "42      6  3.974664                 4\n",
      "43      3  2.574627                 3\n",
      "44      7  4.763894                 5\n",
      "45      7  4.805254                 5\n",
      "46      3  2.842724                 3\n",
      "47     11  7.565403                 8\n",
      "48      9  5.737079                 6\n",
      "49      3  4.612268                 5\n",
      "50      3  2.638190                 3\n",
      "51      3  3.526564                 4\n",
      "52      8  8.555764                 9\n",
      "53      6  2.299859                 2\n",
      "54      6  5.473819                 5\n",
      "55      5  5.335785                 5\n",
      "56      5  3.558772                 4\n",
      "57      7  7.646187                 8\n",
      "58      3  3.943266                 4\n",
      "59      3  2.669960                 3\n",
      "60      9  6.245771                 6\n",
      "61      9  5.369682                 5\n",
      "62      9  7.186973                 7\n",
      "63      3  4.431772                 4\n",
      "64      9  6.319286                 6\n",
      "65     10  4.941484                 5\n",
      "66      6  6.183178                 6\n",
      "67      3  4.403622                 4\n",
      "68      5  6.040269                 6\n",
      "69      5  2.450592                 2\n",
      "70      3  3.119501                 3\n",
      "71      6  5.472508                 5\n",
      "72      5  6.584960                 7\n",
      "73      8  7.246942                 7\n",
      "74      5  5.215209                 5\n",
      "75      3  6.129046                 6\n",
      "76      3  2.685936                 3\n",
      "77      7  4.572984                 5\n",
      "78      3  3.458838                 3\n",
      "79      3  4.530656                 5\n",
      "80     11  8.412297                 8\n",
      "81      3  2.968185                 3\n",
      "82      3  3.985776                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  4  0  1\n",
      "         2 11 14  8  1\n",
      "         3  2  6  5  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.1120          \n",
      "                                          \n",
      "                  Kappa : 0.3513          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.2004          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.5833  0.38462  0.00000\n",
      "Specificity            0.8750   0.6552  0.86957  1.00000\n",
      "Pos Pred Value         0.8529   0.4118  0.35714      NaN\n",
      "Neg Pred Value         0.7292   0.7917  0.88235  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3537   0.1707  0.06098  0.00000\n",
      "Detection Prevalence   0.4146   0.4146  0.17073  0.00000\n",
      "Balanced Accuracy      0.7827   0.6193  0.62709  0.50000\n",
      "Fold 2 Accuracy: 0.585365853658537 \n",
      "Fold 2 AUC: 0.639766483516483 \n",
      "Fold 2 Overall Sensitivity: 0.414606227106227 \n",
      "Fold 2 Overall Specificity: 0.849934407796102 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.51734152451264 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  5.080454                 5\n",
      "2       4  4.418183                 4\n",
      "3       3  5.251783                 5\n",
      "4       3  5.555921                 6\n",
      "5       3  3.276293                 3\n",
      "6       3  2.415562                 2\n",
      "7       5  4.768613                 5\n",
      "8       3  5.185938                 5\n",
      "9       5  9.283583                 9\n",
      "10      4  3.325134                 3\n",
      "11      3  3.256691                 3\n",
      "12      3  6.750168                 7\n",
      "13      3  4.433676                 4\n",
      "14      6  5.587202                 6\n",
      "15     12  6.889142                 7\n",
      "16      6  7.375101                 7\n",
      "17      3  4.278207                 4\n",
      "18      5  5.647632                 6\n",
      "19      3  4.154331                 4\n",
      "20      3  4.327018                 4\n",
      "21      3  3.032666                 3\n",
      "22      3  3.626426                 4\n",
      "23      3  3.550895                 4\n",
      "24      3  2.901863                 3\n",
      "25      3  5.260062                 5\n",
      "26      3  5.437737                 5\n",
      "27      3  3.593859                 4\n",
      "28      3  3.945161                 4\n",
      "29      5  4.616183                 5\n",
      "30      4  9.342939                 9\n",
      "31      3  5.236723                 5\n",
      "32      3  3.369366                 3\n",
      "33      3  4.664736                 5\n",
      "34      3  3.454382                 3\n",
      "35      8  5.622154                 6\n",
      "36      6  6.872070                 7\n",
      "37      5  2.629410                 3\n",
      "38      3  5.328093                 5\n",
      "39      9  7.706183                 8\n",
      "40     12  5.622646                 6\n",
      "41      5  5.906303                 6\n",
      "42      7  5.396955                 5\n",
      "43      6  6.767060                 7\n",
      "44      5  4.740031                 5\n",
      "45      8  7.419164                 7\n",
      "46      5  4.568506                 5\n",
      "47      7  5.877002                 6\n",
      "48      5  4.905472                 5\n",
      "49      5  5.904206                 6\n",
      "50      3  3.246249                 3\n",
      "51      5  6.203086                 6\n",
      "52      8  6.286251                 6\n",
      "53      3  4.055142                 4\n",
      "54      5  6.892952                 7\n",
      "55      6  5.139320                 5\n",
      "56      6  5.791809                 6\n",
      "57      5  4.002091                 4\n",
      "58      6  4.747774                 5\n",
      "59      5  3.321130                 3\n",
      "60     11  4.030347                 4\n",
      "61      3  3.407626                 3\n",
      "62      3  1.939943                 2\n",
      "63      5  4.665219                 5\n",
      "64      3  4.356740                 4\n",
      "65      3  4.977752                 5\n",
      "66      7  5.967164                 6\n",
      "67      3  2.792466                 3\n",
      "68      5  6.860196                 7\n",
      "69      8  7.517034                 8\n",
      "70      8  6.222090                 6\n",
      "71      3  4.756285                 5\n",
      "72      3  3.052880                 3\n",
      "73      7  7.007770                 7\n",
      "74      3  4.407899                 4\n",
      "75      7  5.367381                 5\n",
      "76      3  3.901177                 4\n",
      "77      3  3.748637                 4\n",
      "78      3  3.803737                 4\n",
      "79      3  3.563817                 4\n",
      "80      5  3.841841                 4\n",
      "81      8  5.094977                 5\n",
      "82      8  8.213333                 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 33 12  1  0\n",
      "         2  9 10 10  3\n",
      "         3  0  2  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5366         \n",
      "                 95% CI : (0.423, 0.6475)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.3706         \n",
      "                                         \n",
      "                  Kappa : 0.2163         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7857   0.4167  0.08333  0.00000\n",
      "Specificity            0.6750   0.6207  0.95714  1.00000\n",
      "Pos Pred Value         0.7174   0.3125  0.25000      NaN\n",
      "Neg Pred Value         0.7500   0.7200  0.85897  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4024   0.1220  0.01220  0.00000\n",
      "Detection Prevalence   0.5610   0.3902  0.04878  0.00000\n",
      "Balanced Accuracy      0.7304   0.5187  0.52024  0.50000\n",
      "Fold 3 Accuracy: 0.536585365853659 \n",
      "Fold 3 AUC: 0.751322751322751 \n",
      "Fold 3 Overall Sensitivity: 0.321428571428571 \n",
      "Fold 3 Overall Specificity: 0.813208128078818 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.509159073096179 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  2.922898                 3\n",
      "2       3  6.240832                 6\n",
      "3       3  2.894035                 3\n",
      "4       3  5.783510                 6\n",
      "5       3  4.204417                 4\n",
      "6       7  7.539658                 8\n",
      "7       5  3.911913                 4\n",
      "8       3  4.751857                 5\n",
      "9       3  4.926844                 5\n",
      "10      3  4.930330                 5\n",
      "11      4  3.576674                 4\n",
      "12      3  4.098811                 4\n",
      "13      3  4.986108                 5\n",
      "14      3  2.600081                 3\n",
      "15      8  4.538583                 5\n",
      "16      3  3.984672                 4\n",
      "17      3  5.376103                 5\n",
      "18      5  5.312459                 5\n",
      "19      3  3.558313                 4\n",
      "20      7  6.058175                 6\n",
      "21      3  3.320172                 3\n",
      "22      3  2.864766                 3\n",
      "23      6  5.615138                 6\n",
      "24      8  5.588346                 6\n",
      "25      3  2.946775                 3\n",
      "26     10  5.050745                 5\n",
      "27      3  3.132543                 3\n",
      "28      3  4.340332                 4\n",
      "29      3  3.190292                 3\n",
      "30      3  2.664862                 3\n",
      "31      3  3.702099                 4\n",
      "32      6  4.428934                 4\n",
      "33      6  5.954001                 6\n",
      "34      7  4.846280                 5\n",
      "35      5  7.227366                 7\n",
      "36      3  3.771136                 4\n",
      "37      5  3.724059                 4\n",
      "38      5  3.295437                 3\n",
      "39      3  3.364086                 3\n",
      "40      6  4.390476                 4\n",
      "41      3  4.435296                 4\n",
      "42      8  4.752274                 5\n",
      "43      5  4.635043                 5\n",
      "44      5  4.580007                 5\n",
      "45     12  5.579557                 6\n",
      "46      5  5.387348                 5\n",
      "47      6  4.185446                 4\n",
      "48      5  4.907810                 5\n",
      "49      6  5.882441                 6\n",
      "50      3  3.722407                 4\n",
      "51      5  4.378543                 4\n",
      "52      5  2.744566                 3\n",
      "53      3  3.673590                 4\n",
      "54      7  5.239419                 5\n",
      "55      8  5.985931                 6\n",
      "56      3  4.055380                 4\n",
      "57      3  3.733236                 4\n",
      "58      5  7.087971                 7\n",
      "59      7  4.743645                 5\n",
      "60     10  7.377620                 7\n",
      "61      5  4.533269                 5\n",
      "62      3  5.364256                 5\n",
      "63      3  3.705124                 4\n",
      "64      5  3.600293                 4\n",
      "65      5  4.298426                 4\n",
      "66      3  3.366591                 3\n",
      "67      5  4.536031                 5\n",
      "68      3  4.950260                 5\n",
      "69      8  5.181424                 5\n",
      "70      3  4.081694                 4\n",
      "71      9  5.570899                 6\n",
      "72      3  3.322520                 3\n",
      "73      3  3.310780                 3\n",
      "74      3  2.649651                 3\n",
      "75      3  4.138713                 4\n",
      "76      3  3.031042                 3\n",
      "77      3  3.213207                 3\n",
      "78     10  4.902896                 5\n",
      "79      8  4.290682                 4\n",
      "80      3  4.041508                 4\n",
      "81      5  2.575512                 3\n",
      "82      3  3.250933                 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  8  1  0\n",
      "         2 14 11  5  0\n",
      "         3  0  4  6  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5679          \n",
      "                 95% CI : (0.4531, 0.6776)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.2185          \n",
      "                                          \n",
      "                  Kappa : 0.318           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.4783  0.50000  0.25000\n",
      "Specificity            0.7692   0.6724  0.89855  1.00000\n",
      "Pos Pred Value         0.7568   0.3667  0.46154  1.00000\n",
      "Neg Pred Value         0.6818   0.7647  0.91176  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3457   0.1358  0.07407  0.01235\n",
      "Detection Prevalence   0.4568   0.3704  0.16049  0.01235\n",
      "Balanced Accuracy      0.7179   0.5753  0.69928  0.62500\n",
      "Fold 4 Accuracy: 0.567901234567901 \n",
      "Fold 4 AUC: 0.837711352657005 \n",
      "Fold 4 Overall Sensitivity: 0.473731884057971 \n",
      "Fold 4 Overall Specificity: 0.835048821742975 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.720541234341377 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.376460                 4\n",
      "2       3  3.913031                 4\n",
      "3       3  2.921668                 3\n",
      "4       4  6.245954                 6\n",
      "5       3  4.327981                 4\n",
      "6       6  6.024784                 6\n",
      "7       3  4.119711                 4\n",
      "8       4  6.157776                 6\n",
      "9       3  4.765979                 5\n",
      "10      3  4.925620                 5\n",
      "11      3  4.045965                 4\n",
      "12      6  2.197322                 2\n",
      "13      3  2.980811                 3\n",
      "14      3  2.263558                 2\n",
      "15      6  5.909769                 6\n",
      "16      3  4.881875                 5\n",
      "17      5  7.711325                 8\n",
      "18      3  3.637403                 4\n",
      "19      3  2.738333                 3\n",
      "20      9  8.324523                 8\n",
      "21      3  4.317606                 4\n",
      "22      3  3.765948                 4\n",
      "23      3  4.307812                 4\n",
      "24      3  3.062390                 3\n",
      "25      5  5.173048                 5\n",
      "26      3  4.352676                 4\n",
      "27      3  5.056269                 5\n",
      "28      5  4.770276                 5\n",
      "29      5  3.544778                 4\n",
      "30      3  4.812588                 5\n",
      "31      3  5.316266                 5\n",
      "32      3  2.565805                 3\n",
      "33      8  7.237253                 7\n",
      "34      3  3.386983                 3\n",
      "35      3  3.410676                 3\n",
      "36      3  4.497850                 4\n",
      "37      3  3.944439                 4\n",
      "38      3  3.634397                 4\n",
      "39      5  5.342347                 5\n",
      "40      5  5.597362                 6\n",
      "41      6  5.992425                 6\n",
      "42      8  3.900071                 4\n",
      "43      9  5.544767                 6\n",
      "44      6  5.166285                 5\n",
      "45      9  7.127601                 7\n",
      "46      8  5.276327                 5\n",
      "47      3  3.458908                 3\n",
      "48      9  5.827280                 6\n",
      "49      7  7.104567                 7\n",
      "50     11  8.128351                 8\n",
      "51      8  6.288407                 6\n",
      "52      6  7.447797                 7\n",
      "53      5  3.890594                 4\n",
      "54      5  5.273217                 5\n",
      "55      3  3.820332                 4\n",
      "56      3  3.427300                 3\n",
      "57      3  4.671109                 5\n",
      "58      5  5.251329                 5\n",
      "59     12  9.901233                10\n",
      "60      5  4.484928                 4\n",
      "61      5  8.481559                 8\n",
      "62      5  4.381661                 4\n",
      "63     10  6.726700                 7\n",
      "64      3  3.701060                 4\n",
      "65      5  2.872782                 3\n",
      "66      5  4.428500                 4\n",
      "67     11  8.759946                 9\n",
      "68      5  4.298805                 4\n",
      "69      3  4.756064                 5\n",
      "70      3  5.063796                 5\n",
      "71      3  3.586556                 4\n",
      "72      3  5.706995                 6\n",
      "73      3  5.591560                 6\n",
      "74      3  4.434107                 4\n",
      "75      6  4.510661                 5\n",
      "76      3  4.114752                 4\n",
      "77      7  5.006190                 5\n",
      "78      9  7.738328                 8\n",
      "79      9  7.742009                 8\n",
      "80      6  6.568228                 7\n",
      "81      3  4.533962                 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  7  1  0\n",
      "         2  9  9  6  1\n",
      "         3  2  6  4  1\n",
      "         4  0  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5625         \n",
      "                 95% CI : (0.447, 0.6732)\n",
      "    No Information Rate : 0.525          \n",
      "    P-Value [Acc > NIR] : 0.2884         \n",
      "                                         \n",
      "                  Kappa : 0.3038         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381   0.3913   0.3333   0.3333\n",
      "Specificity            0.7895   0.7193   0.8676   0.9740\n",
      "Pos Pred Value         0.7949   0.3600   0.3077   0.3333\n",
      "Neg Pred Value         0.7317   0.7455   0.8806   0.9740\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.3875   0.1125   0.0500   0.0125\n",
      "Detection Prevalence   0.4875   0.3125   0.1625   0.0375\n",
      "Balanced Accuracy      0.7638   0.5553   0.6005   0.6537\n",
      "Fold 5 Accuracy: 0.5625 \n",
      "Fold 5 AUC: 0.768669484702093 \n",
      "Fold 5 Overall Sensitivity: 0.449016563146998 \n",
      "Fold 5 Overall Specificity: 0.837611240668516 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.631509826788349 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  5.027189                 5\n",
      "2       3  4.044119                 4\n",
      "3       3  4.454510                 4\n",
      "4       3  4.630089                 5\n",
      "5       3  8.509421                 9\n",
      "6       3  4.753662                 5\n",
      "7       5  2.663383                 3\n",
      "8       5  6.440507                 6\n",
      "9       4  3.877533                 4\n",
      "10      3  4.421749                 4\n",
      "11      7  8.051805                 8\n",
      "12      3  2.927089                 3\n",
      "13      7  6.455413                 6\n",
      "14      3  2.459069                 2\n",
      "15      3  3.719366                 4\n",
      "16      8  5.570612                 6\n",
      "17      3  4.003523                 4\n",
      "18      5  3.635854                 4\n",
      "19      3  3.381531                 3\n",
      "20      3  4.899308                 5\n",
      "21      3  4.413894                 4\n",
      "22      3  4.401381                 4\n",
      "23      5  6.403929                 6\n",
      "24      3  2.313119                 2\n",
      "25      6  9.975093                10\n",
      "26      6  7.210951                 7\n",
      "27      5  5.707866                 6\n",
      "28      5  5.274782                 5\n",
      "29      5  4.733721                 5\n",
      "30      3  3.932532                 4\n",
      "31      3  4.052040                 4\n",
      "32      4  4.957777                 5\n",
      "33      7  5.224380                 5\n",
      "34      3  3.161100                 3\n",
      "35      3  1.366422                 1\n",
      "36      5  4.436171                 4\n",
      "37      5  5.116669                 5\n",
      "38      7  8.424405                 8\n",
      "39     10 10.854289                11\n",
      "40     10  8.503710                 9\n",
      "41      3  4.200303                 4\n",
      "42      3  4.015347                 4\n",
      "43      8  5.428980                 5\n",
      "44      3  4.016796                 4\n",
      "45      7  6.100661                 6\n",
      "46      3  2.240558                 2\n",
      "47      7  4.127018                 4\n",
      "48      6  6.868932                 7\n",
      "49     11  5.271097                 5\n",
      "50      3  3.335142                 3\n",
      "51      6  7.766725                 8\n",
      "52      6  6.315468                 6\n",
      "53      3  4.702858                 5\n",
      "54      3  4.423020                 4\n",
      "55      9 10.366436                10\n",
      "56      3  4.378461                 4\n",
      "57      6  3.466199                 3\n",
      "58      3  4.946748                 5\n",
      "59      5  4.093926                 4\n",
      "60      5  7.630568                 8\n",
      "61      3  2.512855                 3\n",
      "62      3  2.630006                 3\n",
      "63      3  2.911995                 3\n",
      "64      9  5.526711                 6\n",
      "65      6  5.691842                 6\n",
      "66      5  3.768403                 4\n",
      "67      7  8.883320                 9\n",
      "68      3  3.230129                 3\n",
      "69      4  7.381097                 7\n",
      "70      3  3.983620                 4\n",
      "71      5  4.840999                 5\n",
      "72      6  7.727137                 8\n",
      "73      8  6.665287                 7\n",
      "74      6  4.357172                 4\n",
      "75      3  4.373514                 4\n",
      "76      3  4.841378                 5\n",
      "77      3  4.194904                 4\n",
      "78      3  5.660787                 6\n",
      "79      5  7.968447                 8\n",
      "80      3  2.364099                 2\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 155  38   3   1\n",
      "         2  51  57  36   6\n",
      "         3   4  22  21   9\n",
      "         4   0   1   1   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5774          \n",
      "                 95% CI : (0.5278, 0.6259)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.007443        \n",
      "                                          \n",
      "                  Kappa : 0.3209          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.011795        \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381   0.4831   0.3443 0.111111\n",
      "Specificity            0.7868   0.6782   0.8988 0.994859\n",
      "Pos Pred Value         0.7868   0.3800   0.3750 0.500000\n",
      "Neg Pred Value         0.7381   0.7626   0.8860 0.960298\n",
      "Prevalence             0.5160   0.2899   0.1499 0.044226\n",
      "Detection Rate         0.3808   0.1400   0.0516 0.004914\n",
      "Detection Prevalence   0.4840   0.3686   0.1376 0.009828\n",
      "Balanced Accuracy      0.7624   0.5806   0.6216 0.552985\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5772998 \n",
      "Average AUC: 0.7628373 \n",
      "Average Sensitivity: 0.4201495 \n",
      "Average Specificity: 0.8396174 \n",
      "Average Pearson Correlation: 0.6177747 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 25)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c096fbd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38  9  3  1\n",
      "         2  4 13  4  0\n",
      "         3  0  2  5  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6829          \n",
      "                 95% CI : (0.5708, 0.7813)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.001285        \n",
      "                                          \n",
      "                  Kappa : 0.4613          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.5417  0.41667  0.00000\n",
      "Specificity            0.6750   0.8621  0.92857  1.00000\n",
      "Pos Pred Value         0.7451   0.6190  0.50000      NaN\n",
      "Neg Pred Value         0.8710   0.8197  0.90278  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4634   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.6220   0.2561  0.12195  0.00000\n",
      "Balanced Accuracy      0.7899   0.7019  0.67262  0.50000\n",
      "Fold 1 Accuracy: 0.682926829268293 \n",
      "Fold 1 AUC: 0.755208333333333 \n",
      "Fold 1 Overall Sensitivity: 0.46577380952381 \n",
      "Fold 1 Overall Specificity: 0.866410098522167 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.710321857592923 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  7.775430                   7\n",
      "2       3  2.978218                   2\n",
      "3       3  3.749069                   3\n",
      "4       5  3.943935                   3\n",
      "5       5  5.976147                   5\n",
      "6       4  5.996992                   5\n",
      "7       3  4.942150                   4\n",
      "8       5  4.555329                   4\n",
      "9       5  4.613303                   4\n",
      "10      3  3.717765                   3\n",
      "11      3  4.816445                   4\n",
      "12      3  3.433309                   3\n",
      "13      5  5.377219                   5\n",
      "14     10  9.414483                   9\n",
      "15      3  3.318553                   3\n",
      "16      3  3.021815                   3\n",
      "17      6  7.217221                   7\n",
      "18      3  4.309497                   4\n",
      "19      3  2.501869                   2\n",
      "20      3  3.695681                   3\n",
      "21      3  5.350545                   5\n",
      "22      3  3.102687                   3\n",
      "23      3  2.866556                   2\n",
      "24      8  7.261834                   7\n",
      "25      3  3.400767                   3\n",
      "26      3  2.808754                   2\n",
      "27      3  4.409124                   4\n",
      "28      3  4.330890                   4\n",
      "29      3  4.485528                   4\n",
      "30      3  3.667582                   3\n",
      "31      5  2.720271                   2\n",
      "32      3  3.139868                   3\n",
      "33      3  3.194654                   3\n",
      "34      3  1.887526                   1\n",
      "35      5  6.141328                   6\n",
      "36      3  5.257840                   5\n",
      "37      6  5.776188                   5\n",
      "38      6  6.559079                   6\n",
      "39      3  4.155356                   4\n",
      "40      5  3.432640                   3\n",
      "41      5  5.373151                   5\n",
      "42      6  3.974664                   3\n",
      "43      3  2.574627                   2\n",
      "44      7  4.763894                   4\n",
      "45      7  4.805254                   4\n",
      "46      3  2.842724                   2\n",
      "47     11  7.565403                   7\n",
      "48      9  5.737079                   5\n",
      "49      3  4.612268                   4\n",
      "50      3  2.638190                   2\n",
      "51      3  3.526564                   3\n",
      "52      8  8.555764                   8\n",
      "53      6  2.299859                   2\n",
      "54      6  5.473819                   5\n",
      "55      5  5.335785                   5\n",
      "56      5  3.558772                   3\n",
      "57      7  7.646187                   7\n",
      "58      3  3.943266                   3\n",
      "59      3  2.669960                   2\n",
      "60      9  6.245771                   6\n",
      "61      9  5.369682                   5\n",
      "62      9  7.186973                   7\n",
      "63      3  4.431772                   4\n",
      "64      9  6.319286                   6\n",
      "65     10  4.941484                   4\n",
      "66      6  6.183178                   6\n",
      "67      3  4.403622                   4\n",
      "68      5  6.040269                   6\n",
      "69      5  2.450592                   2\n",
      "70      3  3.119501                   3\n",
      "71      6  5.472508                   5\n",
      "72      5  6.584960                   6\n",
      "73      8  7.246942                   7\n",
      "74      5  5.215209                   5\n",
      "75      3  6.129046                   6\n",
      "76      3  2.685936                   2\n",
      "77      7  4.572984                   4\n",
      "78      3  3.458838                   3\n",
      "79      3  4.530656                   4\n",
      "80     11  8.412297                   8\n",
      "81      3  2.968185                   2\n",
      "82      3  3.985776                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32 11  0  1\n",
      "         2  9 11  8  2\n",
      "         3  1  2  5  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.112           \n",
      "                                          \n",
      "                  Kappa : 0.3119          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.4583  0.38462  0.00000\n",
      "Specificity            0.7000   0.6724  0.95652  1.00000\n",
      "Pos Pred Value         0.7273   0.3667  0.62500      NaN\n",
      "Neg Pred Value         0.7368   0.7500  0.89189  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3902   0.1341  0.06098  0.00000\n",
      "Detection Prevalence   0.5366   0.3659  0.09756  0.00000\n",
      "Balanced Accuracy      0.7310   0.5654  0.67057  0.50000\n",
      "Fold 2 Accuracy: 0.585365853658537 \n",
      "Fold 2 AUC: 0.6346599002849 \n",
      "Fold 2 Overall Sensitivity: 0.40121336996337 \n",
      "Fold 2 Overall Specificity: 0.832233883058471 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.51734152451264 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  5.080454                   5\n",
      "2       4  4.418183                   4\n",
      "3       3  5.251783                   5\n",
      "4       3  5.555921                   5\n",
      "5       3  3.276293                   3\n",
      "6       3  2.415562                   2\n",
      "7       5  4.768613                   4\n",
      "8       3  5.185938                   5\n",
      "9       5  9.283583                   9\n",
      "10      4  3.325134                   3\n",
      "11      3  3.256691                   3\n",
      "12      3  6.750168                   6\n",
      "13      3  4.433676                   4\n",
      "14      6  5.587202                   5\n",
      "15     12  6.889142                   6\n",
      "16      6  7.375101                   7\n",
      "17      3  4.278207                   4\n",
      "18      5  5.647632                   5\n",
      "19      3  4.154331                   4\n",
      "20      3  4.327018                   4\n",
      "21      3  3.032666                   3\n",
      "22      3  3.626426                   3\n",
      "23      3  3.550895                   3\n",
      "24      3  2.901863                   2\n",
      "25      3  5.260062                   5\n",
      "26      3  5.437737                   5\n",
      "27      3  3.593859                   3\n",
      "28      3  3.945161                   3\n",
      "29      5  4.616183                   4\n",
      "30      4  9.342939                   9\n",
      "31      3  5.236723                   5\n",
      "32      3  3.369366                   3\n",
      "33      3  4.664736                   4\n",
      "34      3  3.454382                   3\n",
      "35      8  5.622154                   5\n",
      "36      6  6.872070                   6\n",
      "37      5  2.629410                   2\n",
      "38      3  5.328093                   5\n",
      "39      9  7.706183                   7\n",
      "40     12  5.622646                   5\n",
      "41      5  5.906303                   5\n",
      "42      7  5.396955                   5\n",
      "43      6  6.767060                   6\n",
      "44      5  4.740031                   4\n",
      "45      8  7.419164                   7\n",
      "46      5  4.568506                   4\n",
      "47      7  5.877002                   5\n",
      "48      5  4.905472                   4\n",
      "49      5  5.904206                   5\n",
      "50      3  3.246249                   3\n",
      "51      5  6.203086                   6\n",
      "52      8  6.286251                   6\n",
      "53      3  4.055142                   4\n",
      "54      5  6.892952                   6\n",
      "55      6  5.139320                   5\n",
      "56      6  5.791809                   5\n",
      "57      5  4.002091                   4\n",
      "58      6  4.747774                   4\n",
      "59      5  3.321130                   3\n",
      "60     11  4.030347                   4\n",
      "61      3  3.407626                   3\n",
      "62      3  1.939943                   1\n",
      "63      5  4.665219                   4\n",
      "64      3  4.356740                   4\n",
      "65      3  4.977752                   4\n",
      "66      7  5.967164                   5\n",
      "67      3  2.792466                   2\n",
      "68      5  6.860196                   6\n",
      "69      8  7.517034                   7\n",
      "70      8  6.222090                   6\n",
      "71      3  4.756285                   4\n",
      "72      3  3.052880                   3\n",
      "73      7  7.007770                   7\n",
      "74      3  4.407899                   4\n",
      "75      7  5.367381                   5\n",
      "76      3  3.901177                   3\n",
      "77      3  3.748637                   3\n",
      "78      3  3.803737                   3\n",
      "79      3  3.563817                   3\n",
      "80      5  3.841841                   3\n",
      "81      8  5.094977                   5\n",
      "82      8  8.213333                   8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 17  5  1\n",
      "         2  4  5  6  2\n",
      "         3  0  2  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5366         \n",
      "                 95% CI : (0.423, 0.6475)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.370593       \n",
      "                                         \n",
      "                  Kappa : 0.1592         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 0.004084       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048  0.20833  0.08333  0.00000\n",
      "Specificity            0.4250  0.79310  0.95714  1.00000\n",
      "Pos Pred Value         0.6230  0.29412  0.25000      NaN\n",
      "Neg Pred Value         0.8095  0.70769  0.85897  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4634  0.06098  0.01220  0.00000\n",
      "Detection Prevalence   0.7439  0.20732  0.04878  0.00000\n",
      "Balanced Accuracy      0.6649  0.50072  0.52024  0.50000\n",
      "Fold 3 Accuracy: 0.536585365853659 \n",
      "Fold 3 AUC: 0.697089947089947 \n",
      "Fold 3 Overall Sensitivity: 0.299107142857143 \n",
      "Fold 3 Overall Specificity: 0.79381157635468 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.509159073096179 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  2.922898                   2\n",
      "2       3  6.240832                   6\n",
      "3       3  2.894035                   2\n",
      "4       3  5.783510                   5\n",
      "5       3  4.204417                   4\n",
      "6       7  7.539658                   7\n",
      "7       5  3.911913                   3\n",
      "8       3  4.751857                   4\n",
      "9       3  4.926844                   4\n",
      "10      3  4.930330                   4\n",
      "11      4  3.576674                   3\n",
      "12      3  4.098811                   4\n",
      "13      3  4.986108                   4\n",
      "14      3  2.600081                   2\n",
      "15      8  4.538583                   4\n",
      "16      3  3.984672                   3\n",
      "17      3  5.376103                   5\n",
      "18      5  5.312459                   5\n",
      "19      3  3.558313                   3\n",
      "20      7  6.058175                   6\n",
      "21      3  3.320172                   3\n",
      "22      3  2.864766                   2\n",
      "23      6  5.615138                   5\n",
      "24      8  5.588346                   5\n",
      "25      3  2.946775                   2\n",
      "26     10  5.050745                   5\n",
      "27      3  3.132543                   3\n",
      "28      3  4.340332                   4\n",
      "29      3  3.190292                   3\n",
      "30      3  2.664862                   2\n",
      "31      3  3.702099                   3\n",
      "32      6  4.428934                   4\n",
      "33      6  5.954001                   5\n",
      "34      7  4.846280                   4\n",
      "35      5  7.227366                   7\n",
      "36      3  3.771136                   3\n",
      "37      5  3.724059                   3\n",
      "38      5  3.295437                   3\n",
      "39      3  3.364086                   3\n",
      "40      6  4.390476                   4\n",
      "41      3  4.435296                   4\n",
      "42      8  4.752274                   4\n",
      "43      5  4.635043                   4\n",
      "44      5  4.580007                   4\n",
      "45     12  5.579557                   5\n",
      "46      5  5.387348                   5\n",
      "47      6  4.185446                   4\n",
      "48      5  4.907810                   4\n",
      "49      6  5.882441                   5\n",
      "50      3  3.722407                   3\n",
      "51      5  4.378543                   4\n",
      "52      5  2.744566                   2\n",
      "53      3  3.673590                   3\n",
      "54      7  5.239419                   5\n",
      "55      8  5.985931                   5\n",
      "56      3  4.055380                   4\n",
      "57      3  3.733236                   3\n",
      "58      5  7.087971                   7\n",
      "59      7  4.743645                   4\n",
      "60     10  7.377620                   7\n",
      "61      5  4.533269                   4\n",
      "62      3  5.364256                   5\n",
      "63      3  3.705124                   3\n",
      "64      5  3.600293                   3\n",
      "65      5  4.298426                   4\n",
      "66      3  3.366591                   3\n",
      "67      5  4.536031                   4\n",
      "68      3  4.950260                   4\n",
      "69      8  5.181424                   5\n",
      "70      3  4.081694                   4\n",
      "71      9  5.570899                   5\n",
      "72      3  3.322520                   3\n",
      "73      3  3.310780                   3\n",
      "74      3  2.649651                   2\n",
      "75      3  4.138713                   4\n",
      "76      3  3.031042                   3\n",
      "77      3  3.213207                   3\n",
      "78     10  4.902896                   4\n",
      "79      8  4.290682                   4\n",
      "80      3  4.041508                   4\n",
      "81      5  2.575512                   2\n",
      "82      3  3.250933                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35 10  1  0\n",
      "         2  7 10  5  1\n",
      "         3  0  3  6  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6296          \n",
      "                 95% CI : (0.5151, 0.7344)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.02879         \n",
      "                                          \n",
      "                  Kappa : 0.3857          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.4348  0.50000  0.00000\n",
      "Specificity            0.7179   0.7759  0.91304  1.00000\n",
      "Pos Pred Value         0.7609   0.4348  0.50000      NaN\n",
      "Neg Pred Value         0.8000   0.7759  0.91304  0.95062\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4321   0.1235  0.07407  0.00000\n",
      "Detection Prevalence   0.5679   0.2840  0.14815  0.00000\n",
      "Balanced Accuracy      0.7756   0.6053  0.70652  0.50000\n",
      "Fold 4 Accuracy: 0.62962962962963 \n",
      "Fold 4 AUC: 0.810461956521739 \n",
      "Fold 4 Overall Sensitivity: 0.442028985507246 \n",
      "Fold 4 Overall Specificity: 0.851713566293776 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.720541234341377 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.376460                   4\n",
      "2       3  3.913031                   3\n",
      "3       3  2.921668                   2\n",
      "4       4  6.245954                   6\n",
      "5       3  4.327981                   4\n",
      "6       6  6.024784                   6\n",
      "7       3  4.119711                   4\n",
      "8       4  6.157776                   6\n",
      "9       3  4.765979                   4\n",
      "10      3  4.925620                   4\n",
      "11      3  4.045965                   4\n",
      "12      6  2.197322                   2\n",
      "13      3  2.980811                   2\n",
      "14      3  2.263558                   2\n",
      "15      6  5.909769                   5\n",
      "16      3  4.881875                   4\n",
      "17      5  7.711325                   7\n",
      "18      3  3.637403                   3\n",
      "19      3  2.738333                   2\n",
      "20      9  8.324523                   8\n",
      "21      3  4.317606                   4\n",
      "22      3  3.765948                   3\n",
      "23      3  4.307812                   4\n",
      "24      3  3.062390                   3\n",
      "25      5  5.173048                   5\n",
      "26      3  4.352676                   4\n",
      "27      3  5.056269                   5\n",
      "28      5  4.770276                   4\n",
      "29      5  3.544778                   3\n",
      "30      3  4.812588                   4\n",
      "31      3  5.316266                   5\n",
      "32      3  2.565805                   2\n",
      "33      8  7.237253                   7\n",
      "34      3  3.386983                   3\n",
      "35      3  3.410676                   3\n",
      "36      3  4.497850                   4\n",
      "37      3  3.944439                   3\n",
      "38      3  3.634397                   3\n",
      "39      5  5.342347                   5\n",
      "40      5  5.597362                   5\n",
      "41      6  5.992425                   5\n",
      "42      8  3.900071                   3\n",
      "43      9  5.544767                   5\n",
      "44      6  5.166285                   5\n",
      "45      9  7.127601                   7\n",
      "46      8  5.276327                   5\n",
      "47      3  3.458908                   3\n",
      "48      9  5.827280                   5\n",
      "49      7  7.104567                   7\n",
      "50     11  8.128351                   8\n",
      "51      8  6.288407                   6\n",
      "52      6  7.447797                   7\n",
      "53      5  3.890594                   3\n",
      "54      5  5.273217                   5\n",
      "55      3  3.820332                   3\n",
      "56      3  3.427300                   3\n",
      "57      3  4.671109                   4\n",
      "58      5  5.251329                   5\n",
      "59     12  9.901233                   9\n",
      "60      5  4.484928                   4\n",
      "61      5  8.481559                   8\n",
      "62      5  4.381661                   4\n",
      "63     10  6.726700                   6\n",
      "64      3  3.701060                   3\n",
      "65      5  2.872782                   2\n",
      "66      5  4.428500                   4\n",
      "67     11  8.759946                   8\n",
      "68      5  4.298805                   4\n",
      "69      3  4.756064                   4\n",
      "70      3  5.063796                   5\n",
      "71      3  3.586556                   3\n",
      "72      3  5.706995                   5\n",
      "73      3  5.591560                   5\n",
      "74      3  4.434107                   4\n",
      "75      6  4.510661                   4\n",
      "76      3  4.114752                   4\n",
      "77      7  5.006190                   5\n",
      "78      9  7.738328                   7\n",
      "79      9  7.742009                   7\n",
      "80      6  6.568228                   6\n",
      "81      3  4.533962                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38  9  1  0\n",
      "         2  2  8  7  1\n",
      "         3  2  6  3  1\n",
      "         4  0  0  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.625           \n",
      "                 95% CI : (0.5096, 0.7308)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.04591         \n",
      "                                          \n",
      "                  Kappa : 0.3717          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.3478   0.2500   0.3333\n",
      "Specificity            0.7368   0.8246   0.8676   0.9870\n",
      "Pos Pred Value         0.7917   0.4444   0.2500   0.5000\n",
      "Neg Pred Value         0.8750   0.7581   0.8676   0.9744\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4750   0.1000   0.0375   0.0125\n",
      "Detection Prevalence   0.6000   0.2250   0.1500   0.0250\n",
      "Balanced Accuracy      0.8208   0.5862   0.5588   0.6602\n",
      "Fold 5 Accuracy: 0.625 \n",
      "Fold 5 AUC: 0.799387508626639 \n",
      "Fold 5 Overall Sensitivity: 0.45898033126294 \n",
      "Fold 5 Overall Specificity: 0.854015888652112 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.631509826788349 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  5.027189                   5\n",
      "2       3  4.044119                   4\n",
      "3       3  4.454510                   4\n",
      "4       3  4.630089                   4\n",
      "5       3  8.509421                   8\n",
      "6       3  4.753662                   4\n",
      "7       5  2.663383                   2\n",
      "8       5  6.440507                   6\n",
      "9       4  3.877533                   3\n",
      "10      3  4.421749                   4\n",
      "11      7  8.051805                   8\n",
      "12      3  2.927089                   2\n",
      "13      7  6.455413                   6\n",
      "14      3  2.459069                   2\n",
      "15      3  3.719366                   3\n",
      "16      8  5.570612                   5\n",
      "17      3  4.003523                   4\n",
      "18      5  3.635854                   3\n",
      "19      3  3.381531                   3\n",
      "20      3  4.899308                   4\n",
      "21      3  4.413894                   4\n",
      "22      3  4.401381                   4\n",
      "23      5  6.403929                   6\n",
      "24      3  2.313119                   2\n",
      "25      6  9.975093                   9\n",
      "26      6  7.210951                   7\n",
      "27      5  5.707866                   5\n",
      "28      5  5.274782                   5\n",
      "29      5  4.733721                   4\n",
      "30      3  3.932532                   3\n",
      "31      3  4.052040                   4\n",
      "32      4  4.957777                   4\n",
      "33      7  5.224380                   5\n",
      "34      3  3.161100                   3\n",
      "35      3  1.366422                   1\n",
      "36      5  4.436171                   4\n",
      "37      5  5.116669                   5\n",
      "38      7  8.424405                   8\n",
      "39     10 10.854289                  10\n",
      "40     10  8.503710                   8\n",
      "41      3  4.200303                   4\n",
      "42      3  4.015347                   4\n",
      "43      8  5.428980                   5\n",
      "44      3  4.016796                   4\n",
      "45      7  6.100661                   6\n",
      "46      3  2.240558                   2\n",
      "47      7  4.127018                   4\n",
      "48      6  6.868932                   6\n",
      "49     11  5.271097                   5\n",
      "50      3  3.335142                   3\n",
      "51      6  7.766725                   7\n",
      "52      6  6.315468                   6\n",
      "53      3  4.702858                   4\n",
      "54      3  4.423020                   4\n",
      "55      9 10.366436                  10\n",
      "56      3  4.378461                   4\n",
      "57      6  3.466199                   3\n",
      "58      3  4.946748                   4\n",
      "59      5  4.093926                   4\n",
      "60      5  7.630568                   7\n",
      "61      3  2.512855                   2\n",
      "62      3  2.630006                   2\n",
      "63      3  2.911995                   2\n",
      "64      9  5.526711                   5\n",
      "65      6  5.691842                   5\n",
      "66      5  3.768403                   3\n",
      "67      7  8.883320                   8\n",
      "68      3  3.230129                   3\n",
      "69      4  7.381097                   7\n",
      "70      3  3.983620                   3\n",
      "71      5  4.840999                   4\n",
      "72      6  7.727137                   7\n",
      "73      8  6.665287                   6\n",
      "74      6  4.357172                   4\n",
      "75      3  4.373514                   4\n",
      "76      3  4.841378                   4\n",
      "77      3  4.194904                   4\n",
      "78      3  5.660787                   5\n",
      "79      5  7.968447                   7\n",
      "80      3  2.364099                   2\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 181  56  10   3\n",
      "         2  26  47  30   6\n",
      "         3   3  15  20   8\n",
      "         4   0   0   1   1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6118          \n",
      "                 95% CI : (0.5625, 0.6594)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 6.232e-05       \n",
      "                                          \n",
      "                  Kappa : 0.3401          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 6.184e-06       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8619   0.3983  0.32787 0.055556\n",
      "Specificity            0.6497   0.7855  0.92486 0.997429\n",
      "Pos Pred Value         0.7240   0.4312  0.43478 0.500000\n",
      "Neg Pred Value         0.8153   0.7617  0.88643 0.958025\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4447   0.1155  0.04914 0.002457\n",
      "Detection Prevalence   0.6143   0.2678  0.11302 0.004914\n",
      "Balanced Accuracy      0.7558   0.5919  0.62636 0.526492\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6119015 \n",
      "Average AUC: 0.7393615 \n",
      "Average Sensitivity: 0.4134207 \n",
      "Average Specificity: 0.839637 \n",
      "Average Pearson Correlation: 0.6177747 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 25)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Truncate the predicted values before classification\n",
    "    truncated_predictions <- floor(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(truncated_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Truncated_Predicted = truncated_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6c0cf",
   "metadata": {},
   "source": [
    "# 20 Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24ba2c38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  8  1  0\n",
      "         2  8 13  6  1\n",
      "         3  0  3  5  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6341          \n",
      "                 95% CI : (0.5205, 0.7378)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.01746         \n",
      "                                          \n",
      "                  Kappa : 0.402           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.5417  0.41667  0.00000\n",
      "Specificity            0.7750   0.7414  0.91429  1.00000\n",
      "Pos Pred Value         0.7907   0.4643  0.45455      NaN\n",
      "Neg Pred Value         0.7949   0.7963  0.90141  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4146   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.5244   0.3415  0.13415  0.00000\n",
      "Balanced Accuracy      0.7923   0.6415  0.66548  0.50000\n",
      "Fold 1 Accuracy: 0.634146341463415 \n",
      "Fold 1 AUC: 0.811135912698413 \n",
      "Fold 1 Overall Sensitivity: 0.441964285714286 \n",
      "Fold 1 Overall Specificity: 0.857666256157635 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.721533059667909 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  7.544700                 8\n",
      "2       3  3.340160                 3\n",
      "3       3  3.548084                 4\n",
      "4       5  4.311209                 4\n",
      "5       5  5.661858                 6\n",
      "6       4  6.126035                 6\n",
      "7       3  4.900270                 5\n",
      "8       5  4.216586                 4\n",
      "9       5  4.820332                 5\n",
      "10      3  3.325913                 3\n",
      "11      3  4.354567                 4\n",
      "12      3  3.473020                 3\n",
      "13      5  4.777799                 5\n",
      "14     10  9.312461                 9\n",
      "15      3  3.915770                 4\n",
      "16      3  3.175495                 3\n",
      "17      6  6.983921                 7\n",
      "18      3  3.845029                 4\n",
      "19      3  2.904013                 3\n",
      "20      3  3.846995                 4\n",
      "21      3  5.226660                 5\n",
      "22      3  3.033950                 3\n",
      "23      3  2.710546                 3\n",
      "24      8  7.391781                 7\n",
      "25      3  3.688779                 4\n",
      "26      3  2.951289                 3\n",
      "27      3  4.966112                 5\n",
      "28      3  4.282450                 4\n",
      "29      3  4.851845                 5\n",
      "30      3  3.561770                 4\n",
      "31      5  3.007984                 3\n",
      "32      3  2.757554                 3\n",
      "33      3  3.349723                 3\n",
      "34      3  2.232017                 2\n",
      "35      5  5.995832                 6\n",
      "36      3  5.085776                 5\n",
      "37      6  5.694800                 6\n",
      "38      6  6.393633                 6\n",
      "39      3  3.986177                 4\n",
      "40      5  3.492472                 3\n",
      "41      5  4.806513                 5\n",
      "42      6  4.112126                 4\n",
      "43      3  3.229325                 3\n",
      "44      7  5.076479                 5\n",
      "45      7  4.374602                 4\n",
      "46      3  3.334626                 3\n",
      "47     11  7.454027                 7\n",
      "48      9  5.790042                 6\n",
      "49      3  4.249066                 4\n",
      "50      3  2.735460                 3\n",
      "51      3  3.397360                 3\n",
      "52      8  8.412965                 8\n",
      "53      6  2.119296                 2\n",
      "54      6  5.148623                 5\n",
      "55      5  5.036065                 5\n",
      "56      5  4.348604                 4\n",
      "57      7  7.448960                 7\n",
      "58      3  3.560056                 4\n",
      "59      3  2.525923                 3\n",
      "60      9  6.264445                 6\n",
      "61      9  5.272273                 5\n",
      "62      9  6.733634                 7\n",
      "63      3  4.091685                 4\n",
      "64      9  6.303696                 6\n",
      "65     10  4.882106                 5\n",
      "66      6  6.166815                 6\n",
      "67      3  4.384027                 4\n",
      "68      5  6.127986                 6\n",
      "69      5  2.978250                 3\n",
      "70      3  3.096767                 3\n",
      "71      6  5.210730                 5\n",
      "72      5  6.741589                 7\n",
      "73      8  7.063072                 7\n",
      "74      5  5.127929                 5\n",
      "75      3  5.781474                 6\n",
      "76      3  3.126093                 3\n",
      "77      7  4.517844                 5\n",
      "78      3  3.853480                 4\n",
      "79      3  4.740788                 5\n",
      "80     11  8.564849                 9\n",
      "81      3  2.736821                 3\n",
      "82      3  4.119171                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  4  0  1\n",
      "         2 12 14  8  1\n",
      "         3  2  6  5  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5732          \n",
      "                 95% CI : (0.4591, 0.6818)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.1601          \n",
      "                                          \n",
      "                  Kappa : 0.335           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.1581          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.5833  0.38462  0.00000\n",
      "Specificity            0.8750   0.6379  0.86957  1.00000\n",
      "Pos Pred Value         0.8485   0.4000  0.35714      NaN\n",
      "Neg Pred Value         0.7143   0.7872  0.88235  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3415   0.1707  0.06098  0.00000\n",
      "Detection Prevalence   0.4024   0.4268  0.17073  0.00000\n",
      "Balanced Accuracy      0.7708   0.6106  0.62709  0.50000\n",
      "Fold 2 Accuracy: 0.573170731707317 \n",
      "Fold 2 AUC: 0.635734635734636 \n",
      "Fold 2 Overall Sensitivity: 0.408653846153846 \n",
      "Fold 2 Overall Specificity: 0.845624062968516 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.517893800442859 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  5.126320                 5\n",
      "2       4  4.366571                 4\n",
      "3       3  5.089496                 5\n",
      "4       3  5.609014                 6\n",
      "5       3  3.461907                 3\n",
      "6       3  2.723536                 3\n",
      "7       5  4.804826                 5\n",
      "8       3  5.158886                 5\n",
      "9       5  9.281954                 9\n",
      "10      4  3.249788                 3\n",
      "11      3  3.296683                 3\n",
      "12      3  6.622824                 7\n",
      "13      3  4.505701                 5\n",
      "14      6  5.632388                 6\n",
      "15     12  6.926602                 7\n",
      "16      6  7.199072                 7\n",
      "17      3  4.145204                 4\n",
      "18      5  5.934674                 6\n",
      "19      3  4.082607                 4\n",
      "20      3  4.412077                 4\n",
      "21      3  3.065357                 3\n",
      "22      3  3.583839                 4\n",
      "23      3  3.534250                 4\n",
      "24      3  2.821417                 3\n",
      "25      3  5.201352                 5\n",
      "26      3  5.460340                 5\n",
      "27      3  3.647010                 4\n",
      "28      3  3.974614                 4\n",
      "29      5  4.575732                 5\n",
      "30      4  9.339944                 9\n",
      "31      3  5.126649                 5\n",
      "32      3  3.363872                 3\n",
      "33      3  4.546559                 5\n",
      "34      3  3.592786                 4\n",
      "35      8  5.664656                 6\n",
      "36      6  6.734027                 7\n",
      "37      5  2.737959                 3\n",
      "38      3  5.360230                 5\n",
      "39      9  7.592487                 8\n",
      "40     12  5.577357                 6\n",
      "41      5  5.794244                 6\n",
      "42      7  5.280523                 5\n",
      "43      6  6.914053                 7\n",
      "44      5  4.811338                 5\n",
      "45      8  7.566129                 8\n",
      "46      5  4.645316                 5\n",
      "47      7  5.969039                 6\n",
      "48      5  4.942350                 5\n",
      "49      5  5.836791                 6\n",
      "50      3  3.194849                 3\n",
      "51      5  6.175711                 6\n",
      "52      8  6.280658                 6\n",
      "53      3  4.208910                 4\n",
      "54      5  6.813274                 7\n",
      "55      6  5.112108                 5\n",
      "56      6  5.934442                 6\n",
      "57      5  3.878520                 4\n",
      "58      6  4.730797                 5\n",
      "59      5  3.302064                 3\n",
      "60     11  4.037584                 4\n",
      "61      3  3.383182                 3\n",
      "62      3  2.249579                 2\n",
      "63      5  4.543231                 5\n",
      "64      3  4.309772                 4\n",
      "65      3  4.937566                 5\n",
      "66      7  5.986814                 6\n",
      "67      3  2.785075                 3\n",
      "68      5  6.880866                 7\n",
      "69      8  7.605406                 8\n",
      "70      8  6.166271                 6\n",
      "71      3  4.797011                 5\n",
      "72      3  3.145627                 3\n",
      "73      7  7.088804                 7\n",
      "74      3  4.428645                 4\n",
      "75      7  5.301918                 5\n",
      "76      3  4.066178                 4\n",
      "77      3  3.704994                 4\n",
      "78      3  3.870603                 4\n",
      "79      3  3.501505                 4\n",
      "80      5  4.029125                 4\n",
      "81      8  4.869189                 5\n",
      "82      8  8.203324                 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 33 14  2  0\n",
      "         2  9  8  9  3\n",
      "         3  0  2  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5122          \n",
      "                 95% CI : (0.3992, 0.6242)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.5443          \n",
      "                                          \n",
      "                  Kappa : 0.1637          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7857  0.33333  0.08333  0.00000\n",
      "Specificity            0.6000  0.63793  0.95714  1.00000\n",
      "Pos Pred Value         0.6735  0.27586  0.25000      NaN\n",
      "Neg Pred Value         0.7273  0.69811  0.85897  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4024  0.09756  0.01220  0.00000\n",
      "Detection Prevalence   0.5976  0.35366  0.04878  0.00000\n",
      "Balanced Accuracy      0.6929  0.48563  0.52024  0.50000\n",
      "Fold 3 Accuracy: 0.51219512195122 \n",
      "Fold 3 AUC: 0.747850529100529 \n",
      "Fold 3 Overall Sensitivity: 0.300595238095238 \n",
      "Fold 3 Overall Specificity: 0.798768472906404 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.521634556892494 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  2.851303                 3\n",
      "2       3  6.441232                 6\n",
      "3       3  2.823854                 3\n",
      "4       3  5.774906                 6\n",
      "5       3  4.338117                 4\n",
      "6       7  7.536084                 8\n",
      "7       5  3.894602                 4\n",
      "8       3  4.684821                 5\n",
      "9       3  4.745263                 5\n",
      "10      3  4.923599                 5\n",
      "11      4  3.512092                 4\n",
      "12      3  4.198474                 4\n",
      "13      3  4.924252                 5\n",
      "14      3  2.960953                 3\n",
      "15      8  4.470383                 4\n",
      "16      3  3.969648                 4\n",
      "17      3  5.211800                 5\n",
      "18      5  5.227935                 5\n",
      "19      3  3.562453                 4\n",
      "20      7  6.092214                 6\n",
      "21      3  3.557342                 4\n",
      "22      3  2.800173                 3\n",
      "23      6  5.791856                 6\n",
      "24      8  5.698138                 6\n",
      "25      3  3.006576                 3\n",
      "26     10  5.102480                 5\n",
      "27      3  3.508448                 4\n",
      "28      3  4.410970                 4\n",
      "29      3  3.267348                 3\n",
      "30      3  2.730591                 3\n",
      "31      3  3.573865                 4\n",
      "32      6  4.431444                 4\n",
      "33      6  6.066825                 6\n",
      "34      7  4.853105                 5\n",
      "35      5  7.352260                 7\n",
      "36      3  3.697458                 4\n",
      "37      5  4.067154                 4\n",
      "38      5  3.833743                 4\n",
      "39      3  3.312672                 3\n",
      "40      6  4.429388                 4\n",
      "41      3  4.392635                 4\n",
      "42      8  4.618621                 5\n",
      "43      5  4.712947                 5\n",
      "44      5  4.544407                 5\n",
      "45     12  5.663244                 6\n",
      "46      5  5.444657                 5\n",
      "47      6  4.105494                 4\n",
      "48      5  4.909962                 5\n",
      "49      6  5.915883                 6\n",
      "50      3  3.832041                 4\n",
      "51      5  4.314412                 4\n",
      "52      5  3.004610                 3\n",
      "53      3  3.621903                 4\n",
      "54      7  5.145810                 5\n",
      "55      8  5.981454                 6\n",
      "56      3  4.025247                 4\n",
      "57      3  3.666834                 4\n",
      "58      5  6.732258                 7\n",
      "59      7  4.689111                 5\n",
      "60     10  7.226698                 7\n",
      "61      5  4.414055                 4\n",
      "62      3  5.174444                 5\n",
      "63      3  3.627301                 4\n",
      "64      5  3.503628                 4\n",
      "65      5  4.414199                 4\n",
      "66      3  3.380605                 3\n",
      "67      5  4.479486                 4\n",
      "68      3  4.950058                 5\n",
      "69      8  5.351305                 5\n",
      "70      3  3.928690                 4\n",
      "71      9  5.505509                 6\n",
      "72      3  3.473281                 3\n",
      "73      3  3.253771                 3\n",
      "74      3  2.737524                 3\n",
      "75      3  4.212564                 4\n",
      "76      3  2.901020                 3\n",
      "77      3  3.383596                 3\n",
      "78     10  5.064827                 5\n",
      "79      8  4.133676                 4\n",
      "80      3  3.963325                 4\n",
      "81      5  2.879498                 3\n",
      "82      3  3.220866                 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 30  7  1  0\n",
      "         2 12 12  5  1\n",
      "         3  0  4  6  2\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6049          \n",
      "                 95% CI : (0.4901, 0.7119)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.07373         \n",
      "                                          \n",
      "                  Kappa : 0.3719          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7143   0.5217  0.50000  0.25000\n",
      "Specificity            0.7949   0.6897  0.91304  1.00000\n",
      "Pos Pred Value         0.7895   0.4000  0.50000  1.00000\n",
      "Neg Pred Value         0.7209   0.7843  0.91304  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3704   0.1481  0.07407  0.01235\n",
      "Detection Prevalence   0.4691   0.3704  0.14815  0.01235\n",
      "Balanced Accuracy      0.7546   0.6057  0.70652  0.62500\n",
      "Fold 4 Accuracy: 0.604938271604938 \n",
      "Fold 4 AUC: 0.805242839889579 \n",
      "Fold 4 Overall Sensitivity: 0.496506211180124 \n",
      "Fold 4 Overall Specificity: 0.849392611386614 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.72491451117058 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.268401                 4\n",
      "2       3  3.788573                 4\n",
      "3       3  2.866853                 3\n",
      "4       4  6.235184                 6\n",
      "5       3  4.340817                 4\n",
      "6       6  6.036498                 6\n",
      "7       3  4.079942                 4\n",
      "8       4  6.149514                 6\n",
      "9       3  4.831271                 5\n",
      "10      3  5.050793                 5\n",
      "11      3  3.965912                 4\n",
      "12      6  2.209707                 2\n",
      "13      3  2.975421                 3\n",
      "14      3  2.396823                 2\n",
      "15      6  6.124861                 6\n",
      "16      3  4.831370                 5\n",
      "17      5  7.800452                 8\n",
      "18      3  3.705605                 4\n",
      "19      3  2.735496                 3\n",
      "20      9  8.350416                 8\n",
      "21      3  4.449099                 4\n",
      "22      3  3.810435                 4\n",
      "23      3  4.319932                 4\n",
      "24      3  2.842461                 3\n",
      "25      5  5.093378                 5\n",
      "26      3  4.382893                 4\n",
      "27      3  5.192481                 5\n",
      "28      5  4.803291                 5\n",
      "29      5  3.522673                 4\n",
      "30      3  4.733243                 5\n",
      "31      3  5.407873                 5\n",
      "32      3  2.559179                 3\n",
      "33      8  7.302064                 7\n",
      "34      3  3.579025                 4\n",
      "35      3  3.651591                 4\n",
      "36      3  4.445984                 4\n",
      "37      3  3.921981                 4\n",
      "38      3  3.558875                 4\n",
      "39      5  5.473653                 5\n",
      "40      5  5.545188                 6\n",
      "41      6  6.050814                 6\n",
      "42      8  4.131163                 4\n",
      "43      9  5.473436                 5\n",
      "44      6  5.094063                 5\n",
      "45      9  7.163220                 7\n",
      "46      8  5.581855                 6\n",
      "47      3  3.384579                 3\n",
      "48      9  6.044120                 6\n",
      "49      7  7.206790                 7\n",
      "50     11  8.076163                 8\n",
      "51      8  6.238143                 6\n",
      "52      6  7.410830                 7\n",
      "53      5  3.940521                 4\n",
      "54      5  5.231506                 5\n",
      "55      3  3.831036                 4\n",
      "56      3  3.294790                 3\n",
      "57      3  4.609386                 5\n",
      "58      5  5.351452                 5\n",
      "59     12 10.035946                10\n",
      "60      5  4.547988                 5\n",
      "61      5  8.337618                 8\n",
      "62      5  4.269883                 4\n",
      "63     10  6.435260                 6\n",
      "64      3  3.730609                 4\n",
      "65      5  2.705808                 3\n",
      "66      5  4.389411                 4\n",
      "67     11  8.775243                 9\n",
      "68      5  4.415730                 4\n",
      "69      3  4.248565                 4\n",
      "70      3  5.074869                 5\n",
      "71      3  3.568984                 4\n",
      "72      3  5.907303                 6\n",
      "73      3  5.592031                 6\n",
      "74      3  4.484319                 4\n",
      "75      6  4.668873                 5\n",
      "76      3  3.922819                 4\n",
      "77      7  4.934330                 5\n",
      "78      9  7.831761                 8\n",
      "79      9  7.689935                 8\n",
      "80      6  6.527398                 7\n",
      "81      3  4.432570                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  6  1  0\n",
      "         2 11  9  6  1\n",
      "         3  2  7  4  1\n",
      "         4  0  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5375          \n",
      "                 95% CI : (0.4224, 0.6497)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.4562          \n",
      "                                          \n",
      "                  Kappa : 0.2763          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.3913   0.3333   0.3333\n",
      "Specificity            0.8158   0.6842   0.8529   0.9740\n",
      "Pos Pred Value         0.8056   0.3333   0.2857   0.3333\n",
      "Neg Pred Value         0.7045   0.7358   0.8788   0.9740\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.3625   0.1125   0.0500   0.0125\n",
      "Detection Prevalence   0.4500   0.3375   0.1750   0.0375\n",
      "Balanced Accuracy      0.7531   0.5378   0.5931   0.6537\n",
      "Fold 5 Accuracy: 0.5375 \n",
      "Fold 5 AUC: 0.758087474120083 \n",
      "Fold 5 Overall Sensitivity: 0.437111801242236 \n",
      "Fold 5 Overall Specificity: 0.831741787624141 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.643189873135773 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.737915                 5\n",
      "2       3  4.300545                 4\n",
      "3       3  4.390939                 4\n",
      "4       3  5.353123                 5\n",
      "5       3  8.973472                 9\n",
      "6       3  5.007412                 5\n",
      "7       5  2.454132                 2\n",
      "8       5  6.723616                 7\n",
      "9       4  3.895105                 4\n",
      "10      3  4.305279                 4\n",
      "11      7  8.203674                 8\n",
      "12      3  2.676052                 3\n",
      "13      7  6.749384                 7\n",
      "14      3  2.672851                 3\n",
      "15      3  3.850967                 4\n",
      "16      8  5.710397                 6\n",
      "17      3  3.804461                 4\n",
      "18      5  3.714082                 4\n",
      "19      3  3.316436                 3\n",
      "20      3  4.864228                 5\n",
      "21      3  3.903183                 4\n",
      "22      3  4.981934                 5\n",
      "23      5  5.819668                 6\n",
      "24      3  2.567203                 3\n",
      "25      6  9.909383                10\n",
      "26      6  7.815366                 8\n",
      "27      5  5.390177                 5\n",
      "28      5  5.272115                 5\n",
      "29      5  4.653288                 5\n",
      "30      3  4.084694                 4\n",
      "31      3  4.282501                 4\n",
      "32      4  4.708024                 5\n",
      "33      7  5.849114                 6\n",
      "34      3  3.311575                 3\n",
      "35      3  1.276902                 1\n",
      "36      5  4.226583                 4\n",
      "37      5  5.089832                 5\n",
      "38      7  8.270699                 8\n",
      "39     10 10.733367                11\n",
      "40     10  8.121776                 8\n",
      "41      3  4.585078                 5\n",
      "42      3  3.697189                 4\n",
      "43      8  5.976153                 6\n",
      "44      3  3.750026                 4\n",
      "45      7  6.178762                 6\n",
      "46      3  2.370083                 2\n",
      "47      7  4.482502                 4\n",
      "48      6  6.785235                 7\n",
      "49     11  5.426223                 5\n",
      "50      3  3.137457                 3\n",
      "51      6  7.850919                 8\n",
      "52      6  6.170236                 6\n",
      "53      3  4.665426                 5\n",
      "54      3  4.224302                 4\n",
      "55      9 10.142396                10\n",
      "56      3  3.631190                 4\n",
      "57      6  3.602370                 4\n",
      "58      3  4.759883                 5\n",
      "59      5  4.337181                 4\n",
      "60      5  7.065272                 7\n",
      "61      3  1.964907                 2\n",
      "62      3  2.681202                 3\n",
      "63      3  2.809698                 3\n",
      "64      9  5.702573                 6\n",
      "65      6  5.758305                 6\n",
      "66      5  4.018541                 4\n",
      "67      7  9.139758                 9\n",
      "68      3  4.004454                 4\n",
      "69      4  7.282522                 7\n",
      "70      3  3.515005                 4\n",
      "71      5  4.760217                 5\n",
      "72      6  7.407775                 7\n",
      "73      8  6.382088                 6\n",
      "74      6  4.599522                 5\n",
      "75      3  4.468073                 4\n",
      "76      3  4.747498                 5\n",
      "77      3  4.363522                 4\n",
      "78      3  5.167855                 5\n",
      "79      5  8.149931                 8\n",
      "80      3  2.464292                 2\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 154  39   5   1\n",
      "         2  52  56  34   7\n",
      "         3   4  22  21   8\n",
      "         4   0   1   1   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5725          \n",
      "                 95% CI : (0.5228, 0.6211)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.01269         \n",
      "                                          \n",
      "                  Kappa : 0.3114          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.01681         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7333   0.4746   0.3443 0.111111\n",
      "Specificity            0.7716   0.6782   0.9017 0.994859\n",
      "Pos Pred Value         0.7739   0.3758   0.3818 0.500000\n",
      "Neg Pred Value         0.7308   0.7597   0.8864 0.960298\n",
      "Prevalence             0.5160   0.2899   0.1499 0.044226\n",
      "Detection Rate         0.3784   0.1376   0.0516 0.004914\n",
      "Detection Prevalence   0.4889   0.3661   0.1351 0.009828\n",
      "Balanced Accuracy      0.7525   0.5764   0.6230 0.552985\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5723901 \n",
      "Average AUC: 0.7516103 \n",
      "Average Sensitivity: 0.4169663 \n",
      "Average Specificity: 0.8366386 \n",
      "Average Pearson Correlation: 0.6258332 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 20)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424822fa",
   "metadata": {},
   "source": [
    "# 25 Markers Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662a7f54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38  9  3  1\n",
      "         2  4 13  4  0\n",
      "         3  0  2  5  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6829          \n",
      "                 95% CI : (0.5708, 0.7813)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.001285        \n",
      "                                          \n",
      "                  Kappa : 0.4613          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.5417  0.41667  0.00000\n",
      "Specificity            0.6750   0.8621  0.92857  1.00000\n",
      "Pos Pred Value         0.7451   0.6190  0.50000      NaN\n",
      "Neg Pred Value         0.8710   0.8197  0.90278  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4634   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.6220   0.2561  0.12195  0.00000\n",
      "Balanced Accuracy      0.7899   0.7019  0.67262  0.50000\n",
      "Fold 1 Accuracy: 0.682926829268293 \n",
      "Fold 1 AUC: 0.755208333333333 \n",
      "Fold 1 Overall Sensitivity: 0.46577380952381 \n",
      "Fold 1 Overall Specificity: 0.866410098522167 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.710321857592923 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  7.775430                   7\n",
      "2       3  2.978218                   2\n",
      "3       3  3.749069                   3\n",
      "4       5  3.943935                   3\n",
      "5       5  5.976147                   5\n",
      "6       4  5.996992                   5\n",
      "7       3  4.942150                   4\n",
      "8       5  4.555329                   4\n",
      "9       5  4.613303                   4\n",
      "10      3  3.717765                   3\n",
      "11      3  4.816445                   4\n",
      "12      3  3.433309                   3\n",
      "13      5  5.377219                   5\n",
      "14     10  9.414483                   9\n",
      "15      3  3.318553                   3\n",
      "16      3  3.021815                   3\n",
      "17      6  7.217221                   7\n",
      "18      3  4.309497                   4\n",
      "19      3  2.501869                   2\n",
      "20      3  3.695681                   3\n",
      "21      3  5.350545                   5\n",
      "22      3  3.102687                   3\n",
      "23      3  2.866556                   2\n",
      "24      8  7.261834                   7\n",
      "25      3  3.400767                   3\n",
      "26      3  2.808754                   2\n",
      "27      3  4.409124                   4\n",
      "28      3  4.330890                   4\n",
      "29      3  4.485528                   4\n",
      "30      3  3.667582                   3\n",
      "31      5  2.720271                   2\n",
      "32      3  3.139868                   3\n",
      "33      3  3.194654                   3\n",
      "34      3  1.887526                   1\n",
      "35      5  6.141328                   6\n",
      "36      3  5.257840                   5\n",
      "37      6  5.776188                   5\n",
      "38      6  6.559079                   6\n",
      "39      3  4.155356                   4\n",
      "40      5  3.432640                   3\n",
      "41      5  5.373151                   5\n",
      "42      6  3.974664                   3\n",
      "43      3  2.574627                   2\n",
      "44      7  4.763894                   4\n",
      "45      7  4.805254                   4\n",
      "46      3  2.842724                   2\n",
      "47     11  7.565403                   7\n",
      "48      9  5.737079                   5\n",
      "49      3  4.612268                   4\n",
      "50      3  2.638190                   2\n",
      "51      3  3.526564                   3\n",
      "52      8  8.555764                   8\n",
      "53      6  2.299859                   2\n",
      "54      6  5.473819                   5\n",
      "55      5  5.335785                   5\n",
      "56      5  3.558772                   3\n",
      "57      7  7.646187                   7\n",
      "58      3  3.943266                   3\n",
      "59      3  2.669960                   2\n",
      "60      9  6.245771                   6\n",
      "61      9  5.369682                   5\n",
      "62      9  7.186973                   7\n",
      "63      3  4.431772                   4\n",
      "64      9  6.319286                   6\n",
      "65     10  4.941484                   4\n",
      "66      6  6.183178                   6\n",
      "67      3  4.403622                   4\n",
      "68      5  6.040269                   6\n",
      "69      5  2.450592                   2\n",
      "70      3  3.119501                   3\n",
      "71      6  5.472508                   5\n",
      "72      5  6.584960                   6\n",
      "73      8  7.246942                   7\n",
      "74      5  5.215209                   5\n",
      "75      3  6.129046                   6\n",
      "76      3  2.685936                   2\n",
      "77      7  4.572984                   4\n",
      "78      3  3.458838                   3\n",
      "79      3  4.530656                   4\n",
      "80     11  8.412297                   8\n",
      "81      3  2.968185                   2\n",
      "82      3  3.985776                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32 11  0  1\n",
      "         2  9 11  8  2\n",
      "         3  1  2  5  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.112           \n",
      "                                          \n",
      "                  Kappa : 0.3119          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.4583  0.38462  0.00000\n",
      "Specificity            0.7000   0.6724  0.95652  1.00000\n",
      "Pos Pred Value         0.7273   0.3667  0.62500      NaN\n",
      "Neg Pred Value         0.7368   0.7500  0.89189  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3902   0.1341  0.06098  0.00000\n",
      "Detection Prevalence   0.5366   0.3659  0.09756  0.00000\n",
      "Balanced Accuracy      0.7310   0.5654  0.67057  0.50000\n",
      "Fold 2 Accuracy: 0.585365853658537 \n",
      "Fold 2 AUC: 0.6346599002849 \n",
      "Fold 2 Overall Sensitivity: 0.40121336996337 \n",
      "Fold 2 Overall Specificity: 0.832233883058471 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.51734152451264 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  5.080454                   5\n",
      "2       4  4.418183                   4\n",
      "3       3  5.251783                   5\n",
      "4       3  5.555921                   5\n",
      "5       3  3.276293                   3\n",
      "6       3  2.415562                   2\n",
      "7       5  4.768613                   4\n",
      "8       3  5.185938                   5\n",
      "9       5  9.283583                   9\n",
      "10      4  3.325134                   3\n",
      "11      3  3.256691                   3\n",
      "12      3  6.750168                   6\n",
      "13      3  4.433676                   4\n",
      "14      6  5.587202                   5\n",
      "15     12  6.889142                   6\n",
      "16      6  7.375101                   7\n",
      "17      3  4.278207                   4\n",
      "18      5  5.647632                   5\n",
      "19      3  4.154331                   4\n",
      "20      3  4.327018                   4\n",
      "21      3  3.032666                   3\n",
      "22      3  3.626426                   3\n",
      "23      3  3.550895                   3\n",
      "24      3  2.901863                   2\n",
      "25      3  5.260062                   5\n",
      "26      3  5.437737                   5\n",
      "27      3  3.593859                   3\n",
      "28      3  3.945161                   3\n",
      "29      5  4.616183                   4\n",
      "30      4  9.342939                   9\n",
      "31      3  5.236723                   5\n",
      "32      3  3.369366                   3\n",
      "33      3  4.664736                   4\n",
      "34      3  3.454382                   3\n",
      "35      8  5.622154                   5\n",
      "36      6  6.872070                   6\n",
      "37      5  2.629410                   2\n",
      "38      3  5.328093                   5\n",
      "39      9  7.706183                   7\n",
      "40     12  5.622646                   5\n",
      "41      5  5.906303                   5\n",
      "42      7  5.396955                   5\n",
      "43      6  6.767060                   6\n",
      "44      5  4.740031                   4\n",
      "45      8  7.419164                   7\n",
      "46      5  4.568506                   4\n",
      "47      7  5.877002                   5\n",
      "48      5  4.905472                   4\n",
      "49      5  5.904206                   5\n",
      "50      3  3.246249                   3\n",
      "51      5  6.203086                   6\n",
      "52      8  6.286251                   6\n",
      "53      3  4.055142                   4\n",
      "54      5  6.892952                   6\n",
      "55      6  5.139320                   5\n",
      "56      6  5.791809                   5\n",
      "57      5  4.002091                   4\n",
      "58      6  4.747774                   4\n",
      "59      5  3.321130                   3\n",
      "60     11  4.030347                   4\n",
      "61      3  3.407626                   3\n",
      "62      3  1.939943                   1\n",
      "63      5  4.665219                   4\n",
      "64      3  4.356740                   4\n",
      "65      3  4.977752                   4\n",
      "66      7  5.967164                   5\n",
      "67      3  2.792466                   2\n",
      "68      5  6.860196                   6\n",
      "69      8  7.517034                   7\n",
      "70      8  6.222090                   6\n",
      "71      3  4.756285                   4\n",
      "72      3  3.052880                   3\n",
      "73      7  7.007770                   7\n",
      "74      3  4.407899                   4\n",
      "75      7  5.367381                   5\n",
      "76      3  3.901177                   3\n",
      "77      3  3.748637                   3\n",
      "78      3  3.803737                   3\n",
      "79      3  3.563817                   3\n",
      "80      5  3.841841                   3\n",
      "81      8  5.094977                   5\n",
      "82      8  8.213333                   8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 17  5  1\n",
      "         2  4  5  6  2\n",
      "         3  0  2  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5366         \n",
      "                 95% CI : (0.423, 0.6475)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.370593       \n",
      "                                         \n",
      "                  Kappa : 0.1592         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 0.004084       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048  0.20833  0.08333  0.00000\n",
      "Specificity            0.4250  0.79310  0.95714  1.00000\n",
      "Pos Pred Value         0.6230  0.29412  0.25000      NaN\n",
      "Neg Pred Value         0.8095  0.70769  0.85897  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4634  0.06098  0.01220  0.00000\n",
      "Detection Prevalence   0.7439  0.20732  0.04878  0.00000\n",
      "Balanced Accuracy      0.6649  0.50072  0.52024  0.50000\n",
      "Fold 3 Accuracy: 0.536585365853659 \n",
      "Fold 3 AUC: 0.697089947089947 \n",
      "Fold 3 Overall Sensitivity: 0.299107142857143 \n",
      "Fold 3 Overall Specificity: 0.79381157635468 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.509159073096179 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  2.922898                   2\n",
      "2       3  6.240832                   6\n",
      "3       3  2.894035                   2\n",
      "4       3  5.783510                   5\n",
      "5       3  4.204417                   4\n",
      "6       7  7.539658                   7\n",
      "7       5  3.911913                   3\n",
      "8       3  4.751857                   4\n",
      "9       3  4.926844                   4\n",
      "10      3  4.930330                   4\n",
      "11      4  3.576674                   3\n",
      "12      3  4.098811                   4\n",
      "13      3  4.986108                   4\n",
      "14      3  2.600081                   2\n",
      "15      8  4.538583                   4\n",
      "16      3  3.984672                   3\n",
      "17      3  5.376103                   5\n",
      "18      5  5.312459                   5\n",
      "19      3  3.558313                   3\n",
      "20      7  6.058175                   6\n",
      "21      3  3.320172                   3\n",
      "22      3  2.864766                   2\n",
      "23      6  5.615138                   5\n",
      "24      8  5.588346                   5\n",
      "25      3  2.946775                   2\n",
      "26     10  5.050745                   5\n",
      "27      3  3.132543                   3\n",
      "28      3  4.340332                   4\n",
      "29      3  3.190292                   3\n",
      "30      3  2.664862                   2\n",
      "31      3  3.702099                   3\n",
      "32      6  4.428934                   4\n",
      "33      6  5.954001                   5\n",
      "34      7  4.846280                   4\n",
      "35      5  7.227366                   7\n",
      "36      3  3.771136                   3\n",
      "37      5  3.724059                   3\n",
      "38      5  3.295437                   3\n",
      "39      3  3.364086                   3\n",
      "40      6  4.390476                   4\n",
      "41      3  4.435296                   4\n",
      "42      8  4.752274                   4\n",
      "43      5  4.635043                   4\n",
      "44      5  4.580007                   4\n",
      "45     12  5.579557                   5\n",
      "46      5  5.387348                   5\n",
      "47      6  4.185446                   4\n",
      "48      5  4.907810                   4\n",
      "49      6  5.882441                   5\n",
      "50      3  3.722407                   3\n",
      "51      5  4.378543                   4\n",
      "52      5  2.744566                   2\n",
      "53      3  3.673590                   3\n",
      "54      7  5.239419                   5\n",
      "55      8  5.985931                   5\n",
      "56      3  4.055380                   4\n",
      "57      3  3.733236                   3\n",
      "58      5  7.087971                   7\n",
      "59      7  4.743645                   4\n",
      "60     10  7.377620                   7\n",
      "61      5  4.533269                   4\n",
      "62      3  5.364256                   5\n",
      "63      3  3.705124                   3\n",
      "64      5  3.600293                   3\n",
      "65      5  4.298426                   4\n",
      "66      3  3.366591                   3\n",
      "67      5  4.536031                   4\n",
      "68      3  4.950260                   4\n",
      "69      8  5.181424                   5\n",
      "70      3  4.081694                   4\n",
      "71      9  5.570899                   5\n",
      "72      3  3.322520                   3\n",
      "73      3  3.310780                   3\n",
      "74      3  2.649651                   2\n",
      "75      3  4.138713                   4\n",
      "76      3  3.031042                   3\n",
      "77      3  3.213207                   3\n",
      "78     10  4.902896                   4\n",
      "79      8  4.290682                   4\n",
      "80      3  4.041508                   4\n",
      "81      5  2.575512                   2\n",
      "82      3  3.250933                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35 10  1  0\n",
      "         2  7 10  5  1\n",
      "         3  0  3  6  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6296          \n",
      "                 95% CI : (0.5151, 0.7344)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.02879         \n",
      "                                          \n",
      "                  Kappa : 0.3857          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.4348  0.50000  0.00000\n",
      "Specificity            0.7179   0.7759  0.91304  1.00000\n",
      "Pos Pred Value         0.7609   0.4348  0.50000      NaN\n",
      "Neg Pred Value         0.8000   0.7759  0.91304  0.95062\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4321   0.1235  0.07407  0.00000\n",
      "Detection Prevalence   0.5679   0.2840  0.14815  0.00000\n",
      "Balanced Accuracy      0.7756   0.6053  0.70652  0.50000\n",
      "Fold 4 Accuracy: 0.62962962962963 \n",
      "Fold 4 AUC: 0.810461956521739 \n",
      "Fold 4 Overall Sensitivity: 0.442028985507246 \n",
      "Fold 4 Overall Specificity: 0.851713566293776 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.720541234341377 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.376460                   4\n",
      "2       3  3.913031                   3\n",
      "3       3  2.921668                   2\n",
      "4       4  6.245954                   6\n",
      "5       3  4.327981                   4\n",
      "6       6  6.024784                   6\n",
      "7       3  4.119711                   4\n",
      "8       4  6.157776                   6\n",
      "9       3  4.765979                   4\n",
      "10      3  4.925620                   4\n",
      "11      3  4.045965                   4\n",
      "12      6  2.197322                   2\n",
      "13      3  2.980811                   2\n",
      "14      3  2.263558                   2\n",
      "15      6  5.909769                   5\n",
      "16      3  4.881875                   4\n",
      "17      5  7.711325                   7\n",
      "18      3  3.637403                   3\n",
      "19      3  2.738333                   2\n",
      "20      9  8.324523                   8\n",
      "21      3  4.317606                   4\n",
      "22      3  3.765948                   3\n",
      "23      3  4.307812                   4\n",
      "24      3  3.062390                   3\n",
      "25      5  5.173048                   5\n",
      "26      3  4.352676                   4\n",
      "27      3  5.056269                   5\n",
      "28      5  4.770276                   4\n",
      "29      5  3.544778                   3\n",
      "30      3  4.812588                   4\n",
      "31      3  5.316266                   5\n",
      "32      3  2.565805                   2\n",
      "33      8  7.237253                   7\n",
      "34      3  3.386983                   3\n",
      "35      3  3.410676                   3\n",
      "36      3  4.497850                   4\n",
      "37      3  3.944439                   3\n",
      "38      3  3.634397                   3\n",
      "39      5  5.342347                   5\n",
      "40      5  5.597362                   5\n",
      "41      6  5.992425                   5\n",
      "42      8  3.900071                   3\n",
      "43      9  5.544767                   5\n",
      "44      6  5.166285                   5\n",
      "45      9  7.127601                   7\n",
      "46      8  5.276327                   5\n",
      "47      3  3.458908                   3\n",
      "48      9  5.827280                   5\n",
      "49      7  7.104567                   7\n",
      "50     11  8.128351                   8\n",
      "51      8  6.288407                   6\n",
      "52      6  7.447797                   7\n",
      "53      5  3.890594                   3\n",
      "54      5  5.273217                   5\n",
      "55      3  3.820332                   3\n",
      "56      3  3.427300                   3\n",
      "57      3  4.671109                   4\n",
      "58      5  5.251329                   5\n",
      "59     12  9.901233                   9\n",
      "60      5  4.484928                   4\n",
      "61      5  8.481559                   8\n",
      "62      5  4.381661                   4\n",
      "63     10  6.726700                   6\n",
      "64      3  3.701060                   3\n",
      "65      5  2.872782                   2\n",
      "66      5  4.428500                   4\n",
      "67     11  8.759946                   8\n",
      "68      5  4.298805                   4\n",
      "69      3  4.756064                   4\n",
      "70      3  5.063796                   5\n",
      "71      3  3.586556                   3\n",
      "72      3  5.706995                   5\n",
      "73      3  5.591560                   5\n",
      "74      3  4.434107                   4\n",
      "75      6  4.510661                   4\n",
      "76      3  4.114752                   4\n",
      "77      7  5.006190                   5\n",
      "78      9  7.738328                   7\n",
      "79      9  7.742009                   7\n",
      "80      6  6.568228                   6\n",
      "81      3  4.533962                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38  9  1  0\n",
      "         2  2  8  7  1\n",
      "         3  2  6  3  1\n",
      "         4  0  0  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.625           \n",
      "                 95% CI : (0.5096, 0.7308)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.04591         \n",
      "                                          \n",
      "                  Kappa : 0.3717          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.3478   0.2500   0.3333\n",
      "Specificity            0.7368   0.8246   0.8676   0.9870\n",
      "Pos Pred Value         0.7917   0.4444   0.2500   0.5000\n",
      "Neg Pred Value         0.8750   0.7581   0.8676   0.9744\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4750   0.1000   0.0375   0.0125\n",
      "Detection Prevalence   0.6000   0.2250   0.1500   0.0250\n",
      "Balanced Accuracy      0.8208   0.5862   0.5588   0.6602\n",
      "Fold 5 Accuracy: 0.625 \n",
      "Fold 5 AUC: 0.799387508626639 \n",
      "Fold 5 Overall Sensitivity: 0.45898033126294 \n",
      "Fold 5 Overall Specificity: 0.854015888652112 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.631509826788349 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  5.027189                   5\n",
      "2       3  4.044119                   4\n",
      "3       3  4.454510                   4\n",
      "4       3  4.630089                   4\n",
      "5       3  8.509421                   8\n",
      "6       3  4.753662                   4\n",
      "7       5  2.663383                   2\n",
      "8       5  6.440507                   6\n",
      "9       4  3.877533                   3\n",
      "10      3  4.421749                   4\n",
      "11      7  8.051805                   8\n",
      "12      3  2.927089                   2\n",
      "13      7  6.455413                   6\n",
      "14      3  2.459069                   2\n",
      "15      3  3.719366                   3\n",
      "16      8  5.570612                   5\n",
      "17      3  4.003523                   4\n",
      "18      5  3.635854                   3\n",
      "19      3  3.381531                   3\n",
      "20      3  4.899308                   4\n",
      "21      3  4.413894                   4\n",
      "22      3  4.401381                   4\n",
      "23      5  6.403929                   6\n",
      "24      3  2.313119                   2\n",
      "25      6  9.975093                   9\n",
      "26      6  7.210951                   7\n",
      "27      5  5.707866                   5\n",
      "28      5  5.274782                   5\n",
      "29      5  4.733721                   4\n",
      "30      3  3.932532                   3\n",
      "31      3  4.052040                   4\n",
      "32      4  4.957777                   4\n",
      "33      7  5.224380                   5\n",
      "34      3  3.161100                   3\n",
      "35      3  1.366422                   1\n",
      "36      5  4.436171                   4\n",
      "37      5  5.116669                   5\n",
      "38      7  8.424405                   8\n",
      "39     10 10.854289                  10\n",
      "40     10  8.503710                   8\n",
      "41      3  4.200303                   4\n",
      "42      3  4.015347                   4\n",
      "43      8  5.428980                   5\n",
      "44      3  4.016796                   4\n",
      "45      7  6.100661                   6\n",
      "46      3  2.240558                   2\n",
      "47      7  4.127018                   4\n",
      "48      6  6.868932                   6\n",
      "49     11  5.271097                   5\n",
      "50      3  3.335142                   3\n",
      "51      6  7.766725                   7\n",
      "52      6  6.315468                   6\n",
      "53      3  4.702858                   4\n",
      "54      3  4.423020                   4\n",
      "55      9 10.366436                  10\n",
      "56      3  4.378461                   4\n",
      "57      6  3.466199                   3\n",
      "58      3  4.946748                   4\n",
      "59      5  4.093926                   4\n",
      "60      5  7.630568                   7\n",
      "61      3  2.512855                   2\n",
      "62      3  2.630006                   2\n",
      "63      3  2.911995                   2\n",
      "64      9  5.526711                   5\n",
      "65      6  5.691842                   5\n",
      "66      5  3.768403                   3\n",
      "67      7  8.883320                   8\n",
      "68      3  3.230129                   3\n",
      "69      4  7.381097                   7\n",
      "70      3  3.983620                   3\n",
      "71      5  4.840999                   4\n",
      "72      6  7.727137                   7\n",
      "73      8  6.665287                   6\n",
      "74      6  4.357172                   4\n",
      "75      3  4.373514                   4\n",
      "76      3  4.841378                   4\n",
      "77      3  4.194904                   4\n",
      "78      3  5.660787                   5\n",
      "79      5  7.968447                   7\n",
      "80      3  2.364099                   2\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 181  56  10   3\n",
      "         2  26  47  30   6\n",
      "         3   3  15  20   8\n",
      "         4   0   0   1   1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6118          \n",
      "                 95% CI : (0.5625, 0.6594)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 6.232e-05       \n",
      "                                          \n",
      "                  Kappa : 0.3401          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 6.184e-06       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8619   0.3983  0.32787 0.055556\n",
      "Specificity            0.6497   0.7855  0.92486 0.997429\n",
      "Pos Pred Value         0.7240   0.4312  0.43478 0.500000\n",
      "Neg Pred Value         0.8153   0.7617  0.88643 0.958025\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4447   0.1155  0.04914 0.002457\n",
      "Detection Prevalence   0.6143   0.2678  0.11302 0.004914\n",
      "Balanced Accuracy      0.7558   0.5919  0.62636 0.526492\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6119015 \n",
      "Average AUC: 0.7393615 \n",
      "Average Sensitivity: 0.4134207 \n",
      "Average Specificity: 0.839637 \n",
      "Average Pearson Correlation: 0.6177747 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 25)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Truncate the predicted values before classification\n",
    "    truncated_predictions <- floor(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(truncated_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Truncated_Predicted = truncated_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237354b",
   "metadata": {},
   "source": [
    "# 15 Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeb49bf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  8  2  0\n",
      "         2  8 13  5  1\n",
      "         3  0  3  5  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6341          \n",
      "                 95% CI : (0.5205, 0.7378)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.01746         \n",
      "                                          \n",
      "                  Kappa : 0.3994          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.5417  0.41667  0.00000\n",
      "Specificity            0.7500   0.7586  0.91429  1.00000\n",
      "Pos Pred Value         0.7727   0.4815  0.45455      NaN\n",
      "Neg Pred Value         0.7895   0.8000  0.90141  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4146   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.5366   0.3293  0.13415  0.00000\n",
      "Balanced Accuracy      0.7798   0.6501  0.66548  0.50000\n",
      "Fold 1 Accuracy: 0.634146341463415 \n",
      "Fold 1 AUC: 0.799851190476191 \n",
      "Fold 1 Overall Sensitivity: 0.441964285714286 \n",
      "Fold 1 Overall Specificity: 0.855726600985222 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.736576251668698 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  6.941802                 7\n",
      "2       3  3.394260                 3\n",
      "3       3  3.424460                 3\n",
      "4       5  4.154265                 4\n",
      "5       5  5.583478                 6\n",
      "6       4  6.077646                 6\n",
      "7       3  4.860882                 5\n",
      "8       5  4.206424                 4\n",
      "9       5  4.855399                 5\n",
      "10      3  3.399985                 3\n",
      "11      3  4.198636                 4\n",
      "12      3  3.418613                 3\n",
      "13      5  4.718085                 5\n",
      "14     10  9.312093                 9\n",
      "15      3  3.665059                 4\n",
      "16      3  3.136755                 3\n",
      "17      6  7.077955                 7\n",
      "18      3  3.813540                 4\n",
      "19      3  3.074844                 3\n",
      "20      3  3.933425                 4\n",
      "21      3  5.254164                 5\n",
      "22      3  2.951714                 3\n",
      "23      3  2.717552                 3\n",
      "24      8  7.362459                 7\n",
      "25      3  3.622622                 4\n",
      "26      3  2.939534                 3\n",
      "27      3  5.000735                 5\n",
      "28      3  4.226450                 4\n",
      "29      3  4.597995                 5\n",
      "30      3  3.766963                 4\n",
      "31      5  3.014728                 3\n",
      "32      3  3.327735                 3\n",
      "33      3  3.432786                 3\n",
      "34      3  2.157115                 2\n",
      "35      5  6.040184                 6\n",
      "36      3  4.972865                 5\n",
      "37      6  5.644599                 6\n",
      "38      6  6.473463                 6\n",
      "39      3  4.039827                 4\n",
      "40      5  3.903519                 4\n",
      "41      5  5.207992                 5\n",
      "42      6  4.091229                 4\n",
      "43      3  3.346582                 3\n",
      "44      7  4.920536                 5\n",
      "45      7  4.369304                 4\n",
      "46      3  3.501804                 4\n",
      "47     11  7.342177                 7\n",
      "48      9  5.774317                 6\n",
      "49      3  4.084275                 4\n",
      "50      3  2.711728                 3\n",
      "51      3  3.386186                 3\n",
      "52      8  8.512715                 9\n",
      "53      6  2.211775                 2\n",
      "54      6  5.178506                 5\n",
      "55      5  5.210391                 5\n",
      "56      5  3.890116                 4\n",
      "57      7  7.464926                 7\n",
      "58      3  3.548958                 4\n",
      "59      3  2.432736                 2\n",
      "60      9  6.204623                 6\n",
      "61      9  5.540480                 6\n",
      "62      9  7.083535                 7\n",
      "63      3  4.236818                 4\n",
      "64      9  6.263856                 6\n",
      "65     10  4.891531                 5\n",
      "66      6  5.946223                 6\n",
      "67      3  4.318787                 4\n",
      "68      5  6.018421                 6\n",
      "69      5  3.349518                 3\n",
      "70      3  2.887618                 3\n",
      "71      6  5.204084                 5\n",
      "72      5  6.798497                 7\n",
      "73      8  7.091065                 7\n",
      "74      5  5.266558                 5\n",
      "75      3  5.201997                 5\n",
      "76      3  3.330921                 3\n",
      "77      7  4.341716                 4\n",
      "78      3  3.841778                 4\n",
      "79      3  4.685140                 5\n",
      "80     11  8.538467                 9\n",
      "81      3  2.726056                 3\n",
      "82      3  4.095720                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  6  0  1\n",
      "         2 12 13  8  2\n",
      "         3  2  5  5  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.561          \n",
      "                 95% CI : (0.447, 0.6704)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2199         \n",
      "                                         \n",
      "                  Kappa : 0.3067         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.5417  0.38462  0.00000\n",
      "Specificity            0.8250   0.6207  0.89855  1.00000\n",
      "Pos Pred Value         0.8000   0.3714  0.41667      NaN\n",
      "Neg Pred Value         0.7021   0.7660  0.88571  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3415   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.4268   0.4268  0.14634  0.00000\n",
      "Balanced Accuracy      0.7458   0.5812  0.64158  0.50000\n",
      "Fold 2 Accuracy: 0.560975609756098 \n",
      "Fold 2 AUC: 0.584783272283272 \n",
      "Fold 2 Overall Sensitivity: 0.398237179487179 \n",
      "Fold 2 Overall Specificity: 0.836060094952524 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.515047571287942 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  5.128586                 5\n",
      "2       4  4.373379                 4\n",
      "3       3  4.694598                 5\n",
      "4       3  5.409739                 5\n",
      "5       3  3.458662                 3\n",
      "6       3  2.800703                 3\n",
      "7       5  4.415177                 4\n",
      "8       3  5.070885                 5\n",
      "9       5  9.282936                 9\n",
      "10      4  3.286255                 3\n",
      "11      3  3.358172                 3\n",
      "12      3  6.568207                 7\n",
      "13      3  4.296091                 4\n",
      "14      6  5.360576                 5\n",
      "15     12  6.158936                 6\n",
      "16      6  7.139025                 7\n",
      "17      3  4.142126                 4\n",
      "18      5  5.930594                 6\n",
      "19      3  3.943438                 4\n",
      "20      3  4.424368                 4\n",
      "21      3  3.144164                 3\n",
      "22      3  3.578613                 4\n",
      "23      3  3.584980                 4\n",
      "24      3  2.490509                 2\n",
      "25      3  5.137055                 5\n",
      "26      3  4.673328                 5\n",
      "27      3  3.713641                 4\n",
      "28      3  3.806855                 4\n",
      "29      5  4.563552                 5\n",
      "30      4  9.016488                 9\n",
      "31      3  5.043836                 5\n",
      "32      3  3.440736                 3\n",
      "33      3  4.622522                 5\n",
      "34      3  3.425176                 3\n",
      "35      8  5.675042                 6\n",
      "36      6  6.423128                 6\n",
      "37      5  2.788291                 3\n",
      "38      3  5.283568                 5\n",
      "39      9  7.518227                 8\n",
      "40     12  6.291300                 6\n",
      "41      5  5.567056                 6\n",
      "42      7  5.276805                 5\n",
      "43      6  6.797524                 7\n",
      "44      5  4.639495                 5\n",
      "45      8  7.531132                 8\n",
      "46      5  4.501475                 5\n",
      "47      7  5.387056                 5\n",
      "48      5  4.980341                 5\n",
      "49      5  5.517047                 6\n",
      "50      3  3.237688                 3\n",
      "51      5  6.185699                 6\n",
      "52      8  5.815645                 6\n",
      "53      3  4.257814                 4\n",
      "54      5  6.766539                 7\n",
      "55      6  5.105289                 5\n",
      "56      6  6.285633                 6\n",
      "57      5  3.923472                 4\n",
      "58      6  5.276543                 5\n",
      "59      5  3.374081                 3\n",
      "60     11  3.775372                 4\n",
      "61      3  3.424400                 3\n",
      "62      3  2.317911                 2\n",
      "63      5  4.223365                 4\n",
      "64      3  4.230657                 4\n",
      "65      3  5.028375                 5\n",
      "66      7  5.842651                 6\n",
      "67      3  2.819582                 3\n",
      "68      5  6.774359                 7\n",
      "69      8  7.553539                 8\n",
      "70      8  5.956173                 6\n",
      "71      3  4.747736                 5\n",
      "72      3  3.625552                 4\n",
      "73      7  7.271787                 7\n",
      "74      3  4.445810                 4\n",
      "75      7  5.341599                 5\n",
      "76      3  4.039141                 4\n",
      "77      3  3.769292                 4\n",
      "78      3  4.527602                 5\n",
      "79      3  3.601106                 4\n",
      "80      5  4.122932                 4\n",
      "81      8  4.691832                 5\n",
      "82      8  8.264181                 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35 10  3  0\n",
      "         2  7 13  8  3\n",
      "         3  0  1  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5976          \n",
      "                 95% CI : (0.4834, 0.7044)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.07514         \n",
      "                                          \n",
      "                  Kappa : 0.3111          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.5417  0.08333  0.00000\n",
      "Specificity            0.6750   0.6897  0.97143  1.00000\n",
      "Pos Pred Value         0.7292   0.4194  0.33333      NaN\n",
      "Neg Pred Value         0.7941   0.7843  0.86076  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4268   0.1585  0.01220  0.00000\n",
      "Detection Prevalence   0.5854   0.3780  0.03659  0.00000\n",
      "Balanced Accuracy      0.7542   0.6157  0.52738  0.50000\n",
      "Fold 3 Accuracy: 0.597560975609756 \n",
      "Fold 3 AUC: 0.746238425925926 \n",
      "Fold 3 Overall Sensitivity: 0.364583333333333 \n",
      "Fold 3 Overall Specificity: 0.834020935960591 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.474426778967444 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  3.802240                 4\n",
      "2       3  6.149721                 6\n",
      "3       3  3.626635                 4\n",
      "4       3  5.420969                 5\n",
      "5       3  4.281910                 4\n",
      "6       7  7.614967                 8\n",
      "7       5  3.684494                 4\n",
      "8       3  4.128988                 4\n",
      "9       3  4.261791                 4\n",
      "10      3  4.788842                 5\n",
      "11      4  3.942318                 4\n",
      "12      3  4.188947                 4\n",
      "13      3  4.978270                 5\n",
      "14      3  2.909207                 3\n",
      "15      8  4.346301                 4\n",
      "16      3  3.788574                 4\n",
      "17      3  6.370447                 6\n",
      "18      5  5.619471                 6\n",
      "19      3  4.137069                 4\n",
      "20      7  5.479012                 5\n",
      "21      3  4.054833                 4\n",
      "22      3  3.072800                 3\n",
      "23      6  4.513350                 5\n",
      "24      8  5.253856                 5\n",
      "25      3  4.011727                 4\n",
      "26     10  5.623792                 6\n",
      "27      3  3.654846                 4\n",
      "28      3  4.198568                 4\n",
      "29      3  4.371117                 4\n",
      "30      3  2.951514                 3\n",
      "31      3  4.230382                 4\n",
      "32      6  3.945386                 4\n",
      "33      6  5.072469                 5\n",
      "34      7  5.268080                 5\n",
      "35      5  7.112339                 7\n",
      "36      3  3.539060                 4\n",
      "37      5  4.526984                 5\n",
      "38      5  4.662478                 5\n",
      "39      3  3.482011                 3\n",
      "40      6  3.319496                 3\n",
      "41      3  4.352543                 4\n",
      "42      8  3.678690                 4\n",
      "43      5  4.553774                 5\n",
      "44      5  4.489386                 4\n",
      "45     12  5.218443                 5\n",
      "46      5  4.888901                 5\n",
      "47      6  4.108590                 4\n",
      "48      5  4.939876                 5\n",
      "49      6  5.987520                 6\n",
      "50      3  3.142911                 3\n",
      "51      5  4.258797                 4\n",
      "52      5  4.228946                 4\n",
      "53      3  4.226581                 4\n",
      "54      7  4.942919                 5\n",
      "55      8  4.938313                 5\n",
      "56      3  3.581967                 4\n",
      "57      3  4.186224                 4\n",
      "58      5  6.272300                 6\n",
      "59      7  6.268826                 6\n",
      "60     10  6.785799                 7\n",
      "61      5  4.669605                 5\n",
      "62      3  4.750437                 5\n",
      "63      3  2.860410                 3\n",
      "64      5  4.548833                 5\n",
      "65      5  4.089924                 4\n",
      "66      3  3.190180                 3\n",
      "67      5  5.195082                 5\n",
      "68      3  5.296770                 5\n",
      "69      8  5.907770                 6\n",
      "70      3  3.305408                 3\n",
      "71      9  5.455228                 5\n",
      "72      3  3.730409                 4\n",
      "73      3  3.873537                 4\n",
      "74      3  3.593589                 4\n",
      "75      3  4.280584                 4\n",
      "76      3  3.307308                 3\n",
      "77      3  3.985197                 4\n",
      "78     10  5.167257                 5\n",
      "79      8  3.481889                 3\n",
      "80      3  3.671298                 4\n",
      "81      5  3.855379                 4\n",
      "82      3  3.311525                 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 30  8  0  0\n",
      "         2 12 12  6  0\n",
      "         3  0  3  6  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6049          \n",
      "                 95% CI : (0.4901, 0.7119)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.07373         \n",
      "                                          \n",
      "                  Kappa : 0.3719          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7143   0.5217  0.50000  0.25000\n",
      "Specificity            0.7949   0.6897  0.91304  1.00000\n",
      "Pos Pred Value         0.7895   0.4000  0.50000  1.00000\n",
      "Neg Pred Value         0.7209   0.7843  0.91304  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3704   0.1481  0.07407  0.01235\n",
      "Detection Prevalence   0.4691   0.3704  0.14815  0.01235\n",
      "Balanced Accuracy      0.7546   0.6057  0.70652  0.62500\n",
      "Fold 4 Accuracy: 0.604938271604938 \n",
      "Fold 4 AUC: 0.860960144927536 \n",
      "Fold 4 Overall Sensitivity: 0.496506211180124 \n",
      "Fold 4 Overall Specificity: 0.849392611386614 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.733314945521598 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.226711                 4\n",
      "2       3  4.252511                 4\n",
      "3       3  2.673361                 3\n",
      "4       4  6.432249                 6\n",
      "5       3  4.443312                 4\n",
      "6       6  5.869793                 6\n",
      "7       3  3.921247                 4\n",
      "8       4  6.352297                 6\n",
      "9       3  4.849033                 5\n",
      "10      3  4.804680                 5\n",
      "11      3  4.020830                 4\n",
      "12      6  2.067969                 2\n",
      "13      3  3.233840                 3\n",
      "14      3  2.455722                 2\n",
      "15      6  6.135239                 6\n",
      "16      3  4.630342                 5\n",
      "17      5  7.829142                 8\n",
      "18      3  3.971126                 4\n",
      "19      3  2.888593                 3\n",
      "20      9  8.338334                 8\n",
      "21      3  4.283440                 4\n",
      "22      3  4.057574                 4\n",
      "23      3  4.082502                 4\n",
      "24      3  2.937707                 3\n",
      "25      5  5.001245                 5\n",
      "26      3  4.370119                 4\n",
      "27      3  5.371262                 5\n",
      "28      5  4.599573                 5\n",
      "29      5  3.834217                 4\n",
      "30      3  4.405794                 4\n",
      "31      3  5.355742                 5\n",
      "32      3  2.212429                 2\n",
      "33      8  7.397674                 7\n",
      "34      3  3.961310                 4\n",
      "35      3  3.458215                 3\n",
      "36      3  4.674798                 5\n",
      "37      3  3.503863                 4\n",
      "38      3  3.642919                 4\n",
      "39      5  5.359218                 5\n",
      "40      5  5.552827                 6\n",
      "41      6  5.998956                 6\n",
      "42      8  4.589609                 5\n",
      "43      9  5.480498                 5\n",
      "44      6  5.087243                 5\n",
      "45      9  6.949555                 7\n",
      "46      8  5.440156                 5\n",
      "47      3  3.239106                 3\n",
      "48      9  6.196118                 6\n",
      "49      7  7.051044                 7\n",
      "50     11  8.162805                 8\n",
      "51      8  6.215795                 6\n",
      "52      6  7.252252                 7\n",
      "53      5  4.003951                 4\n",
      "54      5  5.411156                 5\n",
      "55      3  4.167702                 4\n",
      "56      3  3.253635                 3\n",
      "57      3  4.634778                 5\n",
      "58      5  5.289705                 5\n",
      "59     12  9.916499                10\n",
      "60      5  4.318791                 4\n",
      "61      5  7.911314                 8\n",
      "62      5  4.227174                 4\n",
      "63     10  6.935460                 7\n",
      "64      3  3.537033                 4\n",
      "65      5  2.399933                 2\n",
      "66      5  4.138707                 4\n",
      "67     11  8.875423                 9\n",
      "68      5  4.948093                 5\n",
      "69      3  4.062590                 4\n",
      "70      3  5.414308                 5\n",
      "71      3  3.410218                 3\n",
      "72      3  5.960984                 6\n",
      "73      3  5.448450                 5\n",
      "74      3  4.301969                 4\n",
      "75      6  4.350136                 4\n",
      "76      3  3.673006                 4\n",
      "77      7  4.834607                 5\n",
      "78      9  7.903439                 8\n",
      "79      9  7.791090                 8\n",
      "80      6  6.422231                 6\n",
      "81      3  4.120080                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  5  0  0\n",
      "         2 11 12  6  1\n",
      "         3  1  5  5  1\n",
      "         4  1  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5875          \n",
      "                 95% CI : (0.4718, 0.6965)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.1568          \n",
      "                                          \n",
      "                  Kappa : 0.3602          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.6306          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.5217   0.4167   0.3333\n",
      "Specificity            0.8684   0.6842   0.8971   0.9610\n",
      "Pos Pred Value         0.8529   0.4000   0.4167   0.2500\n",
      "Neg Pred Value         0.7174   0.7800   0.8971   0.9737\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.3625   0.1500   0.0625   0.0125\n",
      "Detection Prevalence   0.4250   0.3750   0.1500   0.0500\n",
      "Balanced Accuracy      0.7794   0.6030   0.6569   0.6472\n",
      "Fold 5 Accuracy: 0.5875 \n",
      "Fold 5 AUC: 0.776095583160801 \n",
      "Fold 5 Overall Sensitivity: 0.490553830227743 \n",
      "Fold 5 Overall Specificity: 0.852682340878935 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.611802092329494 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.745888                 5\n",
      "2       3  4.449810                 4\n",
      "3       3  4.643437                 5\n",
      "4       3  5.348280                 5\n",
      "5       3  9.688750                10\n",
      "6       3  4.815954                 5\n",
      "7       5  2.286330                 2\n",
      "8       5  6.385020                 6\n",
      "9       4  4.149598                 4\n",
      "10      3  4.293013                 4\n",
      "11      7  8.044916                 8\n",
      "12      3  2.950112                 3\n",
      "13      7  6.529831                 7\n",
      "14      3  2.593404                 3\n",
      "15      3  3.308544                 3\n",
      "16      8  5.841033                 6\n",
      "17      3  3.646305                 4\n",
      "18      5  3.580804                 4\n",
      "19      3  3.346572                 3\n",
      "20      3  4.933884                 5\n",
      "21      3  4.445883                 4\n",
      "22      3  4.741313                 5\n",
      "23      5  5.551264                 6\n",
      "24      3  3.563183                 4\n",
      "25      6 10.047535                10\n",
      "26      6  7.979218                 8\n",
      "27      5  5.282655                 5\n",
      "28      5  5.280417                 5\n",
      "29      5  5.490699                 5\n",
      "30      3  3.700337                 4\n",
      "31      3  4.480859                 4\n",
      "32      4  4.820043                 5\n",
      "33      7  5.975256                 6\n",
      "34      3  3.173454                 3\n",
      "35      3  1.202823                 1\n",
      "36      5  4.715142                 5\n",
      "37      5  5.481523                 5\n",
      "38      7  8.055081                 8\n",
      "39     10 10.691624                11\n",
      "40     10  8.095486                 8\n",
      "41      3  4.469098                 4\n",
      "42      3  3.967417                 4\n",
      "43      8  5.485962                 5\n",
      "44      3  3.314320                 3\n",
      "45      7  5.544986                 6\n",
      "46      3  2.259206                 2\n",
      "47      7  4.763017                 5\n",
      "48      6  6.481135                 6\n",
      "49     11  4.816347                 5\n",
      "50      3  2.868120                 3\n",
      "51      6  7.840812                 8\n",
      "52      6  5.992818                 6\n",
      "53      3  4.751735                 5\n",
      "54      3  4.325908                 4\n",
      "55      9 10.120458                10\n",
      "56      3  4.175242                 4\n",
      "57      6  4.099561                 4\n",
      "58      3  5.076561                 5\n",
      "59      5  4.528436                 5\n",
      "60      5  6.737850                 7\n",
      "61      3  1.199187                 1\n",
      "62      3  2.937767                 3\n",
      "63      3  2.890250                 3\n",
      "64      9  5.754594                 6\n",
      "65      6  5.567558                 6\n",
      "66      5  3.588589                 4\n",
      "67      7  9.478968                 9\n",
      "68      3  3.172780                 3\n",
      "69      4  7.477733                 7\n",
      "70      3  2.870668                 3\n",
      "71      5  4.867698                 5\n",
      "72      6  7.131563                 7\n",
      "73      8  6.617221                 7\n",
      "74      6  4.304061                 4\n",
      "75      3  4.346321                 4\n",
      "76      3  4.582041                 5\n",
      "77      3  4.141873                 4\n",
      "78      3  5.483430                 5\n",
      "79      5  8.498734                 8\n",
      "80      3  3.248318                 3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 156  37   5   1\n",
      "         2  50  63  33   7\n",
      "         3   3  17  22   8\n",
      "         4   1   1   1   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5971          \n",
      "                 95% CI : (0.5476, 0.6451)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.0006085       \n",
      "                                          \n",
      "                  Kappa : 0.3498          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.0075900       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7429   0.5339  0.36066 0.111111\n",
      "Specificity            0.7817   0.6886  0.91908 0.992288\n",
      "Pos Pred Value         0.7839   0.4118  0.44000 0.400000\n",
      "Neg Pred Value         0.7404   0.7835  0.89076 0.960199\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.3833   0.1548  0.05405 0.004914\n",
      "Detection Prevalence   0.4889   0.3759  0.12285 0.012285\n",
      "Balanced Accuracy      0.7623   0.6112  0.63987 0.551700\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5970242 \n",
      "Average AUC: 0.7535857 \n",
      "Average Sensitivity: 0.438369 \n",
      "Average Specificity: 0.8455765 \n",
      "Average Pearson Correlation: 0.6142335 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 15)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d297b",
   "metadata": {},
   "source": [
    "# 15 Markers Truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f88f98a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 10  3  1\n",
      "         2  4 13  4  0\n",
      "         3  0  1  5  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6829          \n",
      "                 95% CI : (0.5708, 0.7813)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.001285        \n",
      "                                          \n",
      "                  Kappa : 0.4572          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.5417  0.41667  0.00000\n",
      "Specificity            0.6500   0.8621  0.94286  1.00000\n",
      "Pos Pred Value         0.7308   0.6190  0.55556      NaN\n",
      "Neg Pred Value         0.8667   0.8197  0.90411  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4634   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.6341   0.2561  0.10976  0.00000\n",
      "Balanced Accuracy      0.7774   0.7019  0.67976  0.50000\n",
      "Fold 1 Accuracy: 0.682926829268293 \n",
      "Fold 1 AUC: 0.759507275132275 \n",
      "Fold 1 Overall Sensitivity: 0.46577380952381 \n",
      "Fold 1 Overall Specificity: 0.863731527093596 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.736576251668698 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  6.941802                   6\n",
      "2       3  3.394260                   3\n",
      "3       3  3.424460                   3\n",
      "4       5  4.154265                   4\n",
      "5       5  5.583478                   5\n",
      "6       4  6.077646                   6\n",
      "7       3  4.860882                   4\n",
      "8       5  4.206424                   4\n",
      "9       5  4.855399                   4\n",
      "10      3  3.399985                   3\n",
      "11      3  4.198636                   4\n",
      "12      3  3.418613                   3\n",
      "13      5  4.718085                   4\n",
      "14     10  9.312093                   9\n",
      "15      3  3.665059                   3\n",
      "16      3  3.136755                   3\n",
      "17      6  7.077955                   7\n",
      "18      3  3.813540                   3\n",
      "19      3  3.074844                   3\n",
      "20      3  3.933425                   3\n",
      "21      3  5.254164                   5\n",
      "22      3  2.951714                   2\n",
      "23      3  2.717552                   2\n",
      "24      8  7.362459                   7\n",
      "25      3  3.622622                   3\n",
      "26      3  2.939534                   2\n",
      "27      3  5.000735                   5\n",
      "28      3  4.226450                   4\n",
      "29      3  4.597995                   4\n",
      "30      3  3.766963                   3\n",
      "31      5  3.014728                   3\n",
      "32      3  3.327735                   3\n",
      "33      3  3.432786                   3\n",
      "34      3  2.157115                   2\n",
      "35      5  6.040184                   6\n",
      "36      3  4.972865                   4\n",
      "37      6  5.644599                   5\n",
      "38      6  6.473463                   6\n",
      "39      3  4.039827                   4\n",
      "40      5  3.903519                   3\n",
      "41      5  5.207992                   5\n",
      "42      6  4.091229                   4\n",
      "43      3  3.346582                   3\n",
      "44      7  4.920536                   4\n",
      "45      7  4.369304                   4\n",
      "46      3  3.501804                   3\n",
      "47     11  7.342177                   7\n",
      "48      9  5.774317                   5\n",
      "49      3  4.084275                   4\n",
      "50      3  2.711728                   2\n",
      "51      3  3.386186                   3\n",
      "52      8  8.512715                   8\n",
      "53      6  2.211775                   2\n",
      "54      6  5.178506                   5\n",
      "55      5  5.210391                   5\n",
      "56      5  3.890116                   3\n",
      "57      7  7.464926                   7\n",
      "58      3  3.548958                   3\n",
      "59      3  2.432736                   2\n",
      "60      9  6.204623                   6\n",
      "61      9  5.540480                   5\n",
      "62      9  7.083535                   7\n",
      "63      3  4.236818                   4\n",
      "64      9  6.263856                   6\n",
      "65     10  4.891531                   4\n",
      "66      6  5.946223                   5\n",
      "67      3  4.318787                   4\n",
      "68      5  6.018421                   6\n",
      "69      5  3.349518                   3\n",
      "70      3  2.887618                   2\n",
      "71      6  5.204084                   5\n",
      "72      5  6.798497                   6\n",
      "73      8  7.091065                   7\n",
      "74      5  5.266558                   5\n",
      "75      3  5.201997                   5\n",
      "76      3  3.330921                   3\n",
      "77      7  4.341716                   4\n",
      "78      3  3.841778                   3\n",
      "79      3  4.685140                   4\n",
      "80     11  8.538467                   8\n",
      "81      3  2.726056                   2\n",
      "82      3  4.095720                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 33 10  1  1\n",
      "         2  8 12  7  2\n",
      "         3  1  2  5  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6098          \n",
      "                 95% CI : (0.4957, 0.7156)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.0483          \n",
      "                                          \n",
      "                  Kappa : 0.3495          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7857   0.5000  0.38462  0.00000\n",
      "Specificity            0.7000   0.7069  0.95652  1.00000\n",
      "Pos Pred Value         0.7333   0.4138  0.62500      NaN\n",
      "Neg Pred Value         0.7568   0.7736  0.89189  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4024   0.1463  0.06098  0.00000\n",
      "Detection Prevalence   0.5488   0.3537  0.09756  0.00000\n",
      "Balanced Accuracy      0.7429   0.6034  0.67057  0.50000\n",
      "Fold 2 Accuracy: 0.609756097560976 \n",
      "Fold 2 AUC: 0.631740944240944 \n",
      "Fold 2 Overall Sensitivity: 0.417582417582418 \n",
      "Fold 2 Overall Specificity: 0.840854572713643 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.515047571287942 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  5.128586                   5\n",
      "2       4  4.373379                   4\n",
      "3       3  4.694598                   4\n",
      "4       3  5.409739                   5\n",
      "5       3  3.458662                   3\n",
      "6       3  2.800703                   2\n",
      "7       5  4.415177                   4\n",
      "8       3  5.070885                   5\n",
      "9       5  9.282936                   9\n",
      "10      4  3.286255                   3\n",
      "11      3  3.358172                   3\n",
      "12      3  6.568207                   6\n",
      "13      3  4.296091                   4\n",
      "14      6  5.360576                   5\n",
      "15     12  6.158936                   6\n",
      "16      6  7.139025                   7\n",
      "17      3  4.142126                   4\n",
      "18      5  5.930594                   5\n",
      "19      3  3.943438                   3\n",
      "20      3  4.424368                   4\n",
      "21      3  3.144164                   3\n",
      "22      3  3.578613                   3\n",
      "23      3  3.584980                   3\n",
      "24      3  2.490509                   2\n",
      "25      3  5.137055                   5\n",
      "26      3  4.673328                   4\n",
      "27      3  3.713641                   3\n",
      "28      3  3.806855                   3\n",
      "29      5  4.563552                   4\n",
      "30      4  9.016488                   9\n",
      "31      3  5.043836                   5\n",
      "32      3  3.440736                   3\n",
      "33      3  4.622522                   4\n",
      "34      3  3.425176                   3\n",
      "35      8  5.675042                   5\n",
      "36      6  6.423128                   6\n",
      "37      5  2.788291                   2\n",
      "38      3  5.283568                   5\n",
      "39      9  7.518227                   7\n",
      "40     12  6.291300                   6\n",
      "41      5  5.567056                   5\n",
      "42      7  5.276805                   5\n",
      "43      6  6.797524                   6\n",
      "44      5  4.639495                   4\n",
      "45      8  7.531132                   7\n",
      "46      5  4.501475                   4\n",
      "47      7  5.387056                   5\n",
      "48      5  4.980341                   4\n",
      "49      5  5.517047                   5\n",
      "50      3  3.237688                   3\n",
      "51      5  6.185699                   6\n",
      "52      8  5.815645                   5\n",
      "53      3  4.257814                   4\n",
      "54      5  6.766539                   6\n",
      "55      6  5.105289                   5\n",
      "56      6  6.285633                   6\n",
      "57      5  3.923472                   3\n",
      "58      6  5.276543                   5\n",
      "59      5  3.374081                   3\n",
      "60     11  3.775372                   3\n",
      "61      3  3.424400                   3\n",
      "62      3  2.317911                   2\n",
      "63      5  4.223365                   4\n",
      "64      3  4.230657                   4\n",
      "65      3  5.028375                   5\n",
      "66      7  5.842651                   5\n",
      "67      3  2.819582                   2\n",
      "68      5  6.774359                   6\n",
      "69      8  7.553539                   7\n",
      "70      8  5.956173                   5\n",
      "71      3  4.747736                   4\n",
      "72      3  3.625552                   3\n",
      "73      7  7.271787                   7\n",
      "74      3  4.445810                   4\n",
      "75      7  5.341599                   5\n",
      "76      3  4.039141                   4\n",
      "77      3  3.769292                   3\n",
      "78      3  4.527602                   4\n",
      "79      3  3.601106                   3\n",
      "80      5  4.122932                   4\n",
      "81      8  4.691832                   4\n",
      "82      8  8.264181                   8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 18  5  0\n",
      "         2  4  5  6  4\n",
      "         3  0  1  1  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5366         \n",
      "                 95% CI : (0.423, 0.6475)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.3706         \n",
      "                                         \n",
      "                  Kappa : 0.1537         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048  0.20833  0.08333  0.00000\n",
      "Specificity            0.4250  0.75862  0.98571  1.00000\n",
      "Pos Pred Value         0.6230  0.26316  0.50000      NaN\n",
      "Neg Pred Value         0.8095  0.69841  0.86250  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4634  0.06098  0.01220  0.00000\n",
      "Detection Prevalence   0.7439  0.23171  0.02439  0.00000\n",
      "Balanced Accuracy      0.6649  0.48348  0.53452  0.50000\n",
      "Fold 3 Accuracy: 0.536585365853659 \n",
      "Fold 3 AUC: 0.744254298941799 \n",
      "Fold 3 Overall Sensitivity: 0.299107142857143 \n",
      "Fold 3 Overall Specificity: 0.792333743842365 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.474426778967444 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  3.802240                   3\n",
      "2       3  6.149721                   6\n",
      "3       3  3.626635                   3\n",
      "4       3  5.420969                   5\n",
      "5       3  4.281910                   4\n",
      "6       7  7.614967                   7\n",
      "7       5  3.684494                   3\n",
      "8       3  4.128988                   4\n",
      "9       3  4.261791                   4\n",
      "10      3  4.788842                   4\n",
      "11      4  3.942318                   3\n",
      "12      3  4.188947                   4\n",
      "13      3  4.978270                   4\n",
      "14      3  2.909207                   2\n",
      "15      8  4.346301                   4\n",
      "16      3  3.788574                   3\n",
      "17      3  6.370447                   6\n",
      "18      5  5.619471                   5\n",
      "19      3  4.137069                   4\n",
      "20      7  5.479012                   5\n",
      "21      3  4.054833                   4\n",
      "22      3  3.072800                   3\n",
      "23      6  4.513350                   4\n",
      "24      8  5.253856                   5\n",
      "25      3  4.011727                   4\n",
      "26     10  5.623792                   5\n",
      "27      3  3.654846                   3\n",
      "28      3  4.198568                   4\n",
      "29      3  4.371117                   4\n",
      "30      3  2.951514                   2\n",
      "31      3  4.230382                   4\n",
      "32      6  3.945386                   3\n",
      "33      6  5.072469                   5\n",
      "34      7  5.268080                   5\n",
      "35      5  7.112339                   7\n",
      "36      3  3.539060                   3\n",
      "37      5  4.526984                   4\n",
      "38      5  4.662478                   4\n",
      "39      3  3.482011                   3\n",
      "40      6  3.319496                   3\n",
      "41      3  4.352543                   4\n",
      "42      8  3.678690                   3\n",
      "43      5  4.553774                   4\n",
      "44      5  4.489386                   4\n",
      "45     12  5.218443                   5\n",
      "46      5  4.888901                   4\n",
      "47      6  4.108590                   4\n",
      "48      5  4.939876                   4\n",
      "49      6  5.987520                   5\n",
      "50      3  3.142911                   3\n",
      "51      5  4.258797                   4\n",
      "52      5  4.228946                   4\n",
      "53      3  4.226581                   4\n",
      "54      7  4.942919                   4\n",
      "55      8  4.938313                   4\n",
      "56      3  3.581967                   3\n",
      "57      3  4.186224                   4\n",
      "58      5  6.272300                   6\n",
      "59      7  6.268826                   6\n",
      "60     10  6.785799                   6\n",
      "61      5  4.669605                   4\n",
      "62      3  4.750437                   4\n",
      "63      3  2.860410                   2\n",
      "64      5  4.548833                   4\n",
      "65      5  4.089924                   4\n",
      "66      3  3.190180                   3\n",
      "67      5  5.195082                   5\n",
      "68      3  5.296770                   5\n",
      "69      8  5.907770                   5\n",
      "70      3  3.305408                   3\n",
      "71      9  5.455228                   5\n",
      "72      3  3.730409                   3\n",
      "73      3  3.873537                   3\n",
      "74      3  3.593589                   3\n",
      "75      3  4.280584                   4\n",
      "76      3  3.307308                   3\n",
      "77      3  3.985197                   3\n",
      "78     10  5.167257                   5\n",
      "79      8  3.481889                   3\n",
      "80      3  3.671298                   3\n",
      "81      5  3.855379                   3\n",
      "82      3  3.311525                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35 10  2  0\n",
      "         2  7 10  5  1\n",
      "         3  0  3  5  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6173          \n",
      "                 95% CI : (0.5026, 0.7231)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.04712         \n",
      "                                          \n",
      "                  Kappa : 0.3604          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.4348  0.41667  0.00000\n",
      "Specificity            0.6923   0.7759  0.91304  1.00000\n",
      "Pos Pred Value         0.7447   0.4348  0.45455      NaN\n",
      "Neg Pred Value         0.7941   0.7759  0.90000  0.95062\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4321   0.1235  0.06173  0.00000\n",
      "Detection Prevalence   0.5802   0.2840  0.13580  0.00000\n",
      "Balanced Accuracy      0.7628   0.6053  0.66486  0.50000\n",
      "Fold 4 Accuracy: 0.617283950617284 \n",
      "Fold 4 AUC: 0.801076892109501 \n",
      "Fold 4 Overall Sensitivity: 0.421195652173913 \n",
      "Fold 4 Overall Specificity: 0.84530330988352 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.733314945521598 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.226711                   4\n",
      "2       3  4.252511                   4\n",
      "3       3  2.673361                   2\n",
      "4       4  6.432249                   6\n",
      "5       3  4.443312                   4\n",
      "6       6  5.869793                   5\n",
      "7       3  3.921247                   3\n",
      "8       4  6.352297                   6\n",
      "9       3  4.849033                   4\n",
      "10      3  4.804680                   4\n",
      "11      3  4.020830                   4\n",
      "12      6  2.067969                   2\n",
      "13      3  3.233840                   3\n",
      "14      3  2.455722                   2\n",
      "15      6  6.135239                   6\n",
      "16      3  4.630342                   4\n",
      "17      5  7.829142                   7\n",
      "18      3  3.971126                   3\n",
      "19      3  2.888593                   2\n",
      "20      9  8.338334                   8\n",
      "21      3  4.283440                   4\n",
      "22      3  4.057574                   4\n",
      "23      3  4.082502                   4\n",
      "24      3  2.937707                   2\n",
      "25      5  5.001245                   5\n",
      "26      3  4.370119                   4\n",
      "27      3  5.371262                   5\n",
      "28      5  4.599573                   4\n",
      "29      5  3.834217                   3\n",
      "30      3  4.405794                   4\n",
      "31      3  5.355742                   5\n",
      "32      3  2.212429                   2\n",
      "33      8  7.397674                   7\n",
      "34      3  3.961310                   3\n",
      "35      3  3.458215                   3\n",
      "36      3  4.674798                   4\n",
      "37      3  3.503863                   3\n",
      "38      3  3.642919                   3\n",
      "39      5  5.359218                   5\n",
      "40      5  5.552827                   5\n",
      "41      6  5.998956                   5\n",
      "42      8  4.589609                   4\n",
      "43      9  5.480498                   5\n",
      "44      6  5.087243                   5\n",
      "45      9  6.949555                   6\n",
      "46      8  5.440156                   5\n",
      "47      3  3.239106                   3\n",
      "48      9  6.196118                   6\n",
      "49      7  7.051044                   7\n",
      "50     11  8.162805                   8\n",
      "51      8  6.215795                   6\n",
      "52      6  7.252252                   7\n",
      "53      5  4.003951                   4\n",
      "54      5  5.411156                   5\n",
      "55      3  4.167702                   4\n",
      "56      3  3.253635                   3\n",
      "57      3  4.634778                   4\n",
      "58      5  5.289705                   5\n",
      "59     12  9.916499                   9\n",
      "60      5  4.318791                   4\n",
      "61      5  7.911314                   7\n",
      "62      5  4.227174                   4\n",
      "63     10  6.935460                   6\n",
      "64      3  3.537033                   3\n",
      "65      5  2.399933                   2\n",
      "66      5  4.138707                   4\n",
      "67     11  8.875423                   8\n",
      "68      5  4.948093                   4\n",
      "69      3  4.062590                   4\n",
      "70      3  5.414308                   5\n",
      "71      3  3.410218                   3\n",
      "72      3  5.960984                   5\n",
      "73      3  5.448450                   5\n",
      "74      3  4.301969                   4\n",
      "75      6  4.350136                   4\n",
      "76      3  3.673006                   3\n",
      "77      7  4.834607                   4\n",
      "78      9  7.903439                   7\n",
      "79      9  7.791090                   7\n",
      "80      6  6.422231                   6\n",
      "81      3  4.120080                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37  8  1  1\n",
      "         2  3 10  7  0\n",
      "         3  2  4  3  1\n",
      "         4  0  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6375          \n",
      "                 95% CI : (0.5224, 0.7421)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.02785         \n",
      "                                          \n",
      "                  Kappa : 0.3954          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.49066         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810   0.4348   0.2500   0.3333\n",
      "Specificity            0.7368   0.8246   0.8971   0.9740\n",
      "Pos Pred Value         0.7872   0.5000   0.3000   0.3333\n",
      "Neg Pred Value         0.8485   0.7833   0.8714   0.9740\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4625   0.1250   0.0375   0.0125\n",
      "Detection Prevalence   0.5875   0.2500   0.1250   0.0375\n",
      "Balanced Accuracy      0.8089   0.6297   0.5735   0.6537\n",
      "Fold 5 Accuracy: 0.6375 \n",
      "Fold 5 AUC: 0.73060443984357 \n",
      "Fold 5 Overall Sensitivity: 0.474767080745342 \n",
      "Fold 5 Overall Specificity: 0.858122076581829 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.611802092329494 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.745888                   4\n",
      "2       3  4.449810                   4\n",
      "3       3  4.643437                   4\n",
      "4       3  5.348280                   5\n",
      "5       3  9.688750                   9\n",
      "6       3  4.815954                   4\n",
      "7       5  2.286330                   2\n",
      "8       5  6.385020                   6\n",
      "9       4  4.149598                   4\n",
      "10      3  4.293013                   4\n",
      "11      7  8.044916                   8\n",
      "12      3  2.950112                   2\n",
      "13      7  6.529831                   6\n",
      "14      3  2.593404                   2\n",
      "15      3  3.308544                   3\n",
      "16      8  5.841033                   5\n",
      "17      3  3.646305                   3\n",
      "18      5  3.580804                   3\n",
      "19      3  3.346572                   3\n",
      "20      3  4.933884                   4\n",
      "21      3  4.445883                   4\n",
      "22      3  4.741313                   4\n",
      "23      5  5.551264                   5\n",
      "24      3  3.563183                   3\n",
      "25      6 10.047535                  10\n",
      "26      6  7.979218                   7\n",
      "27      5  5.282655                   5\n",
      "28      5  5.280417                   5\n",
      "29      5  5.490699                   5\n",
      "30      3  3.700337                   3\n",
      "31      3  4.480859                   4\n",
      "32      4  4.820043                   4\n",
      "33      7  5.975256                   5\n",
      "34      3  3.173454                   3\n",
      "35      3  1.202823                   1\n",
      "36      5  4.715142                   4\n",
      "37      5  5.481523                   5\n",
      "38      7  8.055081                   8\n",
      "39     10 10.691624                  10\n",
      "40     10  8.095486                   8\n",
      "41      3  4.469098                   4\n",
      "42      3  3.967417                   3\n",
      "43      8  5.485962                   5\n",
      "44      3  3.314320                   3\n",
      "45      7  5.544986                   5\n",
      "46      3  2.259206                   2\n",
      "47      7  4.763017                   4\n",
      "48      6  6.481135                   6\n",
      "49     11  4.816347                   4\n",
      "50      3  2.868120                   2\n",
      "51      6  7.840812                   7\n",
      "52      6  5.992818                   5\n",
      "53      3  4.751735                   4\n",
      "54      3  4.325908                   4\n",
      "55      9 10.120458                  10\n",
      "56      3  4.175242                   4\n",
      "57      6  4.099561                   4\n",
      "58      3  5.076561                   5\n",
      "59      5  4.528436                   4\n",
      "60      5  6.737850                   6\n",
      "61      3  1.199187                   1\n",
      "62      3  2.937767                   2\n",
      "63      3  2.890250                   2\n",
      "64      9  5.754594                   5\n",
      "65      6  5.567558                   5\n",
      "66      5  3.588589                   3\n",
      "67      7  9.478968                   9\n",
      "68      3  3.172780                   3\n",
      "69      4  7.477733                   7\n",
      "70      3  2.870668                   2\n",
      "71      5  4.867698                   4\n",
      "72      6  7.131563                   7\n",
      "73      8  6.617221                   6\n",
      "74      6  4.304061                   4\n",
      "75      3  4.346321                   4\n",
      "76      3  4.582041                   4\n",
      "77      3  4.141873                   4\n",
      "78      3  5.483430                   5\n",
      "79      5  8.498734                   8\n",
      "80      3  3.248318                   3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 181  56  12   3\n",
      "         2  26  50  29   7\n",
      "         3   3  11  19   7\n",
      "         4   0   1   1   1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6167          \n",
      "                 95% CI : (0.5675, 0.6642)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 2.696e-05       \n",
      "                                          \n",
      "                  Kappa : 0.3456          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 2.228e-06       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8619   0.4237  0.31148 0.055556\n",
      "Specificity            0.6396   0.7855  0.93931 0.994859\n",
      "Pos Pred Value         0.7183   0.4464  0.47500 0.333333\n",
      "Neg Pred Value         0.8129   0.7695  0.88556 0.957921\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4447   0.1229  0.04668 0.002457\n",
      "Detection Prevalence   0.6192   0.2752  0.09828 0.007371\n",
      "Balanced Accuracy      0.7507   0.6046  0.62539 0.525207\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6168104 \n",
      "Average AUC: 0.7334368 \n",
      "Average Sensitivity: 0.4156852 \n",
      "Average Specificity: 0.840069 \n",
      "Average Pearson Correlation: 0.6142335 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 15)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Truncate the predicted values before classification\n",
    "    truncated_predictions <- floor(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(truncated_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Truncated_Predicted = truncated_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a71f01",
   "metadata": {},
   "source": [
    "# 10 Markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331856e6",
   "metadata": {},
   "source": [
    "# 10 Markers No Rounding, No Truncating, 0.5 Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e977cf09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36 10  2  0\n",
      "         2  4 13  7  2\n",
      "         3  2  1  3  1\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6463         \n",
      "                 95% CI : (0.533, 0.7488)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.009787       \n",
      "                                         \n",
      "                  Kappa : 0.4049         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.5417  0.25000  0.25000\n",
      "Specificity            0.7000   0.7759  0.94286  1.00000\n",
      "Pos Pred Value         0.7500   0.5000  0.42857  1.00000\n",
      "Neg Pred Value         0.8235   0.8036  0.88000  0.96296\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4390   0.1585  0.03659  0.01220\n",
      "Detection Prevalence   0.5854   0.3171  0.08537  0.01220\n",
      "Balanced Accuracy      0.7786   0.6588  0.59643  0.62500\n",
      "Fold 1 Accuracy: 0.646341463414634 \n",
      "Fold 1 AUC: 0.786003637566138 \n",
      "Fold 1 Overall Sensitivity: 0.474702380952381 \n",
      "Fold 1 Overall Specificity: 0.854679802955665 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.642560532657274 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  6.000271                 6\n",
      "2       3  4.276715                 4\n",
      "3       3  3.165433                 3\n",
      "4       5  3.632901                 4\n",
      "5       5  4.306226                 4\n",
      "6       4  6.784389                 7\n",
      "7       3  4.405510                 4\n",
      "8       5  3.708987                 4\n",
      "9       5  3.558270                 4\n",
      "10      3  3.039578                 3\n",
      "11      3  4.536477                 5\n",
      "12      3  4.076198                 4\n",
      "13      5  4.738698                 5\n",
      "14     10  9.727961                10\n",
      "15      3  3.665806                 4\n",
      "16      3  3.805039                 4\n",
      "17      6  6.759630                 7\n",
      "18      3  3.792517                 4\n",
      "19      3  3.117593                 3\n",
      "20      3  3.589542                 4\n",
      "21      3  6.556075                 7\n",
      "22      3  3.575071                 4\n",
      "23      3  3.233758                 3\n",
      "24      8  7.922504                 8\n",
      "25      3  4.144749                 4\n",
      "26      3  4.353071                 4\n",
      "27      3  4.834076                 5\n",
      "28      3  4.263536                 4\n",
      "29      3  4.476118                 4\n",
      "30      3  4.340056                 4\n",
      "31      5  3.208875                 3\n",
      "32      3  3.440274                 3\n",
      "33      3  4.038716                 4\n",
      "34      3  3.075827                 3\n",
      "35      5  5.689452                 6\n",
      "36      3  4.285897                 4\n",
      "37      6  5.311399                 5\n",
      "38      6  5.635862                 6\n",
      "39      3  4.397827                 4\n",
      "40      5  4.873764                 5\n",
      "41      5  5.567677                 6\n",
      "42      6  3.748444                 4\n",
      "43      3  4.038751                 4\n",
      "44      7  6.095189                 6\n",
      "45      7  5.502147                 6\n",
      "46      3  4.963228                 5\n",
      "47     11  6.314821                 6\n",
      "48      9  5.265806                 5\n",
      "49      3  4.259518                 4\n",
      "50      3  2.940904                 3\n",
      "51      3  3.616928                 4\n",
      "52      8  8.022732                 8\n",
      "53      6  3.716771                 4\n",
      "54      6  5.775449                 6\n",
      "55      5  4.568975                 5\n",
      "56      5  4.460989                 4\n",
      "57      7  5.721420                 6\n",
      "58      3  3.095336                 3\n",
      "59      3  3.140219                 3\n",
      "60      9  3.964907                 4\n",
      "61      9  4.813841                 5\n",
      "62      9  6.103746                 6\n",
      "63      3  4.112763                 4\n",
      "64      9  4.351830                 4\n",
      "65     10  4.798092                 5\n",
      "66      6  6.192749                 6\n",
      "67      3  4.137304                 4\n",
      "68      5  3.745403                 4\n",
      "69      5  3.242244                 3\n",
      "70      3  3.029661                 3\n",
      "71      6  4.931571                 5\n",
      "72      5  5.546602                 6\n",
      "73      8  7.415579                 7\n",
      "74      5  4.597232                 5\n",
      "75      3  5.571274                 6\n",
      "76      3  3.506727                 4\n",
      "77      7  5.428374                 5\n",
      "78      3  4.231030                 4\n",
      "79      3  3.975255                 4\n",
      "80     11  7.954550                 8\n",
      "81      3  3.253260                 3\n",
      "82      3  3.707244                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  6  0  1\n",
      "         2 12 13  8  1\n",
      "         3  2  5  5  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.561          \n",
      "                 95% CI : (0.447, 0.6704)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2199         \n",
      "                                         \n",
      "                  Kappa : 0.3085         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 0.2615         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.5417  0.38462  0.00000\n",
      "Specificity            0.8250   0.6379  0.88406  1.00000\n",
      "Pos Pred Value         0.8000   0.3824  0.38462      NaN\n",
      "Neg Pred Value         0.7021   0.7708  0.88406  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3415   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.4268   0.4146  0.15854  0.00000\n",
      "Balanced Accuracy      0.7458   0.5898  0.63434  0.50000\n",
      "Fold 2 Accuracy: 0.560975609756098 \n",
      "Fold 2 AUC: 0.642653642653643 \n",
      "Fold 2 Overall Sensitivity: 0.398237179487179 \n",
      "Fold 2 Overall Specificity: 0.836747251374313 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.519978278837124 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  5.284922                 5\n",
      "2       4  4.286156                 4\n",
      "3       3  4.852323                 5\n",
      "4       3  5.710145                 6\n",
      "5       3  3.501279                 4\n",
      "6       3  2.712148                 3\n",
      "7       5  4.423597                 4\n",
      "8       3  4.839294                 5\n",
      "9       5  9.049261                 9\n",
      "10      4  3.239906                 3\n",
      "11      3  3.387457                 3\n",
      "12      3  7.305009                 7\n",
      "13      3  4.199736                 4\n",
      "14      6  5.307442                 5\n",
      "15     12  6.323556                 6\n",
      "16      6  7.331307                 7\n",
      "17      3  4.163340                 4\n",
      "18      5  5.731551                 6\n",
      "19      3  3.938198                 4\n",
      "20      3  4.452350                 4\n",
      "21      3  3.230036                 3\n",
      "22      3  3.489963                 3\n",
      "23      3  3.611087                 4\n",
      "24      3  2.154693                 2\n",
      "25      3  5.242201                 5\n",
      "26      3  4.692606                 5\n",
      "27      3  3.223911                 3\n",
      "28      3  4.114610                 4\n",
      "29      5  4.601782                 5\n",
      "30      4  8.721995                 9\n",
      "31      3  4.982139                 5\n",
      "32      3  3.348330                 3\n",
      "33      3  4.754811                 5\n",
      "34      3  3.393562                 3\n",
      "35      8  5.516273                 6\n",
      "36      6  6.724571                 7\n",
      "37      5  2.760959                 3\n",
      "38      3  5.421124                 5\n",
      "39      9  7.530312                 8\n",
      "40     12  6.632856                 7\n",
      "41      5  5.745909                 6\n",
      "42      7  5.345267                 5\n",
      "43      6  6.340785                 6\n",
      "44      5  4.624377                 5\n",
      "45      8  7.308579                 7\n",
      "46      5  4.581736                 5\n",
      "47      7  5.270835                 5\n",
      "48      5  4.879031                 5\n",
      "49      5  5.544631                 6\n",
      "50      3  3.258351                 3\n",
      "51      5  6.157307                 6\n",
      "52      8  5.969111                 6\n",
      "53      3  4.267516                 4\n",
      "54      5  6.870498                 7\n",
      "55      6  4.319721                 4\n",
      "56      6  6.137517                 6\n",
      "57      5  3.944161                 4\n",
      "58      6  5.335346                 5\n",
      "59      5  3.488481                 3\n",
      "60     11  3.938040                 4\n",
      "61      3  3.419484                 3\n",
      "62      3  2.216849                 2\n",
      "63      5  4.566916                 5\n",
      "64      3  3.866299                 4\n",
      "65      3  4.785108                 5\n",
      "66      7  5.782145                 6\n",
      "67      3  2.731516                 3\n",
      "68      5  6.987893                 7\n",
      "69      8  8.062822                 8\n",
      "70      8  6.152204                 6\n",
      "71      3  4.572513                 5\n",
      "72      3  3.736294                 4\n",
      "73      7  7.072123                 7\n",
      "74      3  4.359214                 4\n",
      "75      7  5.080892                 5\n",
      "76      3  4.036771                 4\n",
      "77      3  3.727096                 4\n",
      "78      3  4.579071                 5\n",
      "79      3  3.622411                 4\n",
      "80      5  4.030437                 4\n",
      "81      8  4.567784                 5\n",
      "82      8  8.159735                 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35 12  2  0\n",
      "         2  7 11  9  3\n",
      "         3  0  1  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5732          \n",
      "                 95% CI : (0.4591, 0.6818)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.1601          \n",
      "                                          \n",
      "                  Kappa : 0.266           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.4583  0.08333  0.00000\n",
      "Specificity            0.6500   0.6724  0.97143  1.00000\n",
      "Pos Pred Value         0.7143   0.3667  0.33333      NaN\n",
      "Neg Pred Value         0.7879   0.7500  0.86076  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4268   0.1341  0.01220  0.00000\n",
      "Detection Prevalence   0.5976   0.3659  0.03659  0.00000\n",
      "Balanced Accuracy      0.7417   0.5654  0.52738  0.50000\n",
      "Fold 3 Accuracy: 0.573170731707317 \n",
      "Fold 3 AUC: 0.759259259259259 \n",
      "Fold 3 Overall Sensitivity: 0.34375 \n",
      "Fold 3 Overall Specificity: 0.823460591133005 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.475722721998856 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  3.763716                 4\n",
      "2       3  5.800056                 6\n",
      "3       3  3.404017                 3\n",
      "4       3  5.225839                 5\n",
      "5       3  4.116871                 4\n",
      "6       7  7.662614                 8\n",
      "7       5  3.511488                 4\n",
      "8       3  4.262643                 4\n",
      "9       3  4.187860                 4\n",
      "10      3  4.456501                 4\n",
      "11      4  4.168628                 4\n",
      "12      3  4.448710                 4\n",
      "13      3  4.963586                 5\n",
      "14      3  2.582014                 3\n",
      "15      8  4.689790                 5\n",
      "16      3  3.557320                 4\n",
      "17      3  6.366597                 6\n",
      "18      5  5.724535                 6\n",
      "19      3  4.375340                 4\n",
      "20      7  5.182697                 5\n",
      "21      3  3.866220                 4\n",
      "22      3  3.177799                 3\n",
      "23      6  4.714063                 5\n",
      "24      8  5.030026                 5\n",
      "25      3  3.915273                 4\n",
      "26     10  5.775241                 6\n",
      "27      3  3.541053                 4\n",
      "28      3  4.198225                 4\n",
      "29      3  4.839266                 5\n",
      "30      3  3.126472                 3\n",
      "31      3  4.268918                 4\n",
      "32      6  4.002556                 4\n",
      "33      6  5.130205                 5\n",
      "34      7  5.348803                 5\n",
      "35      5  7.274199                 7\n",
      "36      3  3.472797                 3\n",
      "37      5  4.804682                 5\n",
      "38      5  4.442820                 4\n",
      "39      3  3.084794                 3\n",
      "40      6  3.370279                 3\n",
      "41      3  4.219712                 4\n",
      "42      8  3.617868                 4\n",
      "43      5  4.593028                 5\n",
      "44      5  4.431754                 4\n",
      "45     12  5.217011                 5\n",
      "46      5  5.018267                 5\n",
      "47      6  4.206887                 4\n",
      "48      5  5.290488                 5\n",
      "49      6  5.967424                 6\n",
      "50      3  3.321561                 3\n",
      "51      5  4.375785                 4\n",
      "52      5  4.145724                 4\n",
      "53      3  4.162738                 4\n",
      "54      7  4.942136                 5\n",
      "55      8  4.801872                 5\n",
      "56      3  3.694807                 4\n",
      "57      3  4.279993                 4\n",
      "58      5  6.181805                 6\n",
      "59      7  6.188892                 6\n",
      "60     10  6.829184                 7\n",
      "61      5  4.512866                 5\n",
      "62      3  4.732093                 5\n",
      "63      3  3.091554                 3\n",
      "64      5  4.301774                 4\n",
      "65      5  3.869605                 4\n",
      "66      3  3.363611                 3\n",
      "67      5  4.746593                 5\n",
      "68      3  5.170976                 5\n",
      "69      8  6.062248                 6\n",
      "70      3  3.605913                 4\n",
      "71      9  5.728364                 6\n",
      "72      3  3.741688                 4\n",
      "73      3  4.081262                 4\n",
      "74      3  3.391698                 3\n",
      "75      3  4.234721                 4\n",
      "76      3  2.935033                 3\n",
      "77      3  3.809391                 4\n",
      "78     10  4.885740                 5\n",
      "79      8  3.473616                 3\n",
      "80      3  4.116321                 4\n",
      "81      5  3.286240                 3\n",
      "82      3  3.571029                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  8  1  0\n",
      "         2 10 12  5  0\n",
      "         3  1  3  6  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6173          \n",
      "                 95% CI : (0.5026, 0.7231)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.04712         \n",
      "                                          \n",
      "                  Kappa : 0.3876          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381   0.5217  0.50000  0.25000\n",
      "Specificity            0.7692   0.7414  0.89855  1.00000\n",
      "Pos Pred Value         0.7750   0.4444  0.46154  1.00000\n",
      "Neg Pred Value         0.7317   0.7963  0.91176  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3827   0.1481  0.07407  0.01235\n",
      "Detection Prevalence   0.4938   0.3333  0.16049  0.01235\n",
      "Balanced Accuracy      0.7537   0.6316  0.69928  0.62500\n",
      "Fold 4 Accuracy: 0.617283950617284 \n",
      "Fold 4 AUC: 0.846093570278353 \n",
      "Fold 4 Overall Sensitivity: 0.502458592132505 \n",
      "Fold 4 Overall Specificity: 0.85229020105332 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.767328379209843 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  3.960872                 4\n",
      "2       3  3.865230                 4\n",
      "3       3  2.374564                 2\n",
      "4       4  6.502605                 7\n",
      "5       3  4.440508                 4\n",
      "6       6  5.894686                 6\n",
      "7       3  3.735166                 4\n",
      "8       4  6.343042                 6\n",
      "9       3  4.765550                 5\n",
      "10      3  4.819354                 5\n",
      "11      3  4.004962                 4\n",
      "12      6  2.316336                 2\n",
      "13      3  3.792570                 4\n",
      "14      3  2.306149                 2\n",
      "15      6  6.209162                 6\n",
      "16      3  4.183170                 4\n",
      "17      5  7.724165                 8\n",
      "18      3  3.659344                 4\n",
      "19      3  3.027233                 3\n",
      "20      9  8.191192                 8\n",
      "21      3  4.148547                 4\n",
      "22      3  3.695549                 4\n",
      "23      3  3.869890                 4\n",
      "24      3  3.023371                 3\n",
      "25      5  5.323168                 5\n",
      "26      3  4.162748                 4\n",
      "27      3  5.567448                 6\n",
      "28      5  4.781236                 5\n",
      "29      5  3.983907                 4\n",
      "30      3  4.453094                 4\n",
      "31      3  5.007354                 5\n",
      "32      3  2.430970                 2\n",
      "33      8  7.386844                 7\n",
      "34      3  3.950983                 4\n",
      "35      3  3.600119                 4\n",
      "36      3  4.603948                 5\n",
      "37      3  3.460374                 3\n",
      "38      3  3.596020                 4\n",
      "39      5  5.145048                 5\n",
      "40      5  5.333722                 5\n",
      "41      6  5.297152                 5\n",
      "42      8  4.463996                 4\n",
      "43      9  5.524645                 6\n",
      "44      6  4.873910                 5\n",
      "45      9  7.546466                 8\n",
      "46      8  5.524279                 6\n",
      "47      3  3.522933                 4\n",
      "48      9  5.952977                 6\n",
      "49      7  6.634501                 7\n",
      "50     11  8.620321                 9\n",
      "51      8  5.921157                 6\n",
      "52      6  7.338255                 7\n",
      "53      5  4.507988                 5\n",
      "54      5  5.176543                 5\n",
      "55      3  3.901613                 4\n",
      "56      3  3.466111                 3\n",
      "57      3  4.369114                 4\n",
      "58      5  5.155222                 5\n",
      "59     12  9.791322                10\n",
      "60      5  4.200156                 4\n",
      "61      5  7.697479                 8\n",
      "62      5  3.779623                 4\n",
      "63     10  7.488508                 7\n",
      "64      3  3.815459                 4\n",
      "65      5  3.177237                 3\n",
      "66      5  4.092686                 4\n",
      "67     11  8.877583                 9\n",
      "68      5  4.473813                 4\n",
      "69      3  4.808456                 5\n",
      "70      3  4.600995                 5\n",
      "71      3  3.540310                 4\n",
      "72      3  5.672097                 6\n",
      "73      3  5.065983                 5\n",
      "74      3  3.849248                 4\n",
      "75      6  3.991025                 4\n",
      "76      3  3.645837                 4\n",
      "77      7  5.273054                 5\n",
      "78      9  7.982763                 8\n",
      "79      9  7.655437                 8\n",
      "80      6  6.042645                 6\n",
      "81      3  4.231717                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 27  5  0  0\n",
      "         2 13 12  7  1\n",
      "         3  1  5  4  1\n",
      "         4  1  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.55            \n",
      "                 95% CI : (0.4347, 0.6615)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.3692          \n",
      "                                          \n",
      "                  Kappa : 0.3065          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.4358          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6429   0.5217   0.3333   0.3333\n",
      "Specificity            0.8684   0.6316   0.8971   0.9610\n",
      "Pos Pred Value         0.8438   0.3636   0.3636   0.2500\n",
      "Neg Pred Value         0.6875   0.7660   0.8841   0.9737\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.3375   0.1500   0.0500   0.0125\n",
      "Detection Prevalence   0.4000   0.4125   0.1375   0.0500\n",
      "Balanced Accuracy      0.7556   0.5767   0.6152   0.6472\n",
      "Fold 5 Accuracy: 0.55 \n",
      "Fold 5 AUC: 0.767037612146308 \n",
      "Fold 5 Overall Sensitivity: 0.457815734989648 \n",
      "Fold 5 Overall Specificity: 0.839524446142093 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.604251101522965 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.755234                 5\n",
      "2       3  4.434627                 4\n",
      "3       3  4.852271                 5\n",
      "4       3  5.237284                 5\n",
      "5       3  9.835264                10\n",
      "6       3  4.859699                 5\n",
      "7       5  2.276183                 2\n",
      "8       5  6.268452                 6\n",
      "9       4  4.349702                 4\n",
      "10      3  4.231964                 4\n",
      "11      7  7.995546                 8\n",
      "12      3  2.959054                 3\n",
      "13      7  6.441749                 6\n",
      "14      3  2.549196                 3\n",
      "15      3  3.275475                 3\n",
      "16      8  5.793829                 6\n",
      "17      3  3.450830                 3\n",
      "18      5  3.595025                 4\n",
      "19      3  3.410211                 3\n",
      "20      3  4.952495                 5\n",
      "21      3  4.308418                 4\n",
      "22      3  4.782653                 5\n",
      "23      5  5.542946                 6\n",
      "24      3  3.567875                 4\n",
      "25      6 10.026906                10\n",
      "26      6  7.971283                 8\n",
      "27      5  5.315687                 5\n",
      "28      5  5.149595                 5\n",
      "29      5  5.317281                 5\n",
      "30      3  3.625063                 4\n",
      "31      3  4.553539                 5\n",
      "32      4  4.775998                 5\n",
      "33      7  5.878331                 6\n",
      "34      3  3.226619                 3\n",
      "35      3  1.260656                 1\n",
      "36      5  4.670038                 5\n",
      "37      5  5.429413                 5\n",
      "38      7  7.949066                 8\n",
      "39     10 10.700064                11\n",
      "40     10  7.983047                 8\n",
      "41      3  4.400421                 4\n",
      "42      3  4.048179                 4\n",
      "43      8  5.524548                 6\n",
      "44      3  3.347592                 3\n",
      "45      7  5.544158                 6\n",
      "46      3  2.653283                 3\n",
      "47      7  4.662219                 5\n",
      "48      6  6.407438                 6\n",
      "49     11  4.788890                 5\n",
      "50      3  2.736541                 3\n",
      "51      6  7.846575                 8\n",
      "52      6  5.942769                 6\n",
      "53      3  4.784625                 5\n",
      "54      3  4.345482                 4\n",
      "55      9 10.140466                10\n",
      "56      3  4.101180                 4\n",
      "57      6  4.088753                 4\n",
      "58      3  5.058014                 5\n",
      "59      5  4.510468                 5\n",
      "60      5  6.864259                 7\n",
      "61      3  1.116059                 1\n",
      "62      3  3.027323                 3\n",
      "63      3  2.813000                 3\n",
      "64      9  5.779817                 6\n",
      "65      6  5.478710                 5\n",
      "66      5  3.596844                 4\n",
      "67      7  9.485630                 9\n",
      "68      3  3.148966                 3\n",
      "69      4  7.549323                 8\n",
      "70      3  2.869710                 3\n",
      "71      5  4.818458                 5\n",
      "72      6  6.988452                 7\n",
      "73      8  6.623890                 7\n",
      "74      6  4.283172                 4\n",
      "75      3  4.715940                 5\n",
      "76      3  4.643290                 5\n",
      "77      3  4.034301                 4\n",
      "78      3  5.495984                 5\n",
      "79      5  8.382687                 8\n",
      "80      3  3.192003                 3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 157  41   5   1\n",
      "         2  46  61  36   7\n",
      "         3   6  15  19   7\n",
      "         4   1   1   1   3\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5897          \n",
      "                 95% CI : (0.5402, 0.6379)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.001670        \n",
      "                                          \n",
      "                  Kappa : 0.3345          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.006169        \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7476   0.5169  0.31148 0.166667\n",
      "Specificity            0.7614   0.6920  0.91908 0.992288\n",
      "Pos Pred Value         0.7696   0.4067  0.40426 0.500000\n",
      "Neg Pred Value         0.7389   0.7782  0.88333 0.962594\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.3857   0.1499  0.04668 0.007371\n",
      "Detection Prevalence   0.5012   0.3686  0.11548 0.014742\n",
      "Balanced Accuracy      0.7545   0.6045  0.61528 0.579477\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5895544 \n",
      "Average AUC: 0.7602095 \n",
      "Average Sensitivity: 0.4353928 \n",
      "Average Specificity: 0.8413405 \n",
      "Average Pearson Correlation: 0.6019682 \n",
      "[1] \"\"\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 4.5) {\n",
    "    return(1)\n",
    "  } else if (value >= 4.5 & value < 6.5) {\n",
    "    return(2)\n",
    "  } else if (value >= 6.5 & value < 9.5) {\n",
    "    return(3)\n",
    "  } else if (value >= 9.5 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 10)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5da3c6",
   "metadata": {},
   "source": [
    "# 10 Markers Rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38f35aa6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36 10  2  0\n",
      "         2  4 13  7  2\n",
      "         3  2  1  3  1\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6463         \n",
      "                 95% CI : (0.533, 0.7488)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.009787       \n",
      "                                         \n",
      "                  Kappa : 0.4049         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.5417  0.25000  0.25000\n",
      "Specificity            0.7000   0.7759  0.94286  1.00000\n",
      "Pos Pred Value         0.7500   0.5000  0.42857  1.00000\n",
      "Neg Pred Value         0.8235   0.8036  0.88000  0.96296\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4390   0.1585  0.03659  0.01220\n",
      "Detection Prevalence   0.5854   0.3171  0.08537  0.01220\n",
      "Balanced Accuracy      0.7786   0.6588  0.59643  0.62500\n",
      "Fold 1 Accuracy: 0.646341463414634 \n",
      "Fold 1 AUC: 0.786003637566138 \n",
      "Fold 1 Overall Sensitivity: 0.474702380952381 \n",
      "Fold 1 Overall Specificity: 0.854679802955665 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.642560532657274 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  6.000271                 6\n",
      "2       3  4.276715                 4\n",
      "3       3  3.165433                 3\n",
      "4       5  3.632901                 4\n",
      "5       5  4.306226                 4\n",
      "6       4  6.784389                 7\n",
      "7       3  4.405510                 4\n",
      "8       5  3.708987                 4\n",
      "9       5  3.558270                 4\n",
      "10      3  3.039578                 3\n",
      "11      3  4.536477                 5\n",
      "12      3  4.076198                 4\n",
      "13      5  4.738698                 5\n",
      "14     10  9.727961                10\n",
      "15      3  3.665806                 4\n",
      "16      3  3.805039                 4\n",
      "17      6  6.759630                 7\n",
      "18      3  3.792517                 4\n",
      "19      3  3.117593                 3\n",
      "20      3  3.589542                 4\n",
      "21      3  6.556075                 7\n",
      "22      3  3.575071                 4\n",
      "23      3  3.233758                 3\n",
      "24      8  7.922504                 8\n",
      "25      3  4.144749                 4\n",
      "26      3  4.353071                 4\n",
      "27      3  4.834076                 5\n",
      "28      3  4.263536                 4\n",
      "29      3  4.476118                 4\n",
      "30      3  4.340056                 4\n",
      "31      5  3.208875                 3\n",
      "32      3  3.440274                 3\n",
      "33      3  4.038716                 4\n",
      "34      3  3.075827                 3\n",
      "35      5  5.689452                 6\n",
      "36      3  4.285897                 4\n",
      "37      6  5.311399                 5\n",
      "38      6  5.635862                 6\n",
      "39      3  4.397827                 4\n",
      "40      5  4.873764                 5\n",
      "41      5  5.567677                 6\n",
      "42      6  3.748444                 4\n",
      "43      3  4.038751                 4\n",
      "44      7  6.095189                 6\n",
      "45      7  5.502147                 6\n",
      "46      3  4.963228                 5\n",
      "47     11  6.314821                 6\n",
      "48      9  5.265806                 5\n",
      "49      3  4.259518                 4\n",
      "50      3  2.940904                 3\n",
      "51      3  3.616928                 4\n",
      "52      8  8.022732                 8\n",
      "53      6  3.716771                 4\n",
      "54      6  5.775449                 6\n",
      "55      5  4.568975                 5\n",
      "56      5  4.460989                 4\n",
      "57      7  5.721420                 6\n",
      "58      3  3.095336                 3\n",
      "59      3  3.140219                 3\n",
      "60      9  3.964907                 4\n",
      "61      9  4.813841                 5\n",
      "62      9  6.103746                 6\n",
      "63      3  4.112763                 4\n",
      "64      9  4.351830                 4\n",
      "65     10  4.798092                 5\n",
      "66      6  6.192749                 6\n",
      "67      3  4.137304                 4\n",
      "68      5  3.745403                 4\n",
      "69      5  3.242244                 3\n",
      "70      3  3.029661                 3\n",
      "71      6  4.931571                 5\n",
      "72      5  5.546602                 6\n",
      "73      8  7.415579                 7\n",
      "74      5  4.597232                 5\n",
      "75      3  5.571274                 6\n",
      "76      3  3.506727                 4\n",
      "77      7  5.428374                 5\n",
      "78      3  4.231030                 4\n",
      "79      3  3.975255                 4\n",
      "80     11  7.954550                 8\n",
      "81      3  3.253260                 3\n",
      "82      3  3.707244                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  6  0  1\n",
      "         2 12 13  8  1\n",
      "         3  2  5  5  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.561          \n",
      "                 95% CI : (0.447, 0.6704)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2199         \n",
      "                                         \n",
      "                  Kappa : 0.3085         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 0.2615         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.5417  0.38462  0.00000\n",
      "Specificity            0.8250   0.6379  0.88406  1.00000\n",
      "Pos Pred Value         0.8000   0.3824  0.38462      NaN\n",
      "Neg Pred Value         0.7021   0.7708  0.88406  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3415   0.1585  0.06098  0.00000\n",
      "Detection Prevalence   0.4268   0.4146  0.15854  0.00000\n",
      "Balanced Accuracy      0.7458   0.5898  0.63434  0.50000\n",
      "Fold 2 Accuracy: 0.560975609756098 \n",
      "Fold 2 AUC: 0.642653642653643 \n",
      "Fold 2 Overall Sensitivity: 0.398237179487179 \n",
      "Fold 2 Overall Specificity: 0.836747251374313 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.519978278837124 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  5.284922                 5\n",
      "2       4  4.286156                 4\n",
      "3       3  4.852323                 5\n",
      "4       3  5.710145                 6\n",
      "5       3  3.501279                 4\n",
      "6       3  2.712148                 3\n",
      "7       5  4.423597                 4\n",
      "8       3  4.839294                 5\n",
      "9       5  9.049261                 9\n",
      "10      4  3.239906                 3\n",
      "11      3  3.387457                 3\n",
      "12      3  7.305009                 7\n",
      "13      3  4.199736                 4\n",
      "14      6  5.307442                 5\n",
      "15     12  6.323556                 6\n",
      "16      6  7.331307                 7\n",
      "17      3  4.163340                 4\n",
      "18      5  5.731551                 6\n",
      "19      3  3.938198                 4\n",
      "20      3  4.452350                 4\n",
      "21      3  3.230036                 3\n",
      "22      3  3.489963                 3\n",
      "23      3  3.611087                 4\n",
      "24      3  2.154693                 2\n",
      "25      3  5.242201                 5\n",
      "26      3  4.692606                 5\n",
      "27      3  3.223911                 3\n",
      "28      3  4.114610                 4\n",
      "29      5  4.601782                 5\n",
      "30      4  8.721995                 9\n",
      "31      3  4.982139                 5\n",
      "32      3  3.348330                 3\n",
      "33      3  4.754811                 5\n",
      "34      3  3.393562                 3\n",
      "35      8  5.516273                 6\n",
      "36      6  6.724571                 7\n",
      "37      5  2.760959                 3\n",
      "38      3  5.421124                 5\n",
      "39      9  7.530312                 8\n",
      "40     12  6.632856                 7\n",
      "41      5  5.745909                 6\n",
      "42      7  5.345267                 5\n",
      "43      6  6.340785                 6\n",
      "44      5  4.624377                 5\n",
      "45      8  7.308579                 7\n",
      "46      5  4.581736                 5\n",
      "47      7  5.270835                 5\n",
      "48      5  4.879031                 5\n",
      "49      5  5.544631                 6\n",
      "50      3  3.258351                 3\n",
      "51      5  6.157307                 6\n",
      "52      8  5.969111                 6\n",
      "53      3  4.267516                 4\n",
      "54      5  6.870498                 7\n",
      "55      6  4.319721                 4\n",
      "56      6  6.137517                 6\n",
      "57      5  3.944161                 4\n",
      "58      6  5.335346                 5\n",
      "59      5  3.488481                 3\n",
      "60     11  3.938040                 4\n",
      "61      3  3.419484                 3\n",
      "62      3  2.216849                 2\n",
      "63      5  4.566916                 5\n",
      "64      3  3.866299                 4\n",
      "65      3  4.785108                 5\n",
      "66      7  5.782145                 6\n",
      "67      3  2.731516                 3\n",
      "68      5  6.987893                 7\n",
      "69      8  8.062822                 8\n",
      "70      8  6.152204                 6\n",
      "71      3  4.572513                 5\n",
      "72      3  3.736294                 4\n",
      "73      7  7.072123                 7\n",
      "74      3  4.359214                 4\n",
      "75      7  5.080892                 5\n",
      "76      3  4.036771                 4\n",
      "77      3  3.727096                 4\n",
      "78      3  4.579071                 5\n",
      "79      3  3.622411                 4\n",
      "80      5  4.030437                 4\n",
      "81      8  4.567784                 5\n",
      "82      8  8.159735                 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35 12  2  0\n",
      "         2  7 11  9  3\n",
      "         3  0  1  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5732          \n",
      "                 95% CI : (0.4591, 0.6818)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.1601          \n",
      "                                          \n",
      "                  Kappa : 0.266           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.4583  0.08333  0.00000\n",
      "Specificity            0.6500   0.6724  0.97143  1.00000\n",
      "Pos Pred Value         0.7143   0.3667  0.33333      NaN\n",
      "Neg Pred Value         0.7879   0.7500  0.86076  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4268   0.1341  0.01220  0.00000\n",
      "Detection Prevalence   0.5976   0.3659  0.03659  0.00000\n",
      "Balanced Accuracy      0.7417   0.5654  0.52738  0.50000\n",
      "Fold 3 Accuracy: 0.573170731707317 \n",
      "Fold 3 AUC: 0.759259259259259 \n",
      "Fold 3 Overall Sensitivity: 0.34375 \n",
      "Fold 3 Overall Specificity: 0.823460591133005 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.475722721998856 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  3.763716                 4\n",
      "2       3  5.800056                 6\n",
      "3       3  3.404017                 3\n",
      "4       3  5.225839                 5\n",
      "5       3  4.116871                 4\n",
      "6       7  7.662614                 8\n",
      "7       5  3.511488                 4\n",
      "8       3  4.262643                 4\n",
      "9       3  4.187860                 4\n",
      "10      3  4.456501                 4\n",
      "11      4  4.168628                 4\n",
      "12      3  4.448710                 4\n",
      "13      3  4.963586                 5\n",
      "14      3  2.582014                 3\n",
      "15      8  4.689790                 5\n",
      "16      3  3.557320                 4\n",
      "17      3  6.366597                 6\n",
      "18      5  5.724535                 6\n",
      "19      3  4.375340                 4\n",
      "20      7  5.182697                 5\n",
      "21      3  3.866220                 4\n",
      "22      3  3.177799                 3\n",
      "23      6  4.714063                 5\n",
      "24      8  5.030026                 5\n",
      "25      3  3.915273                 4\n",
      "26     10  5.775241                 6\n",
      "27      3  3.541053                 4\n",
      "28      3  4.198225                 4\n",
      "29      3  4.839266                 5\n",
      "30      3  3.126472                 3\n",
      "31      3  4.268918                 4\n",
      "32      6  4.002556                 4\n",
      "33      6  5.130205                 5\n",
      "34      7  5.348803                 5\n",
      "35      5  7.274199                 7\n",
      "36      3  3.472797                 3\n",
      "37      5  4.804682                 5\n",
      "38      5  4.442820                 4\n",
      "39      3  3.084794                 3\n",
      "40      6  3.370279                 3\n",
      "41      3  4.219712                 4\n",
      "42      8  3.617868                 4\n",
      "43      5  4.593028                 5\n",
      "44      5  4.431754                 4\n",
      "45     12  5.217011                 5\n",
      "46      5  5.018267                 5\n",
      "47      6  4.206887                 4\n",
      "48      5  5.290488                 5\n",
      "49      6  5.967424                 6\n",
      "50      3  3.321561                 3\n",
      "51      5  4.375785                 4\n",
      "52      5  4.145724                 4\n",
      "53      3  4.162738                 4\n",
      "54      7  4.942136                 5\n",
      "55      8  4.801872                 5\n",
      "56      3  3.694807                 4\n",
      "57      3  4.279993                 4\n",
      "58      5  6.181805                 6\n",
      "59      7  6.188892                 6\n",
      "60     10  6.829184                 7\n",
      "61      5  4.512866                 5\n",
      "62      3  4.732093                 5\n",
      "63      3  3.091554                 3\n",
      "64      5  4.301774                 4\n",
      "65      5  3.869605                 4\n",
      "66      3  3.363611                 3\n",
      "67      5  4.746593                 5\n",
      "68      3  5.170976                 5\n",
      "69      8  6.062248                 6\n",
      "70      3  3.605913                 4\n",
      "71      9  5.728364                 6\n",
      "72      3  3.741688                 4\n",
      "73      3  4.081262                 4\n",
      "74      3  3.391698                 3\n",
      "75      3  4.234721                 4\n",
      "76      3  2.935033                 3\n",
      "77      3  3.809391                 4\n",
      "78     10  4.885740                 5\n",
      "79      8  3.473616                 3\n",
      "80      3  4.116321                 4\n",
      "81      5  3.286240                 3\n",
      "82      3  3.571029                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  8  1  0\n",
      "         2 10 12  5  0\n",
      "         3  1  3  6  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6173          \n",
      "                 95% CI : (0.5026, 0.7231)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.04712         \n",
      "                                          \n",
      "                  Kappa : 0.3876          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381   0.5217  0.50000  0.25000\n",
      "Specificity            0.7692   0.7414  0.89855  1.00000\n",
      "Pos Pred Value         0.7750   0.4444  0.46154  1.00000\n",
      "Neg Pred Value         0.7317   0.7963  0.91176  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3827   0.1481  0.07407  0.01235\n",
      "Detection Prevalence   0.4938   0.3333  0.16049  0.01235\n",
      "Balanced Accuracy      0.7537   0.6316  0.69928  0.62500\n",
      "Fold 4 Accuracy: 0.617283950617284 \n",
      "Fold 4 AUC: 0.846093570278353 \n",
      "Fold 4 Overall Sensitivity: 0.502458592132505 \n",
      "Fold 4 Overall Specificity: 0.85229020105332 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.767328379209843 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  3.960872                 4\n",
      "2       3  3.865230                 4\n",
      "3       3  2.374564                 2\n",
      "4       4  6.502605                 7\n",
      "5       3  4.440508                 4\n",
      "6       6  5.894686                 6\n",
      "7       3  3.735166                 4\n",
      "8       4  6.343042                 6\n",
      "9       3  4.765550                 5\n",
      "10      3  4.819354                 5\n",
      "11      3  4.004962                 4\n",
      "12      6  2.316336                 2\n",
      "13      3  3.792570                 4\n",
      "14      3  2.306149                 2\n",
      "15      6  6.209162                 6\n",
      "16      3  4.183170                 4\n",
      "17      5  7.724165                 8\n",
      "18      3  3.659344                 4\n",
      "19      3  3.027233                 3\n",
      "20      9  8.191192                 8\n",
      "21      3  4.148547                 4\n",
      "22      3  3.695549                 4\n",
      "23      3  3.869890                 4\n",
      "24      3  3.023371                 3\n",
      "25      5  5.323168                 5\n",
      "26      3  4.162748                 4\n",
      "27      3  5.567448                 6\n",
      "28      5  4.781236                 5\n",
      "29      5  3.983907                 4\n",
      "30      3  4.453094                 4\n",
      "31      3  5.007354                 5\n",
      "32      3  2.430970                 2\n",
      "33      8  7.386844                 7\n",
      "34      3  3.950983                 4\n",
      "35      3  3.600119                 4\n",
      "36      3  4.603948                 5\n",
      "37      3  3.460374                 3\n",
      "38      3  3.596020                 4\n",
      "39      5  5.145048                 5\n",
      "40      5  5.333722                 5\n",
      "41      6  5.297152                 5\n",
      "42      8  4.463996                 4\n",
      "43      9  5.524645                 6\n",
      "44      6  4.873910                 5\n",
      "45      9  7.546466                 8\n",
      "46      8  5.524279                 6\n",
      "47      3  3.522933                 4\n",
      "48      9  5.952977                 6\n",
      "49      7  6.634501                 7\n",
      "50     11  8.620321                 9\n",
      "51      8  5.921157                 6\n",
      "52      6  7.338255                 7\n",
      "53      5  4.507988                 5\n",
      "54      5  5.176543                 5\n",
      "55      3  3.901613                 4\n",
      "56      3  3.466111                 3\n",
      "57      3  4.369114                 4\n",
      "58      5  5.155222                 5\n",
      "59     12  9.791322                10\n",
      "60      5  4.200156                 4\n",
      "61      5  7.697479                 8\n",
      "62      5  3.779623                 4\n",
      "63     10  7.488508                 7\n",
      "64      3  3.815459                 4\n",
      "65      5  3.177237                 3\n",
      "66      5  4.092686                 4\n",
      "67     11  8.877583                 9\n",
      "68      5  4.473813                 4\n",
      "69      3  4.808456                 5\n",
      "70      3  4.600995                 5\n",
      "71      3  3.540310                 4\n",
      "72      3  5.672097                 6\n",
      "73      3  5.065983                 5\n",
      "74      3  3.849248                 4\n",
      "75      6  3.991025                 4\n",
      "76      3  3.645837                 4\n",
      "77      7  5.273054                 5\n",
      "78      9  7.982763                 8\n",
      "79      9  7.655437                 8\n",
      "80      6  6.042645                 6\n",
      "81      3  4.231717                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 27  5  0  0\n",
      "         2 13 12  7  1\n",
      "         3  1  5  4  1\n",
      "         4  1  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.55            \n",
      "                 95% CI : (0.4347, 0.6615)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.3692          \n",
      "                                          \n",
      "                  Kappa : 0.3065          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.4358          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6429   0.5217   0.3333   0.3333\n",
      "Specificity            0.8684   0.6316   0.8971   0.9610\n",
      "Pos Pred Value         0.8438   0.3636   0.3636   0.2500\n",
      "Neg Pred Value         0.6875   0.7660   0.8841   0.9737\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.3375   0.1500   0.0500   0.0125\n",
      "Detection Prevalence   0.4000   0.4125   0.1375   0.0500\n",
      "Balanced Accuracy      0.7556   0.5767   0.6152   0.6472\n",
      "Fold 5 Accuracy: 0.55 \n",
      "Fold 5 AUC: 0.767037612146308 \n",
      "Fold 5 Overall Sensitivity: 0.457815734989648 \n",
      "Fold 5 Overall Specificity: 0.839524446142093 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.604251101522965 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.755234                 5\n",
      "2       3  4.434627                 4\n",
      "3       3  4.852271                 5\n",
      "4       3  5.237284                 5\n",
      "5       3  9.835264                10\n",
      "6       3  4.859699                 5\n",
      "7       5  2.276183                 2\n",
      "8       5  6.268452                 6\n",
      "9       4  4.349702                 4\n",
      "10      3  4.231964                 4\n",
      "11      7  7.995546                 8\n",
      "12      3  2.959054                 3\n",
      "13      7  6.441749                 6\n",
      "14      3  2.549196                 3\n",
      "15      3  3.275475                 3\n",
      "16      8  5.793829                 6\n",
      "17      3  3.450830                 3\n",
      "18      5  3.595025                 4\n",
      "19      3  3.410211                 3\n",
      "20      3  4.952495                 5\n",
      "21      3  4.308418                 4\n",
      "22      3  4.782653                 5\n",
      "23      5  5.542946                 6\n",
      "24      3  3.567875                 4\n",
      "25      6 10.026906                10\n",
      "26      6  7.971283                 8\n",
      "27      5  5.315687                 5\n",
      "28      5  5.149595                 5\n",
      "29      5  5.317281                 5\n",
      "30      3  3.625063                 4\n",
      "31      3  4.553539                 5\n",
      "32      4  4.775998                 5\n",
      "33      7  5.878331                 6\n",
      "34      3  3.226619                 3\n",
      "35      3  1.260656                 1\n",
      "36      5  4.670038                 5\n",
      "37      5  5.429413                 5\n",
      "38      7  7.949066                 8\n",
      "39     10 10.700064                11\n",
      "40     10  7.983047                 8\n",
      "41      3  4.400421                 4\n",
      "42      3  4.048179                 4\n",
      "43      8  5.524548                 6\n",
      "44      3  3.347592                 3\n",
      "45      7  5.544158                 6\n",
      "46      3  2.653283                 3\n",
      "47      7  4.662219                 5\n",
      "48      6  6.407438                 6\n",
      "49     11  4.788890                 5\n",
      "50      3  2.736541                 3\n",
      "51      6  7.846575                 8\n",
      "52      6  5.942769                 6\n",
      "53      3  4.784625                 5\n",
      "54      3  4.345482                 4\n",
      "55      9 10.140466                10\n",
      "56      3  4.101180                 4\n",
      "57      6  4.088753                 4\n",
      "58      3  5.058014                 5\n",
      "59      5  4.510468                 5\n",
      "60      5  6.864259                 7\n",
      "61      3  1.116059                 1\n",
      "62      3  3.027323                 3\n",
      "63      3  2.813000                 3\n",
      "64      9  5.779817                 6\n",
      "65      6  5.478710                 5\n",
      "66      5  3.596844                 4\n",
      "67      7  9.485630                 9\n",
      "68      3  3.148966                 3\n",
      "69      4  7.549323                 8\n",
      "70      3  2.869710                 3\n",
      "71      5  4.818458                 5\n",
      "72      6  6.988452                 7\n",
      "73      8  6.623890                 7\n",
      "74      6  4.283172                 4\n",
      "75      3  4.715940                 5\n",
      "76      3  4.643290                 5\n",
      "77      3  4.034301                 4\n",
      "78      3  5.495984                 5\n",
      "79      5  8.382687                 8\n",
      "80      3  3.192003                 3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 157  41   5   1\n",
      "         2  46  61  36   7\n",
      "         3   6  15  19   7\n",
      "         4   1   1   1   3\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5897          \n",
      "                 95% CI : (0.5402, 0.6379)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.001670        \n",
      "                                          \n",
      "                  Kappa : 0.3345          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.006169        \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7476   0.5169  0.31148 0.166667\n",
      "Specificity            0.7614   0.6920  0.91908 0.992288\n",
      "Pos Pred Value         0.7696   0.4067  0.40426 0.500000\n",
      "Neg Pred Value         0.7389   0.7782  0.88333 0.962594\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.3857   0.1499  0.04668 0.007371\n",
      "Detection Prevalence   0.5012   0.3686  0.11548 0.014742\n",
      "Balanced Accuracy      0.7545   0.6045  0.61528 0.579477\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5895544 \n",
      "Average AUC: 0.7602095 \n",
      "Average Sensitivity: 0.4353928 \n",
      "Average Specificity: 0.8413405 \n",
      "Average Pearson Correlation: 0.6019682 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 10)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e60d47e",
   "metadata": {},
   "source": [
    "# 10 Markers Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c16aba95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 39 15  3  1\n",
      "         2  3  9  6  1\n",
      "         3  0  0  3  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.622           \n",
      "                 95% CI : (0.5081, 0.7268)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.029708        \n",
      "                                          \n",
      "                  Kappa : 0.3261          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.001835        \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9286   0.3750  0.25000  0.00000\n",
      "Specificity            0.5250   0.8276  0.97143  1.00000\n",
      "Pos Pred Value         0.6724   0.4737  0.60000      NaN\n",
      "Neg Pred Value         0.8750   0.7619  0.88312  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4756   0.1098  0.03659  0.00000\n",
      "Detection Prevalence   0.7073   0.2317  0.06098  0.00000\n",
      "Balanced Accuracy      0.7268   0.6013  0.61071  0.50000\n",
      "Fold 1 Accuracy: 0.621951219512195 \n",
      "Fold 1 AUC: 0.744419642857143 \n",
      "Fold 1 Overall Sensitivity: 0.388392857142857 \n",
      "Fold 1 Overall Specificity: 0.831003694581281 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.642560532657274 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  6.000271                   6\n",
      "2       3  4.276715                   4\n",
      "3       3  3.165433                   3\n",
      "4       5  3.632901                   3\n",
      "5       5  4.306226                   4\n",
      "6       4  6.784389                   6\n",
      "7       3  4.405510                   4\n",
      "8       5  3.708987                   3\n",
      "9       5  3.558270                   3\n",
      "10      3  3.039578                   3\n",
      "11      3  4.536477                   4\n",
      "12      3  4.076198                   4\n",
      "13      5  4.738698                   4\n",
      "14     10  9.727961                   9\n",
      "15      3  3.665806                   3\n",
      "16      3  3.805039                   3\n",
      "17      6  6.759630                   6\n",
      "18      3  3.792517                   3\n",
      "19      3  3.117593                   3\n",
      "20      3  3.589542                   3\n",
      "21      3  6.556075                   6\n",
      "22      3  3.575071                   3\n",
      "23      3  3.233758                   3\n",
      "24      8  7.922504                   7\n",
      "25      3  4.144749                   4\n",
      "26      3  4.353071                   4\n",
      "27      3  4.834076                   4\n",
      "28      3  4.263536                   4\n",
      "29      3  4.476118                   4\n",
      "30      3  4.340056                   4\n",
      "31      5  3.208875                   3\n",
      "32      3  3.440274                   3\n",
      "33      3  4.038716                   4\n",
      "34      3  3.075827                   3\n",
      "35      5  5.689452                   5\n",
      "36      3  4.285897                   4\n",
      "37      6  5.311399                   5\n",
      "38      6  5.635862                   5\n",
      "39      3  4.397827                   4\n",
      "40      5  4.873764                   4\n",
      "41      5  5.567677                   5\n",
      "42      6  3.748444                   3\n",
      "43      3  4.038751                   4\n",
      "44      7  6.095189                   6\n",
      "45      7  5.502147                   5\n",
      "46      3  4.963228                   4\n",
      "47     11  6.314821                   6\n",
      "48      9  5.265806                   5\n",
      "49      3  4.259518                   4\n",
      "50      3  2.940904                   2\n",
      "51      3  3.616928                   3\n",
      "52      8  8.022732                   8\n",
      "53      6  3.716771                   3\n",
      "54      6  5.775449                   5\n",
      "55      5  4.568975                   4\n",
      "56      5  4.460989                   4\n",
      "57      7  5.721420                   5\n",
      "58      3  3.095336                   3\n",
      "59      3  3.140219                   3\n",
      "60      9  3.964907                   3\n",
      "61      9  4.813841                   4\n",
      "62      9  6.103746                   6\n",
      "63      3  4.112763                   4\n",
      "64      9  4.351830                   4\n",
      "65     10  4.798092                   4\n",
      "66      6  6.192749                   6\n",
      "67      3  4.137304                   4\n",
      "68      5  3.745403                   3\n",
      "69      5  3.242244                   3\n",
      "70      3  3.029661                   3\n",
      "71      6  4.931571                   4\n",
      "72      5  5.546602                   5\n",
      "73      8  7.415579                   7\n",
      "74      5  4.597232                   4\n",
      "75      3  5.571274                   5\n",
      "76      3  3.506727                   3\n",
      "77      7  5.428374                   5\n",
      "78      3  4.231030                   4\n",
      "79      3  3.975255                   3\n",
      "80     11  7.954550                   7\n",
      "81      3  3.253260                   3\n",
      "82      3  3.707244                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36 11  1  1\n",
      "         2  4 11  7  2\n",
      "         3  2  2  5  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6341          \n",
      "                 95% CI : (0.5205, 0.7378)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.01746         \n",
      "                                          \n",
      "                  Kappa : 0.3808          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.4583  0.38462  0.00000\n",
      "Specificity            0.6750   0.7759  0.94203  1.00000\n",
      "Pos Pred Value         0.7347   0.4583  0.55556      NaN\n",
      "Neg Pred Value         0.8182   0.7759  0.89041  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4390   0.1341  0.06098  0.00000\n",
      "Detection Prevalence   0.5976   0.2927  0.10976  0.00000\n",
      "Balanced Accuracy      0.7661   0.6171  0.66332  0.50000\n",
      "Fold 2 Accuracy: 0.634146341463415 \n",
      "Fold 2 AUC: 0.644803113553113 \n",
      "Fold 2 Overall Sensitivity: 0.425022893772894 \n",
      "Fold 2 Overall Specificity: 0.848222763618191 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.519978278837124 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  5.284922                   5\n",
      "2       4  4.286156                   4\n",
      "3       3  4.852323                   4\n",
      "4       3  5.710145                   5\n",
      "5       3  3.501279                   3\n",
      "6       3  2.712148                   2\n",
      "7       5  4.423597                   4\n",
      "8       3  4.839294                   4\n",
      "9       5  9.049261                   9\n",
      "10      4  3.239906                   3\n",
      "11      3  3.387457                   3\n",
      "12      3  7.305009                   7\n",
      "13      3  4.199736                   4\n",
      "14      6  5.307442                   5\n",
      "15     12  6.323556                   6\n",
      "16      6  7.331307                   7\n",
      "17      3  4.163340                   4\n",
      "18      5  5.731551                   5\n",
      "19      3  3.938198                   3\n",
      "20      3  4.452350                   4\n",
      "21      3  3.230036                   3\n",
      "22      3  3.489963                   3\n",
      "23      3  3.611087                   3\n",
      "24      3  2.154693                   2\n",
      "25      3  5.242201                   5\n",
      "26      3  4.692606                   4\n",
      "27      3  3.223911                   3\n",
      "28      3  4.114610                   4\n",
      "29      5  4.601782                   4\n",
      "30      4  8.721995                   8\n",
      "31      3  4.982139                   4\n",
      "32      3  3.348330                   3\n",
      "33      3  4.754811                   4\n",
      "34      3  3.393562                   3\n",
      "35      8  5.516273                   5\n",
      "36      6  6.724571                   6\n",
      "37      5  2.760959                   2\n",
      "38      3  5.421124                   5\n",
      "39      9  7.530312                   7\n",
      "40     12  6.632856                   6\n",
      "41      5  5.745909                   5\n",
      "42      7  5.345267                   5\n",
      "43      6  6.340785                   6\n",
      "44      5  4.624377                   4\n",
      "45      8  7.308579                   7\n",
      "46      5  4.581736                   4\n",
      "47      7  5.270835                   5\n",
      "48      5  4.879031                   4\n",
      "49      5  5.544631                   5\n",
      "50      3  3.258351                   3\n",
      "51      5  6.157307                   6\n",
      "52      8  5.969111                   5\n",
      "53      3  4.267516                   4\n",
      "54      5  6.870498                   6\n",
      "55      6  4.319721                   4\n",
      "56      6  6.137517                   6\n",
      "57      5  3.944161                   3\n",
      "58      6  5.335346                   5\n",
      "59      5  3.488481                   3\n",
      "60     11  3.938040                   3\n",
      "61      3  3.419484                   3\n",
      "62      3  2.216849                   2\n",
      "63      5  4.566916                   4\n",
      "64      3  3.866299                   3\n",
      "65      3  4.785108                   4\n",
      "66      7  5.782145                   5\n",
      "67      3  2.731516                   2\n",
      "68      5  6.987893                   6\n",
      "69      8  8.062822                   8\n",
      "70      8  6.152204                   6\n",
      "71      3  4.572513                   4\n",
      "72      3  3.736294                   3\n",
      "73      7  7.072123                   7\n",
      "74      3  4.359214                   4\n",
      "75      7  5.080892                   5\n",
      "76      3  4.036771                   4\n",
      "77      3  3.727096                   3\n",
      "78      3  4.579071                   4\n",
      "79      3  3.622411                   3\n",
      "80      5  4.030437                   4\n",
      "81      8  4.567784                   4\n",
      "82      8  8.159735                   8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 17  5  1\n",
      "         2  4  6  6  3\n",
      "         3  0  1  1  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5488         \n",
      "                 95% CI : (0.4349, 0.659)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2907         \n",
      "                                         \n",
      "                  Kappa : 0.176          \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048  0.25000  0.08333  0.00000\n",
      "Specificity            0.4250  0.77586  0.98571  1.00000\n",
      "Pos Pred Value         0.6230  0.31579  0.50000      NaN\n",
      "Neg Pred Value         0.8095  0.71429  0.86250  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4634  0.07317  0.01220  0.00000\n",
      "Detection Prevalence   0.7439  0.23171  0.02439  0.00000\n",
      "Balanced Accuracy      0.6649  0.51293  0.53452  0.50000\n",
      "Fold 3 Accuracy: 0.548780487804878 \n",
      "Fold 3 AUC: 0.681175595238095 \n",
      "Fold 3 Overall Sensitivity: 0.30952380952381 \n",
      "Fold 3 Overall Specificity: 0.796644088669951 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.475722721998856 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  3.763716                   3\n",
      "2       3  5.800056                   5\n",
      "3       3  3.404017                   3\n",
      "4       3  5.225839                   5\n",
      "5       3  4.116871                   4\n",
      "6       7  7.662614                   7\n",
      "7       5  3.511488                   3\n",
      "8       3  4.262643                   4\n",
      "9       3  4.187860                   4\n",
      "10      3  4.456501                   4\n",
      "11      4  4.168628                   4\n",
      "12      3  4.448710                   4\n",
      "13      3  4.963586                   4\n",
      "14      3  2.582014                   2\n",
      "15      8  4.689790                   4\n",
      "16      3  3.557320                   3\n",
      "17      3  6.366597                   6\n",
      "18      5  5.724535                   5\n",
      "19      3  4.375340                   4\n",
      "20      7  5.182697                   5\n",
      "21      3  3.866220                   3\n",
      "22      3  3.177799                   3\n",
      "23      6  4.714063                   4\n",
      "24      8  5.030026                   5\n",
      "25      3  3.915273                   3\n",
      "26     10  5.775241                   5\n",
      "27      3  3.541053                   3\n",
      "28      3  4.198225                   4\n",
      "29      3  4.839266                   4\n",
      "30      3  3.126472                   3\n",
      "31      3  4.268918                   4\n",
      "32      6  4.002556                   4\n",
      "33      6  5.130205                   5\n",
      "34      7  5.348803                   5\n",
      "35      5  7.274199                   7\n",
      "36      3  3.472797                   3\n",
      "37      5  4.804682                   4\n",
      "38      5  4.442820                   4\n",
      "39      3  3.084794                   3\n",
      "40      6  3.370279                   3\n",
      "41      3  4.219712                   4\n",
      "42      8  3.617868                   3\n",
      "43      5  4.593028                   4\n",
      "44      5  4.431754                   4\n",
      "45     12  5.217011                   5\n",
      "46      5  5.018267                   5\n",
      "47      6  4.206887                   4\n",
      "48      5  5.290488                   5\n",
      "49      6  5.967424                   5\n",
      "50      3  3.321561                   3\n",
      "51      5  4.375785                   4\n",
      "52      5  4.145724                   4\n",
      "53      3  4.162738                   4\n",
      "54      7  4.942136                   4\n",
      "55      8  4.801872                   4\n",
      "56      3  3.694807                   3\n",
      "57      3  4.279993                   4\n",
      "58      5  6.181805                   6\n",
      "59      7  6.188892                   6\n",
      "60     10  6.829184                   6\n",
      "61      5  4.512866                   4\n",
      "62      3  4.732093                   4\n",
      "63      3  3.091554                   3\n",
      "64      5  4.301774                   4\n",
      "65      5  3.869605                   3\n",
      "66      3  3.363611                   3\n",
      "67      5  4.746593                   4\n",
      "68      3  5.170976                   5\n",
      "69      8  6.062248                   6\n",
      "70      3  3.605913                   3\n",
      "71      9  5.728364                   5\n",
      "72      3  3.741688                   3\n",
      "73      3  4.081262                   4\n",
      "74      3  3.391698                   3\n",
      "75      3  4.234721                   4\n",
      "76      3  2.935033                   2\n",
      "77      3  3.809391                   3\n",
      "78     10  4.885740                   4\n",
      "79      8  3.473616                   3\n",
      "80      3  4.116321                   4\n",
      "81      5  3.286240                   3\n",
      "82      3  3.571029                   3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36 11  1  0\n",
      "         2  6  9  6  0\n",
      "         3  0  3  5  4\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6173          \n",
      "                 95% CI : (0.5026, 0.7231)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.04712         \n",
      "                                          \n",
      "                  Kappa : 0.3591          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.3913  0.41667  0.00000\n",
      "Specificity            0.6923   0.7931  0.89855  1.00000\n",
      "Pos Pred Value         0.7500   0.4286  0.41667      NaN\n",
      "Neg Pred Value         0.8182   0.7667  0.89855  0.95062\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4444   0.1111  0.06173  0.00000\n",
      "Detection Prevalence   0.5926   0.2593  0.14815  0.00000\n",
      "Balanced Accuracy      0.7747   0.5922  0.65761  0.50000\n",
      "Fold 4 Accuracy: 0.617283950617284 \n",
      "Fold 4 AUC: 0.848041752933057 \n",
      "Fold 4 Overall Sensitivity: 0.416278467908903 \n",
      "Fold 4 Overall Specificity: 0.845990466305309 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.767328379209843 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  3.960872                   3\n",
      "2       3  3.865230                   3\n",
      "3       3  2.374564                   2\n",
      "4       4  6.502605                   6\n",
      "5       3  4.440508                   4\n",
      "6       6  5.894686                   5\n",
      "7       3  3.735166                   3\n",
      "8       4  6.343042                   6\n",
      "9       3  4.765550                   4\n",
      "10      3  4.819354                   4\n",
      "11      3  4.004962                   4\n",
      "12      6  2.316336                   2\n",
      "13      3  3.792570                   3\n",
      "14      3  2.306149                   2\n",
      "15      6  6.209162                   6\n",
      "16      3  4.183170                   4\n",
      "17      5  7.724165                   7\n",
      "18      3  3.659344                   3\n",
      "19      3  3.027233                   3\n",
      "20      9  8.191192                   8\n",
      "21      3  4.148547                   4\n",
      "22      3  3.695549                   3\n",
      "23      3  3.869890                   3\n",
      "24      3  3.023371                   3\n",
      "25      5  5.323168                   5\n",
      "26      3  4.162748                   4\n",
      "27      3  5.567448                   5\n",
      "28      5  4.781236                   4\n",
      "29      5  3.983907                   3\n",
      "30      3  4.453094                   4\n",
      "31      3  5.007354                   5\n",
      "32      3  2.430970                   2\n",
      "33      8  7.386844                   7\n",
      "34      3  3.950983                   3\n",
      "35      3  3.600119                   3\n",
      "36      3  4.603948                   4\n",
      "37      3  3.460374                   3\n",
      "38      3  3.596020                   3\n",
      "39      5  5.145048                   5\n",
      "40      5  5.333722                   5\n",
      "41      6  5.297152                   5\n",
      "42      8  4.463996                   4\n",
      "43      9  5.524645                   5\n",
      "44      6  4.873910                   4\n",
      "45      9  7.546466                   7\n",
      "46      8  5.524279                   5\n",
      "47      3  3.522933                   3\n",
      "48      9  5.952977                   5\n",
      "49      7  6.634501                   6\n",
      "50     11  8.620321                   8\n",
      "51      8  5.921157                   5\n",
      "52      6  7.338255                   7\n",
      "53      5  4.507988                   4\n",
      "54      5  5.176543                   5\n",
      "55      3  3.901613                   3\n",
      "56      3  3.466111                   3\n",
      "57      3  4.369114                   4\n",
      "58      5  5.155222                   5\n",
      "59     12  9.791322                   9\n",
      "60      5  4.200156                   4\n",
      "61      5  7.697479                   7\n",
      "62      5  3.779623                   3\n",
      "63     10  7.488508                   7\n",
      "64      3  3.815459                   3\n",
      "65      5  3.177237                   3\n",
      "66      5  4.092686                   4\n",
      "67     11  8.877583                   8\n",
      "68      5  4.473813                   4\n",
      "69      3  4.808456                   4\n",
      "70      3  4.600995                   4\n",
      "71      3  3.540310                   3\n",
      "72      3  5.672097                   5\n",
      "73      3  5.065983                   5\n",
      "74      3  3.849248                   3\n",
      "75      6  3.991025                   3\n",
      "76      3  3.645837                   3\n",
      "77      7  5.273054                   5\n",
      "78      9  7.982763                   7\n",
      "79      9  7.655437                   7\n",
      "80      6  6.042645                   6\n",
      "81      3  4.231717                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37  8  1  1\n",
      "         2  3 11  7  0\n",
      "         3  2  3  3  1\n",
      "         4  0  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.65            \n",
      "                 95% CI : (0.5352, 0.7533)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.01611         \n",
      "                                          \n",
      "                  Kappa : 0.4145          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.40051         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810   0.4783   0.2500   0.3333\n",
      "Specificity            0.7368   0.8246   0.9118   0.9740\n",
      "Pos Pred Value         0.7872   0.5238   0.3333   0.3333\n",
      "Neg Pred Value         0.8485   0.7966   0.8732   0.9740\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4625   0.1375   0.0375   0.0125\n",
      "Detection Prevalence   0.5875   0.2625   0.1125   0.0375\n",
      "Balanced Accuracy      0.8089   0.6514   0.5809   0.6537\n",
      "Fold 5 Accuracy: 0.65 \n",
      "Fold 5 AUC: 0.734400161030596 \n",
      "Fold 5 Overall Sensitivity: 0.485636645962733 \n",
      "Fold 5 Overall Specificity: 0.861798547170064 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.604251101522965 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.755234                   4\n",
      "2       3  4.434627                   4\n",
      "3       3  4.852271                   4\n",
      "4       3  5.237284                   5\n",
      "5       3  9.835264                   9\n",
      "6       3  4.859699                   4\n",
      "7       5  2.276183                   2\n",
      "8       5  6.268452                   6\n",
      "9       4  4.349702                   4\n",
      "10      3  4.231964                   4\n",
      "11      7  7.995546                   7\n",
      "12      3  2.959054                   2\n",
      "13      7  6.441749                   6\n",
      "14      3  2.549196                   2\n",
      "15      3  3.275475                   3\n",
      "16      8  5.793829                   5\n",
      "17      3  3.450830                   3\n",
      "18      5  3.595025                   3\n",
      "19      3  3.410211                   3\n",
      "20      3  4.952495                   4\n",
      "21      3  4.308418                   4\n",
      "22      3  4.782653                   4\n",
      "23      5  5.542946                   5\n",
      "24      3  3.567875                   3\n",
      "25      6 10.026906                  10\n",
      "26      6  7.971283                   7\n",
      "27      5  5.315687                   5\n",
      "28      5  5.149595                   5\n",
      "29      5  5.317281                   5\n",
      "30      3  3.625063                   3\n",
      "31      3  4.553539                   4\n",
      "32      4  4.775998                   4\n",
      "33      7  5.878331                   5\n",
      "34      3  3.226619                   3\n",
      "35      3  1.260656                   1\n",
      "36      5  4.670038                   4\n",
      "37      5  5.429413                   5\n",
      "38      7  7.949066                   7\n",
      "39     10 10.700064                  10\n",
      "40     10  7.983047                   7\n",
      "41      3  4.400421                   4\n",
      "42      3  4.048179                   4\n",
      "43      8  5.524548                   5\n",
      "44      3  3.347592                   3\n",
      "45      7  5.544158                   5\n",
      "46      3  2.653283                   2\n",
      "47      7  4.662219                   4\n",
      "48      6  6.407438                   6\n",
      "49     11  4.788890                   4\n",
      "50      3  2.736541                   2\n",
      "51      6  7.846575                   7\n",
      "52      6  5.942769                   5\n",
      "53      3  4.784625                   4\n",
      "54      3  4.345482                   4\n",
      "55      9 10.140466                  10\n",
      "56      3  4.101180                   4\n",
      "57      6  4.088753                   4\n",
      "58      3  5.058014                   5\n",
      "59      5  4.510468                   4\n",
      "60      5  6.864259                   6\n",
      "61      3  1.116059                   1\n",
      "62      3  3.027323                   3\n",
      "63      3  2.813000                   2\n",
      "64      9  5.779817                   5\n",
      "65      6  5.478710                   5\n",
      "66      5  3.596844                   3\n",
      "67      7  9.485630                   9\n",
      "68      3  3.148966                   3\n",
      "69      4  7.549323                   7\n",
      "70      3  2.869710                   2\n",
      "71      5  4.818458                   4\n",
      "72      6  6.988452                   6\n",
      "73      8  6.623890                   6\n",
      "74      6  4.283172                   4\n",
      "75      3  4.715940                   4\n",
      "76      3  4.643290                   4\n",
      "77      3  4.034301                   4\n",
      "78      3  5.495984                   5\n",
      "79      5  8.382687                   8\n",
      "80      3  3.192003                   3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 186  62  11   4\n",
      "         2  20  46  32   6\n",
      "         3   4   9  17   7\n",
      "         4   0   1   1   1\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6143         \n",
      "                 95% CI : (0.565, 0.6618)\n",
      "    No Information Rate : 0.516          \n",
      "    P-Value [Acc > NIR] : 4.120e-05      \n",
      "                                         \n",
      "                  Kappa : 0.3332         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : 5.269e-09      \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8857   0.3898  0.27869 0.055556\n",
      "Specificity            0.6091   0.7993  0.94220 0.994859\n",
      "Pos Pred Value         0.7072   0.4423  0.45946 0.333333\n",
      "Neg Pred Value         0.8333   0.7624  0.88108 0.957921\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4570   0.1130  0.04177 0.002457\n",
      "Detection Prevalence   0.6462   0.2555  0.09091 0.007371\n",
      "Balanced Accuracy      0.7474   0.5946  0.61044 0.525207\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6144324 \n",
      "Average AUC: 0.7305681 \n",
      "Average Sensitivity: 0.4049709 \n",
      "Average Specificity: 0.8367319 \n",
      "Average Pearson Correlation: 0.6019682 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 10)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Truncate the predicted values before classification\n",
    "    truncated_predictions <- floor(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(truncated_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Truncated_Predicted = truncated_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b382b",
   "metadata": {},
   "source": [
    "# 75 Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327af791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:IRanges’:\n",
      "\n",
      "    cov, var\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:S4Vectors’:\n",
      "\n",
      "    cov, var\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:BiocGenerics’:\n",
      "\n",
      "    var\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n",
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Pearson Correlation Coefficient: 0.624765936064906 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  6  2  1\n",
      "         2  7 10  4  0\n",
      "         3  1  8  6  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6098          \n",
      "                 95% CI : (0.4957, 0.7156)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.0483          \n",
      "                                          \n",
      "                  Kappa : 0.3749          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.4167  0.50000  0.00000\n",
      "Specificity            0.7750   0.8103  0.82857  1.00000\n",
      "Pos Pred Value         0.7907   0.4762  0.33333      NaN\n",
      "Neg Pred Value         0.7949   0.7705  0.90625  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4146   0.1220  0.07317  0.00000\n",
      "Detection Prevalence   0.5244   0.2561  0.21951  0.00000\n",
      "Balanced Accuracy      0.7923   0.6135  0.66429  0.50000\n",
      "Fold 1 Accuracy: 0.609756097560976 \n",
      "Fold 1 AUC: 0.722304894179894 \n",
      "Fold 1 Overall Sensitivity: 0.431547619047619 \n",
      "Fold 1 Overall Specificity: 0.853479064039409 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6 8.0697794                 8\n",
      "2       3 2.7786745                 3\n",
      "3       3 3.2857264                 3\n",
      "4       5 4.7956086                 5\n",
      "5       5 7.3686693                 7\n",
      "6       4 6.7834815                 7\n",
      "7       3 5.0864882                 5\n",
      "8       5 5.3719157                 5\n",
      "9       5 4.8820279                 5\n",
      "10      3 3.8871188                 4\n",
      "11      3 3.6135780                 4\n",
      "12      3 3.4970945                 3\n",
      "13      5 4.6556981                 5\n",
      "14     10 8.4426236                 8\n",
      "15      3 1.1841944                 1\n",
      "16      3 1.8375331                 2\n",
      "17      6 7.5691402                 8\n",
      "18      3 3.9740860                 4\n",
      "19      3 2.9419078                 3\n",
      "20      3 3.5515829                 4\n",
      "21      3 4.5640652                 5\n",
      "22      3 3.6488367                 4\n",
      "23      3 1.5707666                 2\n",
      "24      8 6.9008824                 7\n",
      "25      3 3.2079074                 3\n",
      "26      3 3.6912080                 4\n",
      "27      3 4.3499411                 4\n",
      "28      3 3.7632943                 4\n",
      "29      3 3.6222662                 4\n",
      "30      3 4.2417196                 4\n",
      "31      5 1.7761164                 2\n",
      "32      3 4.1829979                 4\n",
      "33      3 2.4018381                 2\n",
      "34      3 2.3471801                 2\n",
      "35      5 8.2994763                 8\n",
      "36      3 6.2883740                 6\n",
      "37      6 6.2869091                 6\n",
      "38      6 6.5559107                 7\n",
      "39      3 1.7866863                 2\n",
      "40      5 3.2612543                 3\n",
      "41      5 5.9806455                 6\n",
      "42      6 3.0996181                 3\n",
      "43      3 2.8979074                 3\n",
      "44      7 4.2415820                 4\n",
      "45      7 4.0024018                 4\n",
      "46      3 4.5726004                 5\n",
      "47     11 7.3633478                 7\n",
      "48      9 6.3416157                 6\n",
      "49      3 6.0115850                 6\n",
      "50      3 0.9709145                 1\n",
      "51      3 3.1701329                 3\n",
      "52      8 7.3864140                 7\n",
      "53      6 1.8262684                 2\n",
      "54      6 6.3651338                 6\n",
      "55      5 6.0633045                 6\n",
      "56      5 2.9086413                 3\n",
      "57      7 8.4598525                 8\n",
      "58      3 4.0221939                 4\n",
      "59      3 2.5756289                 3\n",
      "60      9 5.0358017                 5\n",
      "61      9 5.5604188                 6\n",
      "62      9 6.9306635                 7\n",
      "63      3 3.7993427                 4\n",
      "64      9 7.0729242                 7\n",
      "65     10 4.4543754                 4\n",
      "66      6 7.4635975                 7\n",
      "67      3 2.2093835                 2\n",
      "68      5 6.5308893                 7\n",
      "69      5 2.4032968                 2\n",
      "70      3 3.3830013                 3\n",
      "71      6 5.7729322                 6\n",
      "72      5 7.3089809                 7\n",
      "73      8 7.0222691                 7\n",
      "74      5 4.7178893                 5\n",
      "75      3 4.7015022                 5\n",
      "76      3 2.5816772                 3\n",
      "77      7 4.8038331                 5\n",
      "78      3 3.5693628                 4\n",
      "79      3 4.6962592                 5\n",
      "80     11 8.2385228                 8\n",
      "81      3 1.9207347                 2\n",
      "82      3 4.0462403                 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Pearson Correlation Coefficient: 0.660304912619265 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  4  1  0\n",
      "         2 12 11  2  1\n",
      "         3  1  9 10  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6098          \n",
      "                 95% CI : (0.4957, 0.7156)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.0483          \n",
      "                                          \n",
      "                  Kappa : 0.4017          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.4583   0.7692  0.00000\n",
      "Specificity            0.8750   0.7414   0.8261  1.00000\n",
      "Pos Pred Value         0.8529   0.4231   0.4545      NaN\n",
      "Neg Pred Value         0.7292   0.7679   0.9500  0.96341\n",
      "Prevalence             0.5122   0.2927   0.1585  0.03659\n",
      "Detection Rate         0.3537   0.1341   0.1220  0.00000\n",
      "Detection Prevalence   0.4146   0.3171   0.2683  0.00000\n",
      "Balanced Accuracy      0.7827   0.5999   0.7977  0.50000\n",
      "Fold 2 Accuracy: 0.609756097560976 \n",
      "Fold 2 AUC: 0.748136701261701 \n",
      "Fold 2 Overall Sensitivity: 0.479510073260073 \n",
      "Fold 2 Overall Specificity: 0.860616566716642 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  4.458205                 4\n",
      "2       4  5.334694                 5\n",
      "3       3  4.921855                 5\n",
      "4       3  4.039519                 4\n",
      "5       3  3.120676                 3\n",
      "6       3  2.571854                 3\n",
      "7       5  3.067107                 3\n",
      "8       3  4.561400                 5\n",
      "9       5  7.628589                 8\n",
      "10      4  3.191030                 3\n",
      "11      3  1.292245                 1\n",
      "12      3  7.711550                 8\n",
      "13      3  4.142265                 4\n",
      "14      6  7.201847                 7\n",
      "15     12  7.157880                 7\n",
      "16      6  7.227609                 7\n",
      "17      3  5.726460                 6\n",
      "18      5  5.962631                 6\n",
      "19      3  5.724384                 6\n",
      "20      3  4.891296                 5\n",
      "21      3  2.004038                 2\n",
      "22      3  2.127501                 2\n",
      "23      3  2.953482                 3\n",
      "24      3  4.821969                 5\n",
      "25      3  4.462626                 4\n",
      "26      3  6.252708                 6\n",
      "27      3  3.424267                 3\n",
      "28      3  3.785883                 4\n",
      "29      5  4.575194                 5\n",
      "30      4  5.975731                 6\n",
      "31      3  4.659000                 5\n",
      "32      3  2.611433                 3\n",
      "33      3  5.221441                 5\n",
      "34      3  3.788682                 4\n",
      "35      8  8.908240                 9\n",
      "36      6  8.018099                 8\n",
      "37      5  4.558829                 5\n",
      "38      3  4.443721                 4\n",
      "39      9  7.692079                 8\n",
      "40     12  8.138765                 8\n",
      "41      5  5.919330                 6\n",
      "42      7  5.597360                 6\n",
      "43      6  7.284312                 7\n",
      "44      5  4.247621                 4\n",
      "45      8  8.172492                 8\n",
      "46      5  3.242370                 3\n",
      "47      7  7.976268                 8\n",
      "48      5  4.981418                 5\n",
      "49      5  7.999308                 8\n",
      "50      3  3.636866                 4\n",
      "51      5  6.642237                 7\n",
      "52      8  6.537899                 7\n",
      "53      3  3.097576                 3\n",
      "54      5  5.810172                 6\n",
      "55      6  6.567246                 7\n",
      "56      6  6.524717                 7\n",
      "57      5  5.322646                 5\n",
      "58      6  5.767663                 6\n",
      "59      5  3.740165                 4\n",
      "60     11  5.051104                 5\n",
      "61      3  4.374665                 4\n",
      "62      3  1.225872                 1\n",
      "63      5  4.754037                 5\n",
      "64      3  2.100836                 2\n",
      "65      3  4.101240                 4\n",
      "66      7  6.791798                 7\n",
      "67      3  1.879400                 2\n",
      "68      5  5.190741                 5\n",
      "69      8  7.360333                 7\n",
      "70      8  7.709388                 8\n",
      "71      3  4.555493                 5\n",
      "72      3  3.789440                 4\n",
      "73      7  7.399100                 7\n",
      "74      3  3.594364                 4\n",
      "75      7  4.335601                 4\n",
      "76      3  3.827791                 4\n",
      "77      3  3.901913                 4\n",
      "78      3  4.056320                 4\n",
      "79      3  1.592288                 2\n",
      "80      5  5.532418                 6\n",
      "81      8  5.282355                 5\n",
      "82      8  9.483899                 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Pearson Correlation Coefficient: 0.535289434230383 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31 10  2  0\n",
      "         2 11 11  9  3\n",
      "         3  0  3  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5244          \n",
      "                 95% CI : (0.4111, 0.6359)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.4564          \n",
      "                                          \n",
      "                  Kappa : 0.2088          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381   0.4583  0.08333  0.00000\n",
      "Specificity            0.7000   0.6034  0.94286  1.00000\n",
      "Pos Pred Value         0.7209   0.3235  0.20000      NaN\n",
      "Neg Pred Value         0.7179   0.7292  0.85714  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.3780   0.1341  0.01220  0.00000\n",
      "Detection Prevalence   0.5244   0.4146  0.06098  0.00000\n",
      "Balanced Accuracy      0.7190   0.5309  0.51310  0.50000\n",
      "Fold 3 Accuracy: 0.524390243902439 \n",
      "Fold 3 AUC: 0.722883597883598 \n",
      "Fold 3 Overall Sensitivity: 0.319940476190476 \n",
      "Fold 3 Overall Specificity: 0.811576354679803 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       6  3.189813                 3\n",
      "2       3  4.179587                 4\n",
      "3       3  2.912235                 3\n",
      "4       3  6.177202                 6\n",
      "5       3  3.630890                 4\n",
      "6       7  7.512751                 8\n",
      "7       5  3.949669                 4\n",
      "8       3  4.485274                 4\n",
      "9       3  5.590345                 6\n",
      "10      3  4.861490                 5\n",
      "11      4  3.766701                 4\n",
      "12      3  3.809042                 4\n",
      "13      3  4.126887                 4\n",
      "14      3  1.585085                 2\n",
      "15      8  3.633041                 4\n",
      "16      3  1.760951                 2\n",
      "17      3  5.490607                 5\n",
      "18      5  5.235897                 5\n",
      "19      3  3.167136                 3\n",
      "20      7  6.042591                 6\n",
      "21      3  2.081194                 2\n",
      "22      3  3.053378                 3\n",
      "23      6  5.660289                 6\n",
      "24      8  5.419416                 5\n",
      "25      3  3.244100                 3\n",
      "26     10  5.161082                 5\n",
      "27      3  2.203081                 2\n",
      "28      3  3.823217                 4\n",
      "29      3  4.504367                 5\n",
      "30      3  2.499372                 2\n",
      "31      3  1.714429                 2\n",
      "32      6  5.589360                 6\n",
      "33      6  5.765780                 6\n",
      "34      7  4.834114                 5\n",
      "35      5  7.554079                 8\n",
      "36      3  3.644201                 4\n",
      "37      5  2.689045                 3\n",
      "38      5  1.659974                 2\n",
      "39      3  2.927570                 3\n",
      "40      6  4.881213                 5\n",
      "41      3  4.211612                 4\n",
      "42      8  4.902882                 5\n",
      "43      5  3.911632                 4\n",
      "44      5  5.298162                 5\n",
      "45     12  6.128266                 6\n",
      "46      5  6.515736                 7\n",
      "47      6  4.002676                 4\n",
      "48      5  5.818871                 6\n",
      "49      6  5.304544                 5\n",
      "50      3  3.221199                 3\n",
      "51      5  4.375411                 4\n",
      "52      5  4.024592                 4\n",
      "53      3  4.992192                 5\n",
      "54      7  5.648585                 6\n",
      "55      8  6.228296                 6\n",
      "56      3  3.582764                 4\n",
      "57      3  3.656478                 4\n",
      "58      5  6.998501                 7\n",
      "59      7  5.255712                 5\n",
      "60     10  7.275623                 7\n",
      "61      5  5.108560                 5\n",
      "62      3  4.864699                 5\n",
      "63      3  4.793990                 5\n",
      "64      5  4.009760                 4\n",
      "65      5  5.011618                 5\n",
      "66      3  4.635219                 5\n",
      "67      5  5.036241                 5\n",
      "68      3  5.198010                 5\n",
      "69      8  5.636440                 6\n",
      "70      3  4.620708                 5\n",
      "71      9  6.410240                 6\n",
      "72      3  3.711706                 4\n",
      "73      3  3.236628                 3\n",
      "74      3  2.422770                 2\n",
      "75      3  4.015903                 4\n",
      "76      3  3.206742                 3\n",
      "77      3  4.176389                 4\n",
      "78     10  5.976471                 6\n",
      "79      8  3.964355                 4\n",
      "80      3  4.092628                 4\n",
      "81      5  2.571327                 3\n",
      "82      3  1.916102                 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Pearson Correlation Coefficient: 0.690801464129187 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  5  1  0\n",
      "         2 11 12  5  0\n",
      "         3  2  6  6  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5926          \n",
      "                 95% CI : (0.4777, 0.7005)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.1104          \n",
      "                                          \n",
      "                  Kappa : 0.3694          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.5217  0.50000  0.25000\n",
      "Specificity            0.8462   0.7241  0.84058  1.00000\n",
      "Pos Pred Value         0.8286   0.4286  0.35294  1.00000\n",
      "Neg Pred Value         0.7174   0.7925  0.90625  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.3580   0.1481  0.07407  0.01235\n",
      "Detection Prevalence   0.4321   0.3457  0.20988  0.01235\n",
      "Balanced Accuracy      0.7683   0.6229  0.67029  0.62500\n",
      "Fold 4 Accuracy: 0.592592592592593 \n",
      "Fold 4 AUC: 0.826223544973545 \n",
      "Fold 4 Overall Sensitivity: 0.490553830227743 \n",
      "Fold 4 Overall Specificity: 0.852717871833314 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Rounded_Predicted\n",
      "1       3  3.668661                 4\n",
      "2       3  3.545038                 4\n",
      "3       3  4.672547                 5\n",
      "4       4  6.396081                 6\n",
      "5       3  4.088754                 4\n",
      "6       6  6.798816                 7\n",
      "7       3  3.182141                 3\n",
      "8       4  6.513118                 7\n",
      "9       3  5.285231                 5\n",
      "10      3  5.317005                 5\n",
      "11      3  3.968002                 4\n",
      "12      6  2.704159                 3\n",
      "13      3  2.902432                 3\n",
      "14      3  1.848719                 2\n",
      "15      6  5.660380                 6\n",
      "16      3  4.346520                 4\n",
      "17      5  7.435387                 7\n",
      "18      3  4.035137                 4\n",
      "19      3  1.379281                 1\n",
      "20      9  7.693066                 8\n",
      "21      3  4.252192                 4\n",
      "22      3  3.925958                 4\n",
      "23      3  3.107264                 3\n",
      "24      3  4.028056                 4\n",
      "25      5  5.037932                 5\n",
      "26      3  5.212664                 5\n",
      "27      3  4.835684                 5\n",
      "28      5  5.091413                 5\n",
      "29      5  3.630165                 4\n",
      "30      3  4.738300                 5\n",
      "31      3  6.619928                 7\n",
      "32      3  3.537221                 4\n",
      "33      8  6.373036                 6\n",
      "34      3  3.876572                 4\n",
      "35      3  3.152434                 3\n",
      "36      3  4.090869                 4\n",
      "37      3  3.167331                 3\n",
      "38      3  4.108587                 4\n",
      "39      5  5.381611                 5\n",
      "40      5  7.129293                 7\n",
      "41      6  5.937625                 6\n",
      "42      8  5.035066                 5\n",
      "43      9  5.805880                 6\n",
      "44      6  4.532779                 5\n",
      "45      9  6.612226                 7\n",
      "46      8  4.685313                 5\n",
      "47      3  3.385940                 3\n",
      "48      9  5.753813                 6\n",
      "49      7  7.863295                 8\n",
      "50     11  8.049442                 8\n",
      "51      8  6.787437                 7\n",
      "52      6  6.678453                 7\n",
      "53      5  4.886708                 5\n",
      "54      5  5.367138                 5\n",
      "55      3  4.258941                 4\n",
      "56      3  2.802308                 3\n",
      "57      3  4.446955                 4\n",
      "58      5  5.395853                 5\n",
      "59     12 10.691860                11\n",
      "60      5  4.324976                 4\n",
      "61      5  8.664518                 9\n",
      "62      5  3.445195                 3\n",
      "63     10  7.789456                 8\n",
      "64      3  2.652221                 3\n",
      "65      5  1.568182                 2\n",
      "66      5  4.780863                 5\n",
      "67     11  8.576555                 9\n",
      "68      5  6.100956                 6\n",
      "69      3  6.136719                 6\n",
      "70      3  4.477719                 4\n",
      "71      3  3.679594                 4\n",
      "72      3  5.714840                 6\n",
      "73      3  4.365009                 4\n",
      "74      3  4.811726                 5\n",
      "75      6  4.773755                 5\n",
      "76      3  3.489135                 3\n",
      "77      7  4.414616                 4\n",
      "78      9  7.976021                 8\n",
      "79      9  7.684382                 8\n",
      "80      6  7.276724                 7\n",
      "81      3  4.532416                 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Pearson Correlation Coefficient: 0.699580675664661 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 33  7  0  0\n",
      "         2  7  7  6  0\n",
      "         3  2  8  5  2\n",
      "         4  0  1  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.575           \n",
      "                 95% CI : (0.4594, 0.6849)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.2169          \n",
      "                                          \n",
      "                  Kappa : 0.3279          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7857   0.3043   0.4167   0.3333\n",
      "Specificity            0.8158   0.7719   0.8235   0.9740\n",
      "Pos Pred Value         0.8250   0.3500   0.2941   0.3333\n",
      "Neg Pred Value         0.7750   0.7333   0.8889   0.9740\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4125   0.0875   0.0625   0.0125\n",
      "Detection Prevalence   0.5000   0.2500   0.2125   0.0375\n",
      "Balanced Accuracy      0.8008   0.5381   0.6201   0.6537\n",
      "Fold 5 Accuracy: 0.575 \n",
      "Fold 5 AUC: 0.825195537152059 \n",
      "Fold 5 Overall Sensitivity: 0.460015527950311 \n",
      "Fold 5 Overall Specificity: 0.846318671009073 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual  Predicted Rounded_Predicted\n",
      "1       3  5.6155943                 6\n",
      "2       3  4.0363002                 4\n",
      "3       3  4.1455917                 4\n",
      "4       3  4.7778405                 5\n",
      "5       3  7.9727164                 8\n",
      "6       3  3.8558618                 4\n",
      "7       5  3.5166357                 4\n",
      "8       5  6.2679077                 6\n",
      "9       4  1.9261785                 2\n",
      "10      3  4.0420389                 4\n",
      "11      7  6.1012947                 6\n",
      "12      3  2.0494991                 2\n",
      "13      7  6.2935642                 6\n",
      "14      3  2.3687542                 2\n",
      "15      3  3.9505528                 4\n",
      "16      8  5.3450015                 5\n",
      "17      3  2.8698498                 3\n",
      "18      5  5.5359250                 6\n",
      "19      3  3.3855442                 3\n",
      "20      3  2.4297831                 2\n",
      "21      3  3.8938048                 4\n",
      "22      3  5.9044053                 6\n",
      "23      5  3.8557621                 4\n",
      "24      3  2.1570590                 2\n",
      "25      6  9.7340015                10\n",
      "26      6  5.8504338                 6\n",
      "27      5  4.9883399                 5\n",
      "28      5  6.0255860                 6\n",
      "29      5  4.4427867                 4\n",
      "30      3  3.5663507                 4\n",
      "31      3  3.9406538                 4\n",
      "32      4  4.3579669                 4\n",
      "33      7  5.1832850                 5\n",
      "34      3  4.2466526                 4\n",
      "35      3  0.9242188                 1\n",
      "36      5  4.0048758                 4\n",
      "37      5  6.5398086                 7\n",
      "38      7  7.4686109                 7\n",
      "39     10 10.8936112                11\n",
      "40     10  6.9749861                 7\n",
      "41      3  3.0450789                 3\n",
      "42      3  4.0805542                 4\n",
      "43      8  6.2238774                 6\n",
      "44      3  4.9069858                 5\n",
      "45      7  8.2034397                 8\n",
      "46      3  1.3532698                 1\n",
      "47      7  4.5221501                 5\n",
      "48      6  7.1173251                 7\n",
      "49     11  7.5366016                 8\n",
      "50      3  2.9590849                 3\n",
      "51      6  8.7628529                 9\n",
      "52      6  7.2406076                 7\n",
      "53      3  4.7492357                 5\n",
      "54      3  3.6496560                 4\n",
      "55      9  9.2487301                 9\n",
      "56      3  3.6937385                 4\n",
      "57      6  4.2175525                 4\n",
      "58      3  4.3260021                 4\n",
      "59      5  4.4160146                 4\n",
      "60      5  7.4766846                 7\n",
      "61      3  1.9243254                 2\n",
      "62      3  2.3471886                 2\n",
      "63      3  2.1576244                 2\n",
      "64      9  7.1417007                 7\n",
      "65      6  7.1464649                 7\n",
      "66      5  4.1932089                 4\n",
      "67      7  9.8321557                10\n",
      "68      3  3.2984376                 3\n",
      "69      4  6.8634995                 7\n",
      "70      3  2.2512165                 2\n",
      "71      5  4.7439589                 5\n",
      "72      6  6.5784638                 7\n",
      "73      8  7.0105991                 7\n",
      "74      6  5.9319869                 6\n",
      "75      3  5.4907194                 5\n",
      "76      3  5.8662265                 6\n",
      "77      3  3.6458192                 4\n",
      "78      3  3.3244095                 3\n",
      "79      5  7.9662871                 8\n",
      "80      3  3.2812495                 3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 156  32   6   1\n",
      "         2  48  51  26   4\n",
      "         3   6  34  28  11\n",
      "         4   0   1   1   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5823          \n",
      "                 95% CI : (0.5327, 0.6307)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.004209        \n",
      "                                          \n",
      "                  Kappa : 0.3384          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.017364        \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7429   0.4322   0.4590 0.111111\n",
      "Specificity            0.8020   0.7301   0.8526 0.994859\n",
      "Pos Pred Value         0.8000   0.3953   0.3544 0.500000\n",
      "Neg Pred Value         0.7453   0.7590   0.8994 0.960298\n",
      "Prevalence             0.5160   0.2899   0.1499 0.044226\n",
      "Detection Rate         0.3833   0.1253   0.0688 0.004914\n",
      "Detection Prevalence   0.4791   0.3170   0.1941 0.009828\n",
      "Balanced Accuracy      0.7724   0.5812   0.6558 0.552985\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.582299 \n",
      "Average AUC: 0.7689489 \n",
      "Average Sensitivity: 0.4363135 \n",
      "Average Specificity: 0.8449417 \n",
      "Average Pearson Correlation Coefficient Across All Folds: 0.6421485 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_pearson_correlations <- numeric(k)  # Store Pearson correlation coefficients\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 75)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson's correlation coefficient between actual and predicted values\n",
    "    pearson_correlation <- cor(testing_set$child_pugh_score, predictions, method = \"pearson\")\n",
    "    all_pearson_correlations[i] <- pearson_correlation\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation Coefficient:\", pearson_correlation, \"\\n\"))\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    \n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation Coefficient Across All Folds:\", mean(all_pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0c479",
   "metadata": {},
   "source": [
    "# 75 Markers Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a7d317",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 10  3  1\n",
      "         2  4  8  5  0\n",
      "         3  0  6  4  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6098          \n",
      "                 95% CI : (0.4957, 0.7156)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.0483          \n",
      "                                          \n",
      "                  Kappa : 0.34            \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048  0.33333  0.33333  0.00000\n",
      "Specificity            0.6500  0.84483  0.87143  1.00000\n",
      "Pos Pred Value         0.7308  0.47059  0.30769      NaN\n",
      "Neg Pred Value         0.8667  0.75385  0.88406  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4634  0.09756  0.04878  0.00000\n",
      "Detection Prevalence   0.6341  0.20732  0.15854  0.00000\n",
      "Balanced Accuracy      0.7774  0.58908  0.60238  0.50000\n",
      "Fold 1 Accuracy: 0.609756097560976 \n",
      "Fold 1 AUC: 0.735615079365079 \n",
      "Fold 1 Overall Sensitivity: 0.392857142857143 \n",
      "Fold 1 Overall Specificity: 0.841564039408867 \n",
      "Fold 1 Pearson Correlation between Actual vs Predicted: 0.624765936064906 \n",
      "Fold 1 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6 8.0697794                   8\n",
      "2       3 2.7786745                   2\n",
      "3       3 3.2857264                   3\n",
      "4       5 4.7956086                   4\n",
      "5       5 7.3686693                   7\n",
      "6       4 6.7834815                   6\n",
      "7       3 5.0864882                   5\n",
      "8       5 5.3719157                   5\n",
      "9       5 4.8820279                   4\n",
      "10      3 3.8871188                   3\n",
      "11      3 3.6135780                   3\n",
      "12      3 3.4970945                   3\n",
      "13      5 4.6556981                   4\n",
      "14     10 8.4426236                   8\n",
      "15      3 1.1841944                   1\n",
      "16      3 1.8375331                   1\n",
      "17      6 7.5691402                   7\n",
      "18      3 3.9740860                   3\n",
      "19      3 2.9419078                   2\n",
      "20      3 3.5515829                   3\n",
      "21      3 4.5640652                   4\n",
      "22      3 3.6488367                   3\n",
      "23      3 1.5707666                   1\n",
      "24      8 6.9008824                   6\n",
      "25      3 3.2079074                   3\n",
      "26      3 3.6912080                   3\n",
      "27      3 4.3499411                   4\n",
      "28      3 3.7632943                   3\n",
      "29      3 3.6222662                   3\n",
      "30      3 4.2417196                   4\n",
      "31      5 1.7761164                   1\n",
      "32      3 4.1829979                   4\n",
      "33      3 2.4018381                   2\n",
      "34      3 2.3471801                   2\n",
      "35      5 8.2994763                   8\n",
      "36      3 6.2883740                   6\n",
      "37      6 6.2869091                   6\n",
      "38      6 6.5559107                   6\n",
      "39      3 1.7866863                   1\n",
      "40      5 3.2612543                   3\n",
      "41      5 5.9806455                   5\n",
      "42      6 3.0996181                   3\n",
      "43      3 2.8979074                   2\n",
      "44      7 4.2415820                   4\n",
      "45      7 4.0024018                   4\n",
      "46      3 4.5726004                   4\n",
      "47     11 7.3633478                   7\n",
      "48      9 6.3416157                   6\n",
      "49      3 6.0115850                   6\n",
      "50      3 0.9709145                   0\n",
      "51      3 3.1701329                   3\n",
      "52      8 7.3864140                   7\n",
      "53      6 1.8262684                   1\n",
      "54      6 6.3651338                   6\n",
      "55      5 6.0633045                   6\n",
      "56      5 2.9086413                   2\n",
      "57      7 8.4598525                   8\n",
      "58      3 4.0221939                   4\n",
      "59      3 2.5756289                   2\n",
      "60      9 5.0358017                   5\n",
      "61      9 5.5604188                   5\n",
      "62      9 6.9306635                   6\n",
      "63      3 3.7993427                   3\n",
      "64      9 7.0729242                   7\n",
      "65     10 4.4543754                   4\n",
      "66      6 7.4635975                   7\n",
      "67      3 2.2093835                   2\n",
      "68      5 6.5308893                   6\n",
      "69      5 2.4032968                   2\n",
      "70      3 3.3830013                   3\n",
      "71      6 5.7729322                   5\n",
      "72      5 7.3089809                   7\n",
      "73      8 7.0222691                   7\n",
      "74      5 4.7178893                   4\n",
      "75      3 4.7015022                   4\n",
      "76      3 2.5816772                   2\n",
      "77      7 4.8038331                   4\n",
      "78      3 3.5693628                   3\n",
      "79      3 4.6962592                   4\n",
      "80     11 8.2385228                   8\n",
      "81      3 1.9207347                   1\n",
      "82      3 4.0462403                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35  8  1  0\n",
      "         2  6 10  4  1\n",
      "         3  1  6  8  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6463         \n",
      "                 95% CI : (0.533, 0.7488)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.009787       \n",
      "                                         \n",
      "                  Kappa : 0.4271         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.4167  0.61538  0.00000\n",
      "Specificity            0.7750   0.8103  0.86957  1.00000\n",
      "Pos Pred Value         0.7955   0.4762  0.47059      NaN\n",
      "Neg Pred Value         0.8158   0.7705  0.92308  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4268   0.1220  0.09756  0.00000\n",
      "Detection Prevalence   0.5366   0.2561  0.20732  0.00000\n",
      "Balanced Accuracy      0.8042   0.6135  0.74247  0.50000\n",
      "Fold 2 Accuracy: 0.646341463414634 \n",
      "Fold 2 AUC: 0.777001933251933 \n",
      "Fold 2 Overall Sensitivity: 0.466346153846154 \n",
      "Fold 2 Overall Specificity: 0.863727511244378 \n",
      "Fold 2 Pearson Correlation between Actual vs Predicted: 0.660304912619265 \n",
      "Fold 2 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  4.458205                   4\n",
      "2       4  5.334694                   5\n",
      "3       3  4.921855                   4\n",
      "4       3  4.039519                   4\n",
      "5       3  3.120676                   3\n",
      "6       3  2.571854                   2\n",
      "7       5  3.067107                   3\n",
      "8       3  4.561400                   4\n",
      "9       5  7.628589                   7\n",
      "10      4  3.191030                   3\n",
      "11      3  1.292245                   1\n",
      "12      3  7.711550                   7\n",
      "13      3  4.142265                   4\n",
      "14      6  7.201847                   7\n",
      "15     12  7.157880                   7\n",
      "16      6  7.227609                   7\n",
      "17      3  5.726460                   5\n",
      "18      5  5.962631                   5\n",
      "19      3  5.724384                   5\n",
      "20      3  4.891296                   4\n",
      "21      3  2.004038                   2\n",
      "22      3  2.127501                   2\n",
      "23      3  2.953482                   2\n",
      "24      3  4.821969                   4\n",
      "25      3  4.462626                   4\n",
      "26      3  6.252708                   6\n",
      "27      3  3.424267                   3\n",
      "28      3  3.785883                   3\n",
      "29      5  4.575194                   4\n",
      "30      4  5.975731                   5\n",
      "31      3  4.659000                   4\n",
      "32      3  2.611433                   2\n",
      "33      3  5.221441                   5\n",
      "34      3  3.788682                   3\n",
      "35      8  8.908240                   8\n",
      "36      6  8.018099                   8\n",
      "37      5  4.558829                   4\n",
      "38      3  4.443721                   4\n",
      "39      9  7.692079                   7\n",
      "40     12  8.138765                   8\n",
      "41      5  5.919330                   5\n",
      "42      7  5.597360                   5\n",
      "43      6  7.284312                   7\n",
      "44      5  4.247621                   4\n",
      "45      8  8.172492                   8\n",
      "46      5  3.242370                   3\n",
      "47      7  7.976268                   7\n",
      "48      5  4.981418                   4\n",
      "49      5  7.999308                   7\n",
      "50      3  3.636866                   3\n",
      "51      5  6.642237                   6\n",
      "52      8  6.537899                   6\n",
      "53      3  3.097576                   3\n",
      "54      5  5.810172                   5\n",
      "55      6  6.567246                   6\n",
      "56      6  6.524717                   6\n",
      "57      5  5.322646                   5\n",
      "58      6  5.767663                   5\n",
      "59      5  3.740165                   3\n",
      "60     11  5.051104                   5\n",
      "61      3  4.374665                   4\n",
      "62      3  1.225872                   1\n",
      "63      5  4.754037                   4\n",
      "64      3  2.100836                   2\n",
      "65      3  4.101240                   4\n",
      "66      7  6.791798                   6\n",
      "67      3  1.879400                   1\n",
      "68      5  5.190741                   5\n",
      "69      8  7.360333                   7\n",
      "70      8  7.709388                   7\n",
      "71      3  4.555493                   4\n",
      "72      3  3.789440                   3\n",
      "73      7  7.399100                   7\n",
      "74      3  3.594364                   3\n",
      "75      7  4.335601                   4\n",
      "76      3  3.827791                   3\n",
      "77      3  3.901913                   3\n",
      "78      3  4.056320                   4\n",
      "79      3  1.592288                   1\n",
      "80      5  5.532418                   5\n",
      "81      8  5.282355                   5\n",
      "82      8  9.483899                   9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 11  4  0\n",
      "         2  4 12  7  3\n",
      "         3  0  1  1  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.622           \n",
      "                 95% CI : (0.5081, 0.7268)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.02971         \n",
      "                                          \n",
      "                  Kappa : 0.3377          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.5000  0.08333  0.00000\n",
      "Specificity            0.6250   0.7586  0.97143  1.00000\n",
      "Pos Pred Value         0.7170   0.4615  0.33333      NaN\n",
      "Neg Pred Value         0.8621   0.7857  0.86076  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4634   0.1463  0.01220  0.00000\n",
      "Detection Prevalence   0.6463   0.3171  0.03659  0.00000\n",
      "Balanced Accuracy      0.7649   0.6293  0.52738  0.50000\n",
      "Fold 3 Accuracy: 0.621951219512195 \n",
      "Fold 3 AUC: 0.755787037037037 \n",
      "Fold 3 Overall Sensitivity: 0.37202380952381 \n",
      "Fold 3 Overall Specificity: 0.838762315270936 \n",
      "Fold 3 Pearson Correlation between Actual vs Predicted: 0.535289434230383 \n",
      "Fold 3 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       6  3.189813                   3\n",
      "2       3  4.179587                   4\n",
      "3       3  2.912235                   2\n",
      "4       3  6.177202                   6\n",
      "5       3  3.630890                   3\n",
      "6       7  7.512751                   7\n",
      "7       5  3.949669                   3\n",
      "8       3  4.485274                   4\n",
      "9       3  5.590345                   5\n",
      "10      3  4.861490                   4\n",
      "11      4  3.766701                   3\n",
      "12      3  3.809042                   3\n",
      "13      3  4.126887                   4\n",
      "14      3  1.585085                   1\n",
      "15      8  3.633041                   3\n",
      "16      3  1.760951                   1\n",
      "17      3  5.490607                   5\n",
      "18      5  5.235897                   5\n",
      "19      3  3.167136                   3\n",
      "20      7  6.042591                   6\n",
      "21      3  2.081194                   2\n",
      "22      3  3.053378                   3\n",
      "23      6  5.660289                   5\n",
      "24      8  5.419416                   5\n",
      "25      3  3.244100                   3\n",
      "26     10  5.161082                   5\n",
      "27      3  2.203081                   2\n",
      "28      3  3.823217                   3\n",
      "29      3  4.504367                   4\n",
      "30      3  2.499372                   2\n",
      "31      3  1.714429                   1\n",
      "32      6  5.589360                   5\n",
      "33      6  5.765780                   5\n",
      "34      7  4.834114                   4\n",
      "35      5  7.554079                   7\n",
      "36      3  3.644201                   3\n",
      "37      5  2.689045                   2\n",
      "38      5  1.659974                   1\n",
      "39      3  2.927570                   2\n",
      "40      6  4.881213                   4\n",
      "41      3  4.211612                   4\n",
      "42      8  4.902882                   4\n",
      "43      5  3.911632                   3\n",
      "44      5  5.298162                   5\n",
      "45     12  6.128266                   6\n",
      "46      5  6.515736                   6\n",
      "47      6  4.002676                   4\n",
      "48      5  5.818871                   5\n",
      "49      6  5.304544                   5\n",
      "50      3  3.221199                   3\n",
      "51      5  4.375411                   4\n",
      "52      5  4.024592                   4\n",
      "53      3  4.992192                   4\n",
      "54      7  5.648585                   5\n",
      "55      8  6.228296                   6\n",
      "56      3  3.582764                   3\n",
      "57      3  3.656478                   3\n",
      "58      5  6.998501                   6\n",
      "59      7  5.255712                   5\n",
      "60     10  7.275623                   7\n",
      "61      5  5.108560                   5\n",
      "62      3  4.864699                   4\n",
      "63      3  4.793990                   4\n",
      "64      5  4.009760                   4\n",
      "65      5  5.011618                   5\n",
      "66      3  4.635219                   4\n",
      "67      5  5.036241                   5\n",
      "68      3  5.198010                   5\n",
      "69      8  5.636440                   5\n",
      "70      3  4.620708                   4\n",
      "71      9  6.410240                   6\n",
      "72      3  3.711706                   3\n",
      "73      3  3.236628                   3\n",
      "74      3  2.422770                   2\n",
      "75      3  4.015903                   4\n",
      "76      3  3.206742                   3\n",
      "77      3  4.176389                   4\n",
      "78     10  5.976471                   5\n",
      "79      8  3.964355                   3\n",
      "80      3  4.092628                   4\n",
      "81      5  2.571327                   2\n",
      "82      3  1.916102                   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  9  2  0\n",
      "         2  8 10  6  0\n",
      "         3  0  4  4  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6049          \n",
      "                 95% CI : (0.4901, 0.7119)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.07373         \n",
      "                                          \n",
      "                  Kappa : 0.3492          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.4348  0.33333  0.25000\n",
      "Specificity            0.7179   0.7586  0.89855  1.00000\n",
      "Pos Pred Value         0.7556   0.4167  0.36364  1.00000\n",
      "Neg Pred Value         0.7778   0.7719  0.88571  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4198   0.1235  0.04938  0.01235\n",
      "Detection Prevalence   0.5556   0.2963  0.13580  0.01235\n",
      "Balanced Accuracy      0.7637   0.5967  0.61594  0.62500\n",
      "Fold 4 Accuracy: 0.604938271604938 \n",
      "Fold 4 AUC: 0.838322406257189 \n",
      "Fold 4 Overall Sensitivity: 0.456909937888199 \n",
      "Fold 4 Overall Specificity: 0.843780033060393 \n",
      "Fold 4 Pearson Correlation between Actual vs Predicted: 0.690801464129187 \n",
      "Fold 4 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual Predicted Truncated_Predicted\n",
      "1       3  3.668661                   3\n",
      "2       3  3.545038                   3\n",
      "3       3  4.672547                   4\n",
      "4       4  6.396081                   6\n",
      "5       3  4.088754                   4\n",
      "6       6  6.798816                   6\n",
      "7       3  3.182141                   3\n",
      "8       4  6.513118                   6\n",
      "9       3  5.285231                   5\n",
      "10      3  5.317005                   5\n",
      "11      3  3.968002                   3\n",
      "12      6  2.704159                   2\n",
      "13      3  2.902432                   2\n",
      "14      3  1.848719                   1\n",
      "15      6  5.660380                   5\n",
      "16      3  4.346520                   4\n",
      "17      5  7.435387                   7\n",
      "18      3  4.035137                   4\n",
      "19      3  1.379281                   1\n",
      "20      9  7.693066                   7\n",
      "21      3  4.252192                   4\n",
      "22      3  3.925958                   3\n",
      "23      3  3.107264                   3\n",
      "24      3  4.028056                   4\n",
      "25      5  5.037932                   5\n",
      "26      3  5.212664                   5\n",
      "27      3  4.835684                   4\n",
      "28      5  5.091413                   5\n",
      "29      5  3.630165                   3\n",
      "30      3  4.738300                   4\n",
      "31      3  6.619928                   6\n",
      "32      3  3.537221                   3\n",
      "33      8  6.373036                   6\n",
      "34      3  3.876572                   3\n",
      "35      3  3.152434                   3\n",
      "36      3  4.090869                   4\n",
      "37      3  3.167331                   3\n",
      "38      3  4.108587                   4\n",
      "39      5  5.381611                   5\n",
      "40      5  7.129293                   7\n",
      "41      6  5.937625                   5\n",
      "42      8  5.035066                   5\n",
      "43      9  5.805880                   5\n",
      "44      6  4.532779                   4\n",
      "45      9  6.612226                   6\n",
      "46      8  4.685313                   4\n",
      "47      3  3.385940                   3\n",
      "48      9  5.753813                   5\n",
      "49      7  7.863295                   7\n",
      "50     11  8.049442                   8\n",
      "51      8  6.787437                   6\n",
      "52      6  6.678453                   6\n",
      "53      5  4.886708                   4\n",
      "54      5  5.367138                   5\n",
      "55      3  4.258941                   4\n",
      "56      3  2.802308                   2\n",
      "57      3  4.446955                   4\n",
      "58      5  5.395853                   5\n",
      "59     12 10.691860                  10\n",
      "60      5  4.324976                   4\n",
      "61      5  8.664518                   8\n",
      "62      5  3.445195                   3\n",
      "63     10  7.789456                   7\n",
      "64      3  2.652221                   2\n",
      "65      5  1.568182                   1\n",
      "66      5  4.780863                   4\n",
      "67     11  8.576555                   8\n",
      "68      5  6.100956                   6\n",
      "69      3  6.136719                   6\n",
      "70      3  4.477719                   4\n",
      "71      3  3.679594                   3\n",
      "72      3  5.714840                   5\n",
      "73      3  4.365009                   4\n",
      "74      3  4.811726                   4\n",
      "75      6  4.773755                   4\n",
      "76      3  3.489135                   3\n",
      "77      7  4.414616                   4\n",
      "78      9  7.976021                   7\n",
      "79      9  7.684382                   7\n",
      "80      6  7.276724                   7\n",
      "81      3  4.532416                   4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36  9  1  0\n",
      "         2  5  7  5  1\n",
      "         3  1  7  6  1\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.625           \n",
      "                 95% CI : (0.5096, 0.7308)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.04591         \n",
      "                                          \n",
      "                  Kappa : 0.38            \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.3043   0.5000   0.3333\n",
      "Specificity            0.7368   0.8070   0.8676   1.0000\n",
      "Pos Pred Value         0.7826   0.3889   0.4000   1.0000\n",
      "Neg Pred Value         0.8235   0.7419   0.9077   0.9747\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4500   0.0875   0.0750   0.0125\n",
      "Detection Prevalence   0.5750   0.2250   0.1875   0.0125\n",
      "Balanced Accuracy      0.7970   0.5557   0.6838   0.6667\n",
      "Fold 5 Accuracy: 0.625 \n",
      "Fold 5 AUC: 0.795282666206579 \n",
      "Fold 5 Overall Sensitivity: 0.498706004140787 \n",
      "Fold 5 Overall Specificity: 0.852876676986584 \n",
      "Fold 5 Pearson Correlation between Actual vs Predicted: 0.699580675664661 \n",
      "Fold 5 Actual vs Predicted vs Rounded Predicted values:\n",
      "   Actual  Predicted Truncated_Predicted\n",
      "1       3  5.6155943                   5\n",
      "2       3  4.0363002                   4\n",
      "3       3  4.1455917                   4\n",
      "4       3  4.7778405                   4\n",
      "5       3  7.9727164                   7\n",
      "6       3  3.8558618                   3\n",
      "7       5  3.5166357                   3\n",
      "8       5  6.2679077                   6\n",
      "9       4  1.9261785                   1\n",
      "10      3  4.0420389                   4\n",
      "11      7  6.1012947                   6\n",
      "12      3  2.0494991                   2\n",
      "13      7  6.2935642                   6\n",
      "14      3  2.3687542                   2\n",
      "15      3  3.9505528                   3\n",
      "16      8  5.3450015                   5\n",
      "17      3  2.8698498                   2\n",
      "18      5  5.5359250                   5\n",
      "19      3  3.3855442                   3\n",
      "20      3  2.4297831                   2\n",
      "21      3  3.8938048                   3\n",
      "22      3  5.9044053                   5\n",
      "23      5  3.8557621                   3\n",
      "24      3  2.1570590                   2\n",
      "25      6  9.7340015                   9\n",
      "26      6  5.8504338                   5\n",
      "27      5  4.9883399                   4\n",
      "28      5  6.0255860                   6\n",
      "29      5  4.4427867                   4\n",
      "30      3  3.5663507                   3\n",
      "31      3  3.9406538                   3\n",
      "32      4  4.3579669                   4\n",
      "33      7  5.1832850                   5\n",
      "34      3  4.2466526                   4\n",
      "35      3  0.9242188                   0\n",
      "36      5  4.0048758                   4\n",
      "37      5  6.5398086                   6\n",
      "38      7  7.4686109                   7\n",
      "39     10 10.8936112                  10\n",
      "40     10  6.9749861                   6\n",
      "41      3  3.0450789                   3\n",
      "42      3  4.0805542                   4\n",
      "43      8  6.2238774                   6\n",
      "44      3  4.9069858                   4\n",
      "45      7  8.2034397                   8\n",
      "46      3  1.3532698                   1\n",
      "47      7  4.5221501                   4\n",
      "48      6  7.1173251                   7\n",
      "49     11  7.5366016                   7\n",
      "50      3  2.9590849                   2\n",
      "51      6  8.7628529                   8\n",
      "52      6  7.2406076                   7\n",
      "53      3  4.7492357                   4\n",
      "54      3  3.6496560                   3\n",
      "55      9  9.2487301                   9\n",
      "56      3  3.6937385                   3\n",
      "57      6  4.2175525                   4\n",
      "58      3  4.3260021                   4\n",
      "59      5  4.4160146                   4\n",
      "60      5  7.4766846                   7\n",
      "61      3  1.9243254                   1\n",
      "62      3  2.3471886                   2\n",
      "63      3  2.1576244                   2\n",
      "64      9  7.1417007                   7\n",
      "65      6  7.1464649                   7\n",
      "66      5  4.1932089                   4\n",
      "67      7  9.8321557                   9\n",
      "68      3  3.2984376                   3\n",
      "69      4  6.8634995                   6\n",
      "70      3  2.2512165                   2\n",
      "71      5  4.7439589                   4\n",
      "72      6  6.5784638                   6\n",
      "73      8  7.0105991                   7\n",
      "74      6  5.9319869                   5\n",
      "75      3  5.4907194                   5\n",
      "76      3  5.8662265                   5\n",
      "77      3  3.6458192                   3\n",
      "78      3  3.3244095                   3\n",
      "79      5  7.9662871                   7\n",
      "80      3  3.2812495                   3\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 181  47  11   1\n",
      "         2  27  47  27   5\n",
      "         3   2  24  23  10\n",
      "         4   0   0   0   2\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6216          \n",
      "                 95% CI : (0.5725, 0.6689)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 1.121e-05       \n",
      "                                          \n",
      "                  Kappa : 0.3676          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.0001019       \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8619   0.3983  0.37705 0.111111\n",
      "Specificity            0.7005   0.7958  0.89595 1.000000\n",
      "Pos Pred Value         0.7542   0.4434  0.38983 1.000000\n",
      "Neg Pred Value         0.8263   0.7641  0.89080 0.960494\n",
      "Prevalence             0.5160   0.2899  0.14988 0.044226\n",
      "Detection Rate         0.4447   0.1155  0.05651 0.004914\n",
      "Detection Prevalence   0.5897   0.2604  0.14496 0.004914\n",
      "Balanced Accuracy      0.7812   0.5971  0.63650 0.555556\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6215974 \n",
      "Average AUC: 0.7804018 \n",
      "Average Sensitivity: 0.4373686 \n",
      "Average Specificity: 0.8481421 \n",
      "Average Pearson Correlation: 0.6421485 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "   # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 75)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    \n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Truncate the predicted values before classification\n",
    "    truncated_predictions <- floor(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(truncated_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Truncated_Predicted = truncated_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ab95f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7a5e40e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘xgboost’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    slice\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:XVector’:\n",
      "\n",
      "    slice\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:IRanges’:\n",
      "\n",
      "    slice\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj): [20:11:43] src/objective/multiclass_obj.cu:123: SoftmaxMultiClassObj: label must be in [0, num_class).\nStack trace:\n  [bt] (0) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(+0x6b926) [0x2b193c976926]\n  [bt] (1) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(+0xcfaef) [0x2b193c9daaef]\n  [bt] (2) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(+0x236d71) [0x2b193cb41d71]\n  [bt] (3) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x5f) [0x2b193cb9f15f]\n  [bt] (4) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3d) [0x2b193c97018d]\n  [bt] (5) /u/local/apps/R/4.3.0/gcc10.2.0_intel2022.1.1/lib64/R/lib/libR.so(+0x11794e) [0x2b15db3b694e]\n  [bt] (6) /u/loc\n",
     "output_type": "error",
     "traceback": [
      "Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj): [20:11:43] src/objective/multiclass_obj.cu:123: SoftmaxMultiClassObj: label must be in [0, num_class).\nStack trace:\n  [bt] (0) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(+0x6b926) [0x2b193c976926]\n  [bt] (1) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(+0xcfaef) [0x2b193c9daaef]\n  [bt] (2) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(+0x236d71) [0x2b193cb41d71]\n  [bt] (3) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x5f) [0x2b193cb9f15f]\n  [bt] (4) /u/home/c/ctang04/R/x86_64-pc-linux-gnu-library-RH7/4.3.0/gcc10.2.0_intel2022.1.1/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3d) [0x2b193c97018d]\n  [bt] (5) /u/local/apps/R/4.3.0/gcc10.2.0_intel2022.1.1/lib64/R/lib/libR.so(+0x11794e) [0x2b15db3b694e]\n  [bt] (6) /u/loc\nTraceback:\n",
      "1. xgboost(params = params, data = dtrain, nrounds = 100, verbose = 0)",
      "2. xgb.train(params, dtrain, nrounds, watchlist, verbose = verbose, \n .     print_every_n = print_every_n, early_stopping_rounds = early_stopping_rounds, \n .     maximize = maximize, save_period = save_period, save_name = save_name, \n .     xgb_model = xgb_model, callbacks = callbacks, ...)",
      "3. xgb.iter.update(bst$handle, dtrain, iteration - 1, obj)"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(ggplot2)\n",
    "library(xgboost)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "pearson_correlations <- numeric(k)   # Store Pearson correlations for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the XGBoost model\n",
    "    dtrain <- xgb.DMatrix(data = as.matrix(training_set_filtered[, -1]), label = training_set_filtered$child_pugh_score)\n",
    "    dtest <- xgb.DMatrix(data = as.matrix(testing_set_filtered[, -1]), label = testing_set_filtered$child_pugh_score)\n",
    "\n",
    "    # Set XGBoost parameters (you may need to tune these based on your data)\n",
    "    params <- list(\n",
    "      objective = \"multi:softmax\",  # Multi-class classification\n",
    "      num_class = 4,                # 4 classes for Child-Pugh score\n",
    "      eval_metric = \"mlogloss\",\n",
    "      max_depth = 6,\n",
    "      eta = 0.3\n",
    "    )\n",
    "\n",
    "    xgb_model <- xgboost(params = params, data = dtrain, nrounds = 100, verbose = 0)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(xgb_model, dtest)\n",
    "\n",
    "    # Calculate Pearson correlation between actual and predicted values\n",
    "    pearson_corr <- cor(testing_set$child_pugh_score, predictions)\n",
    "    pearson_correlations[i] <- pearson_corr\n",
    "\n",
    "    # Round the predicted values before classification\n",
    "    rounded_predictions <- round(predictions)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(rounded_predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual, predicted, and rounded predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices],\n",
    "      Rounded_Predicted = rounded_predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Pearson Correlation between Actual vs Predicted:\", pearson_corr, \"\\n\"))\n",
    "\n",
    "    # Print actual, predicted, and rounded predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted vs Rounded Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "\n",
    "    # Plot Actual vs Predicted values\n",
    "    ggplot(data = all_predictions_vs_actual[[i]], aes(x = Actual, y = Predicted)) +\n",
    "      geom_point(color = \"darkred\") +\n",
    "      geom_smooth(method = \"lm\", color = \"lightcoral\", se = FALSE) +\n",
    "      ggtitle(paste(\"Fold\", i, \"Actual vs Predicted Values\")) +\n",
    "      xlab(\"Actual Values\") +\n",
    "      ylab(\"Predicted Values\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(hjust = 0.5))\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "cat(\"Average Pearson Correlation:\", mean(pearson_correlations), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4f1ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "320119af",
   "metadata": {},
   "source": [
    "# 0.5 Cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc39c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  7  2  0\n",
      "         2  9  8  5  0\n",
      "         3  2  9  7  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.561          \n",
      "                 95% CI : (0.447, 0.6704)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2199         \n",
      "                                         \n",
      "                  Kappa : 0.3031         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381  0.33333  0.50000  0.00000\n",
      "Specificity            0.7750  0.75862  0.80882  1.00000\n",
      "Pos Pred Value         0.7750  0.36364  0.35000      NaN\n",
      "Neg Pred Value         0.7381  0.73333  0.88710  0.97561\n",
      "Prevalence             0.5122  0.29268  0.17073  0.02439\n",
      "Detection Rate         0.3780  0.09756  0.08537  0.00000\n",
      "Detection Prevalence   0.4878  0.26829  0.24390  0.00000\n",
      "Balanced Accuracy      0.7565  0.54598  0.65441  0.50000\n",
      "Fold 1 Accuracy: 0.560975609756098 \n",
      "Fold 1 AUC: 0.787934618291761 \n",
      "Fold 1 Overall Sensitivity: 0.392857142857143 \n",
      "Fold 1 Overall Specificity: 0.835611054766734 \n",
      "Fold 1 Actual vs Predicted values:\n",
      "   Actual Predicted\n",
      "1       6  8.586404\n",
      "2       3  2.238850\n",
      "3       3  4.546350\n",
      "4       5  5.171442\n",
      "5       5  7.095618\n",
      "6       4  6.954157\n",
      "7       3  4.838752\n",
      "8       5  6.169339\n",
      "9       5  4.908542\n",
      "10      3  3.866497\n",
      "11      3  3.266065\n",
      "12      3  3.055424\n",
      "13      5  4.556442\n",
      "14     10  8.090156\n",
      "15      3  3.068069\n",
      "16      3  2.265068\n",
      "17      6  7.987336\n",
      "18      3  4.114586\n",
      "19      3  3.405042\n",
      "20      3  3.191306\n",
      "21      3  4.466749\n",
      "22      3  3.038775\n",
      "23      3  1.092566\n",
      "24      8  5.676230\n",
      "25      3  3.483192\n",
      "26      3  3.770425\n",
      "27      3  5.011883\n",
      "28      3  3.755493\n",
      "29      3  3.143744\n",
      "30      3  4.250678\n",
      "31      5  1.865943\n",
      "32      3  5.110044\n",
      "33      3  1.948479\n",
      "34      3  2.387621\n",
      "35      5  7.703621\n",
      "36      3  7.627630\n",
      "37      6  5.819073\n",
      "38      6  7.377768\n",
      "39      3  1.657355\n",
      "40      5  3.469572\n",
      "41      5  6.138008\n",
      "42      6  2.961655\n",
      "43      3  1.263724\n",
      "44      7  5.094025\n",
      "45      7  2.717765\n",
      "46      3  5.014835\n",
      "47     11  6.884615\n",
      "48      9  6.386113\n",
      "49      3  5.901272\n",
      "50      3  0.942686\n",
      "51      3  3.676910\n",
      "52      8  8.123538\n",
      "53      6  2.076911\n",
      "54      6  6.890224\n",
      "55      5  6.448359\n",
      "56      5  3.095921\n",
      "57      7  7.869065\n",
      "58      3  3.497546\n",
      "59      3  2.587999\n",
      "60      9  4.559393\n",
      "61      9  7.308262\n",
      "62      9  6.819449\n",
      "63      3  4.098310\n",
      "64      9  7.152524\n",
      "65     10  4.457664\n",
      "66      6  6.572504\n",
      "67      3  2.397025\n",
      "68      5  6.651625\n",
      "69      5  2.197169\n",
      "70      3  4.152147\n",
      "71      6  6.319567\n",
      "72      5  7.818113\n",
      "73      8  6.919941\n",
      "74      5  3.288485\n",
      "75      3  5.377862\n",
      "76      3  3.394215\n",
      "77      7  5.770257\n",
      "78      3  2.665008\n",
      "79      3  4.851649\n",
      "80     11  8.702833\n",
      "81      3  2.450381\n",
      "82      3  4.689708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 29  6  0  0\n",
      "         2 10  9  6  1\n",
      "         3  3  9  7  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5488         \n",
      "                 95% CI : (0.4349, 0.659)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2907         \n",
      "                                         \n",
      "                  Kappa : 0.3036         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6905   0.3750  0.53846  0.00000\n",
      "Specificity            0.8500   0.7069  0.79710  1.00000\n",
      "Pos Pred Value         0.8286   0.3462  0.33333      NaN\n",
      "Neg Pred Value         0.7234   0.7321  0.90164  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.3537   0.1098  0.08537  0.00000\n",
      "Detection Prevalence   0.4268   0.3171  0.25610  0.00000\n",
      "Balanced Accuracy      0.7702   0.5409  0.66778  0.50000\n",
      "Fold 2 Accuracy: 0.548780487804878 \n",
      "Fold 2 AUC: 0.741344881969882 \n",
      "Fold 2 Overall Sensitivity: 0.400984432234432 \n",
      "Fold 2 Overall Specificity: 0.838499500249875 \n",
      "Fold 2 Actual vs Predicted values:\n",
      "   Actual Predicted\n",
      "1       3 4.5146078\n",
      "2       4 6.5764557\n",
      "3       3 4.3547911\n",
      "4       3 3.6126528\n",
      "5       3 3.1822957\n",
      "6       3 2.4790334\n",
      "7       5 3.8549240\n",
      "8       3 4.0952849\n",
      "9       5 7.8454826\n",
      "10      4 3.0421551\n",
      "11      3 2.4874124\n",
      "12      3 7.3111463\n",
      "13      3 5.1260709\n",
      "14      6 6.5915240\n",
      "15     12 7.1777228\n",
      "16      6 6.9909734\n",
      "17      3 6.4162452\n",
      "18      5 5.4761906\n",
      "19      3 4.5145026\n",
      "20      3 5.1344562\n",
      "21      3 2.1543875\n",
      "22      3 1.7454996\n",
      "23      3 4.1347221\n",
      "24      3 3.5327682\n",
      "25      3 4.6657006\n",
      "26      3 5.2510298\n",
      "27      3 2.0169221\n",
      "28      3 3.9319198\n",
      "29      5 4.0551993\n",
      "30      4 8.5180038\n",
      "31      3 4.9308689\n",
      "32      3 2.6336685\n",
      "33      3 6.2549305\n",
      "34      3 3.4313455\n",
      "35      8 9.5316380\n",
      "36      6 8.6012766\n",
      "37      5 5.0966883\n",
      "38      3 4.1197139\n",
      "39      9 7.0972086\n",
      "40     12 8.3011047\n",
      "41      5 6.2593110\n",
      "42      7 6.2029282\n",
      "43      6 7.5729416\n",
      "44      5 3.9680996\n",
      "45      8 7.8730234\n",
      "46      5 1.9825954\n",
      "47      7 8.3276036\n",
      "48      5 5.4651266\n",
      "49      5 8.6925477\n",
      "50      3 4.1334983\n",
      "51      5 6.1417759\n",
      "52      8 6.3959460\n",
      "53      3 2.1716187\n",
      "54      5 6.1364556\n",
      "55      6 6.7552340\n",
      "56      6 6.6952572\n",
      "57      5 6.8816225\n",
      "58      6 6.2416929\n",
      "59      5 3.1793913\n",
      "60     11 5.1880504\n",
      "61      3 4.5190545\n",
      "62      3 1.6603956\n",
      "63      5 4.3563132\n",
      "64      3 2.0322043\n",
      "65      3 4.4804004\n",
      "66      7 6.2447147\n",
      "67      3 1.4879344\n",
      "68      5 5.5519188\n",
      "69      8 7.4637686\n",
      "70      8 7.6589666\n",
      "71      3 4.4718586\n",
      "72      3 3.3614448\n",
      "73      7 6.3274582\n",
      "74      3 3.0815158\n",
      "75      7 5.1266766\n",
      "76      3 4.3435823\n",
      "77      3 4.0671098\n",
      "78      3 3.2930426\n",
      "79      3 0.4839879\n",
      "80      5 5.1431591\n",
      "81      8 5.3196648\n",
      "82      8 8.9210976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32 11  3  0\n",
      "         2 10  9  8  1\n",
      "         3  0  4  4  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.5488         \n",
      "                 95% CI : (0.4349, 0.659)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.2907         \n",
      "                                         \n",
      "                  Kappa : 0.2415         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3750  0.26667   0.0000\n",
      "Specificity            0.6500   0.6724  0.94030   1.0000\n",
      "Pos Pred Value         0.6957   0.3214  0.50000      NaN\n",
      "Neg Pred Value         0.7222   0.7222  0.85135   0.9878\n",
      "Prevalence             0.5122   0.2927  0.18293   0.0122\n",
      "Detection Rate         0.3902   0.1098  0.04878   0.0000\n",
      "Detection Prevalence   0.5610   0.3415  0.09756   0.0000\n",
      "Balanced Accuracy      0.7060   0.5237  0.60348   0.5000\n",
      "Fold 3 Accuracy: 0.548780487804878 \n",
      "Fold 3 AUC: 0.685416666666667 \n",
      "Fold 3 Overall Sensitivity: 0.350892857142857 \n",
      "Fold 3 Overall Specificity: 0.815678075141534 \n",
      "Fold 3 Actual vs Predicted values:\n",
      "   Actual   Predicted\n",
      "1       6  1.53612000\n",
      "2       3  5.91794724\n",
      "3       3  2.48202340\n",
      "4       3  3.93087133\n",
      "5       3  5.27295866\n",
      "6       7  9.06291592\n",
      "7       5  4.04973943\n",
      "8       3  3.87102097\n",
      "9       3  6.26525546\n",
      "10      3  3.72160259\n",
      "11      4  3.93258938\n",
      "12      3  4.07982976\n",
      "13      3  4.46999884\n",
      "14      3 -0.06942872\n",
      "15      8  3.33364575\n",
      "16      3  0.64319724\n",
      "17      3  5.63598142\n",
      "18      5  4.78657479\n",
      "19      3  2.31141101\n",
      "20      7  5.26517194\n",
      "21      3  2.80168552\n",
      "22      3  2.40047014\n",
      "23      6  6.18106992\n",
      "24      8  4.60809576\n",
      "25      3  1.99428260\n",
      "26     10  6.20201558\n",
      "27      3  1.74433091\n",
      "28      3  3.21519635\n",
      "29      3  3.96750444\n",
      "30      3  2.29398033\n",
      "31      3  1.86419054\n",
      "32      6  6.78882902\n",
      "33      6  6.32569766\n",
      "34      7  4.00684707\n",
      "35      5  7.96836775\n",
      "36      3  3.53115944\n",
      "37      5  3.12045493\n",
      "38      5  1.96484848\n",
      "39      3  1.45387563\n",
      "40      6  6.16093802\n",
      "41      3  4.81000996\n",
      "42      8  4.55035707\n",
      "43      5  4.65617304\n",
      "44      5  5.79768265\n",
      "45     12  6.23325302\n",
      "46      5  6.56090919\n",
      "47      6  4.24203156\n",
      "48      5  5.87554763\n",
      "49      6  4.41295968\n",
      "50      3  2.69438560\n",
      "51      5  4.11446285\n",
      "52      5  4.26630966\n",
      "53      3  4.54690171\n",
      "54      7  5.26677694\n",
      "55      8  6.28226803\n",
      "56      3  2.45147769\n",
      "57      3  3.16078603\n",
      "58      5  7.17266743\n",
      "59      7  4.87200139\n",
      "60     10  8.00127483\n",
      "61      5  5.30459771\n",
      "62      3  4.01060016\n",
      "63      3  4.90969726\n",
      "64      5  1.34865828\n",
      "65      5  6.06763476\n",
      "66      3  3.94423731\n",
      "67      5  4.10746653\n",
      "68      3  2.30511496\n",
      "69      8  5.52865083\n",
      "70      3  4.66139887\n",
      "71      9  7.54797224\n",
      "72      3  4.94671851\n",
      "73      3  3.19875872\n",
      "74      3  2.65315050\n",
      "75      3  3.52362356\n",
      "76      3  2.38705521\n",
      "77      3  5.06873818\n",
      "78     10  6.65472719\n",
      "79      8  4.35008255\n",
      "80      3  3.02151817\n",
      "81      5  3.36434542\n",
      "82      3  3.21818583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 28  7  0  0\n",
      "         2 10  9  7  0\n",
      "         3  4  6  6  2\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5432          \n",
      "                 95% CI : (0.4287, 0.6544)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.3699          \n",
      "                                          \n",
      "                  Kappa : 0.2953          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6667   0.3913  0.46154  0.33333\n",
      "Specificity            0.8205   0.7069  0.82353  0.98718\n",
      "Pos Pred Value         0.8000   0.3462  0.33333  0.50000\n",
      "Neg Pred Value         0.6957   0.7455  0.88889  0.97468\n",
      "Prevalence             0.5185   0.2840  0.16049  0.03704\n",
      "Detection Rate         0.3457   0.1111  0.07407  0.01235\n",
      "Detection Prevalence   0.4321   0.3210  0.22222  0.02469\n",
      "Balanced Accuracy      0.7436   0.5491  0.64253  0.66026\n",
      "Fold 4 Accuracy: 0.54320987654321 \n",
      "Fold 4 AUC: 0.817531100139796 \n",
      "Fold 4 Overall Sensitivity: 0.463210702341137 \n",
      "Fold 4 Overall Specificity: 0.834529567795288 \n",
      "Fold 4 Actual vs Predicted values:\n",
      "   Actual Predicted\n",
      "1       3  4.313330\n",
      "2       3  4.539652\n",
      "3       3  4.631728\n",
      "4       4  6.922639\n",
      "5       3  4.853542\n",
      "6       6  7.316025\n",
      "7       3  4.200451\n",
      "8       4  5.459959\n",
      "9       3  7.343430\n",
      "10      3  5.014862\n",
      "11      3  4.823576\n",
      "12      6  3.563204\n",
      "13      3  2.794440\n",
      "14      3  1.903975\n",
      "15      6  5.386734\n",
      "16      3  3.277842\n",
      "17      5  7.575279\n",
      "18      3  4.450365\n",
      "19      3  2.312539\n",
      "20      9  8.161538\n",
      "21      3  3.192185\n",
      "22      3  3.319337\n",
      "23      3  1.804455\n",
      "24      3  2.840388\n",
      "25      5  5.006015\n",
      "26      3  6.563637\n",
      "27      3  4.786956\n",
      "28      5  6.208354\n",
      "29      5  3.626387\n",
      "30      3  3.749369\n",
      "31      3  5.073185\n",
      "32      3  4.115786\n",
      "33      8  6.742738\n",
      "34      3  4.479225\n",
      "35      3  3.468131\n",
      "36      3  4.230307\n",
      "37      3  3.022107\n",
      "38      3  3.433587\n",
      "39      5  5.077259\n",
      "40      5  8.167677\n",
      "41      6  5.092182\n",
      "42      8  4.535127\n",
      "43      9  5.494329\n",
      "44      6  5.514030\n",
      "45      9  6.428347\n",
      "46      8  6.275537\n",
      "47      3  3.753218\n",
      "48      9  5.259137\n",
      "49      7  8.478213\n",
      "50     11  9.187367\n",
      "51      8  6.264543\n",
      "52      6  7.924666\n",
      "53      5  4.275661\n",
      "54      5  5.023058\n",
      "55      3  3.967514\n",
      "56      3  4.278246\n",
      "57      3  5.384906\n",
      "58      5  6.920882\n",
      "59     12 10.504137\n",
      "60      5  3.108298\n",
      "61      5 10.692194\n",
      "62      5  4.321451\n",
      "63     10  8.294447\n",
      "64      3  3.197316\n",
      "65      5  1.425402\n",
      "66      5  4.759407\n",
      "67     11  8.509217\n",
      "68      5  5.709180\n",
      "69      3  6.544010\n",
      "70      3  4.498166\n",
      "71      3  2.654166\n",
      "72      3  5.115556\n",
      "73      3  3.284409\n",
      "74      3  4.170017\n",
      "75      6  3.123366\n",
      "76      3  3.536088\n",
      "77      7  4.605254\n",
      "78      9  7.907459\n",
      "79      9  8.356541\n",
      "80      6  8.319669\n",
      "81      3  3.273879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 32  5  1  0\n",
      "         2  8  9  5  0\n",
      "         3  2  9  7  1\n",
      "         4  0  0  1  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6            \n",
      "                 95% CI : (0.4844, 0.708)\n",
      "    No Information Rate : 0.525          \n",
      "    P-Value [Acc > NIR] : 0.1088         \n",
      "                                         \n",
      "                  Kappa : 0.3649         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7619   0.3913   0.5000   0.0000\n",
      "Specificity            0.8421   0.7719   0.8182   0.9873\n",
      "Pos Pred Value         0.8421   0.4091   0.3684   0.0000\n",
      "Neg Pred Value         0.7619   0.7586   0.8852   0.9873\n",
      "Prevalence             0.5250   0.2875   0.1750   0.0125\n",
      "Detection Rate         0.4000   0.1125   0.0875   0.0000\n",
      "Detection Prevalence   0.4750   0.2750   0.2375   0.0125\n",
      "Balanced Accuracy      0.8020   0.5816   0.6591   0.4937\n",
      "Fold 5 Accuracy: 0.6 \n",
      "Fold 5 AUC: 0.79684265010352 \n",
      "Fold 5 Overall Sensitivity: 0.413302277432712 \n",
      "Fold 5 Overall Specificity: 0.854889669513254 \n",
      "Fold 5 Actual vs Predicted values:\n",
      "   Actual  Predicted\n",
      "1       3  6.0115671\n",
      "2       3  4.2220313\n",
      "3       3  3.4624508\n",
      "4       3  4.8273377\n",
      "5       3  6.6950447\n",
      "6       3  4.9932358\n",
      "7       5  3.3798212\n",
      "8       5  7.8492929\n",
      "9       4  1.4274760\n",
      "10      3  4.2539116\n",
      "11      7  6.3450229\n",
      "12      3  1.7784142\n",
      "13      7  5.6315160\n",
      "14      3  1.7698328\n",
      "15      3  4.3419335\n",
      "16      8  5.9877573\n",
      "17      3  3.7428955\n",
      "18      5  6.1681106\n",
      "19      3  3.6087007\n",
      "20      3  2.1326893\n",
      "21      3  4.0553741\n",
      "22      3  6.0022415\n",
      "23      5  4.2935866\n",
      "24      3  2.8221544\n",
      "25      6  9.6837416\n",
      "26      6  5.8116334\n",
      "27      5  5.3786511\n",
      "28      5  7.0191108\n",
      "29      5  4.6116597\n",
      "30      3  4.0204475\n",
      "31      3  3.2525126\n",
      "32      4  4.8602159\n",
      "33      7  6.1229599\n",
      "34      3  4.1678666\n",
      "35      3  0.3336202\n",
      "36      5  3.7793994\n",
      "37      5  6.4689309\n",
      "38      7  6.9882447\n",
      "39     10 10.5387493\n",
      "40     10  7.9948061\n",
      "41      3  2.7037541\n",
      "42      3  4.4223567\n",
      "43      8  5.9970951\n",
      "44      3  4.7547385\n",
      "45      7  8.1305426\n",
      "46      3  1.2217954\n",
      "47      7  3.7256029\n",
      "48      6  7.0107744\n",
      "49     11  7.8140070\n",
      "50      3  2.8376885\n",
      "51      6  8.7901585\n",
      "52      6  7.8504806\n",
      "53      3  4.1279211\n",
      "54      3  4.1153760\n",
      "55      9  9.3570316\n",
      "56      3  4.3100292\n",
      "57      6  3.8833026\n",
      "58      3  4.0667764\n",
      "59      5  4.8926072\n",
      "60      5  7.4539351\n",
      "61      3  1.5530113\n",
      "62      3  3.2177917\n",
      "63      3  1.9202919\n",
      "64      9  7.6271800\n",
      "65      6  7.2606816\n",
      "66      5  3.9262762\n",
      "67      7  9.3916246\n",
      "68      3  2.7408500\n",
      "69      4  6.9367162\n",
      "70      3  0.6534420\n",
      "71      5  4.5901975\n",
      "72      6  4.6197961\n",
      "73      8  7.4147288\n",
      "74      6  6.4384773\n",
      "75      3  5.3699272\n",
      "76      3  5.9096986\n",
      "77      3  3.2802899\n",
      "78      3  3.4181398\n",
      "79      5  7.5032575\n",
      "80      3  3.6748416\n",
      "Overall Confusion Matrix across all folds:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 152  36   6   0\n",
      "         2  47  44  31   2\n",
      "         3  11  37  31   7\n",
      "         4   0   1   1   1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5602          \n",
      "                 95% CI : (0.5105, 0.6091)\n",
      "    No Information Rate : 0.516           \n",
      "    P-Value [Acc > NIR] : 0.04114         \n",
      "                                          \n",
      "                  Kappa : 0.3016          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7238   0.3729  0.44928 0.100000\n",
      "Specificity            0.7868   0.7232  0.83728 0.994962\n",
      "Pos Pred Value         0.7835   0.3548  0.36047 0.333333\n",
      "Neg Pred Value         0.7277   0.7385  0.88162 0.977723\n",
      "Prevalence             0.5160   0.2899  0.16953 0.024570\n",
      "Detection Rate         0.3735   0.1081  0.07617 0.002457\n",
      "Detection Prevalence   0.4767   0.3047  0.21130 0.007371\n",
      "Balanced Accuracy      0.7553   0.5480  0.64328 0.547481\n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.5603493 \n",
      "Average AUC: 0.765814 \n",
      "Average Sensitivity: 0.4042495 \n",
      "Average Specificity: 0.8358416 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 4.5) {\n",
    "    return(1)\n",
    "  } else if (value >= 4.5 & value < 6.5) {\n",
    "    return(2)\n",
    "  } else if (value >= 6.5 & value < 10.5) {\n",
    "    return(3)\n",
    "  } else if (value >= 10.5 & value < 15.5) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_predictions_vs_actual <- list()  # Store actual vs predicted for each fold\n",
    "\n",
    "# Initialize vectors to store all predictions and actual values across folds\n",
    "all_classified_predictions <- factor()  # Empty factor to accumulate predictions\n",
    "all_classified_actuals <- factor()      # Empty factor to accumulate actual values\n",
    "\n",
    "# Create k-fold cross-validation indices\n",
    "folds <- createFolds(combined_data_clean$child_pugh_score, k = k, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Store actual vs predicted values\n",
    "    all_predictions_vs_actual[[i]] <- data.frame(\n",
    "      Actual = testing_set$child_pugh_score[valid_indices], \n",
    "      Predicted = predictions[valid_indices]\n",
    "    )\n",
    "\n",
    "    # Accumulate classified predictions and actuals for overall confusion matrix\n",
    "    all_classified_predictions <- c(all_classified_predictions, classified_predictions)\n",
    "    all_classified_actuals <- c(all_classified_actuals, classified_actuals)\n",
    "\n",
    "    # Create a confusion matrix for this fold\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "    \n",
    "    # Print actual vs predicted values\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted values:\\n\"))\n",
    "    print(all_predictions_vs_actual[[i]])\n",
    "}\n",
    "\n",
    "# Create overall confusion matrix\n",
    "overall_conf_matrix <- confusionMatrix(all_classified_predictions, all_classified_actuals)\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix across all folds:\\n\")\n",
    "print(overall_conf_matrix)\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70864c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Actual vs Predicted Values:\n",
      "                 PlasmaAlias Actual Predicted\n",
      "1          plasma-1969-P9-LV      6  8.586404\n",
      "2           plasma-2741-P9-N      3  2.238850\n",
      "3           plasma-2248-P9-N      3  4.546350\n",
      "4           plasma-2549-P9-N      5  5.171442\n",
      "5           plasma-2349-P9-N      5  7.095618\n",
      "6          plasma-3620-P9-BD      4  6.954157\n",
      "7           plasma-2072-P9-N      3  4.838752\n",
      "8          plasma-3629-P9-LV      5  6.169339\n",
      "9           plasma-3013-P9-N      5  4.908542\n",
      "10          plasma-637-P9-CH      3  3.866497\n",
      "11          plasma-641-P9-CH      3  3.266065\n",
      "12          plasma-646-P9-CH      3  3.055424\n",
      "13          plasma-654-P9-CH      5  4.556442\n",
      "14          plasma-657-P9-CH     10  8.090156\n",
      "15          plasma-658-P9-CH      3  3.068069\n",
      "16         plasma-2026-P9-CH      3  2.265068\n",
      "17          plasma-770-P9-LV      6  7.987336\n",
      "18         plasma-1215-P9-CH      3  4.114586\n",
      "19         plasma-4000-P9-CH      3  3.405042\n",
      "20         plasma-2411-P9-CH      3  3.191306\n",
      "21         plasma-1422-P9-CH      3  4.466749\n",
      "22         plasma-1623-P9-CH      3  3.038775\n",
      "23         plasma-1993-P9-CH      3  1.092566\n",
      "24          plasma-754-P9-CH      8  5.676230\n",
      "25         plasma-1423-P9-CH      3  3.483192\n",
      "26         plasma-1428-P9-CH      3  3.770425\n",
      "27         plasma-1630-P9-CH      3  5.011883\n",
      "28         plasma-1429-P9-CH      3  3.755493\n",
      "29         plasma-1631-P9-CH      3  3.143744\n",
      "30         plasma-1633-P9-CH      3  4.250678\n",
      "31         plasma-1634-P9-CH      5  1.865943\n",
      "32         plasma-1432-P9-CH      3  5.110044\n",
      "33         plasma-2083-P9-CH      3  1.948479\n",
      "34         plasma-1637-P9-CH      3  2.387621\n",
      "35          plasma-764-P9-CH      5  7.703621\n",
      "36         plasma-3922-P9-CH      3  7.627630\n",
      "37          plasma-738-P9-CH      6  5.819073\n",
      "38          plasma-737-P9-LV      6  7.377768\n",
      "39         plasma-1217-P9-CH      3  1.657355\n",
      "40         plasma-1137-P9-CH      5  3.469572\n",
      "41         plasma-1644-P9-LV      5  6.138008\n",
      "42         plasma-1066-P9-CH      6  2.961655\n",
      "43         plasma-3132-P9-CH      3  1.263724\n",
      "44         plasma-1138-P9-CH      7  5.094025\n",
      "45          plasma-842-P9-LV      7  2.717765\n",
      "46         plasma-3321-P9-CH      3  5.014835\n",
      "47         plasma-1126-P9-CH     11  6.884615\n",
      "48         plasma-1653-P9-CH      9  6.386113\n",
      "49         plasma-1655-P9-LV      3  5.901272\n",
      "50         plasma-1656-P9-CH      3  0.942686\n",
      "51         plasma-1329-P9-CH      3  3.676910\n",
      "52         plasma-1336-P9-LV      8  8.123538\n",
      "53         plasma-1192-P9-CH      6  2.076911\n",
      "54         plasma-2432-P9-CH      6  6.890224\n",
      "55         plasma-1657-P9-LV      5  6.448359\n",
      "56         plasma-2435-P9-CH      5  3.095921\n",
      "57         plasma-1659-P9-LV      7  7.869065\n",
      "58         plasma-2920-P9-CH      3  3.497546\n",
      "59         plasma-2337-P9-CH      3  2.587999\n",
      "60         plasma-2541-P9-CH      9  4.559393\n",
      "61         plasma-2038-P9-CH      9  7.308262\n",
      "62         plasma-1964-P9-CH      9  6.819449\n",
      "63         plasma-2510-P9-CH      3  4.098310\n",
      "64         plasma-2533-P9-CH      9  7.152524\n",
      "65         plasma-1663-P9-CH     10  4.457664\n",
      "66         plasma-2501-P9-CH      6  6.572504\n",
      "67         plasma-1980-P9-CH      3  2.397025\n",
      "68         plasma-2025-P9-CH      5  6.651625\n",
      "69         plasma-2389-P9-CH      5  2.197169\n",
      "70         plasma-2495-P9-CH      3  4.152147\n",
      "71         plasma-1879-P9-CH      6  6.319567\n",
      "72         plasma-2082-P9-CH      5  7.818113\n",
      "73         plasma-1665-P9-LV      8  6.919941\n",
      "74         plasma-2018-P9-CH      5  3.288485\n",
      "75         plasma-2297-P9-CH      3  5.377862\n",
      "76         plasma-2536-P9-CH      3  3.394215\n",
      "77         plasma-3087-P9-CH      7  5.770257\n",
      "78         plasma-3964-P9-CH      3  2.665008\n",
      "79    plasma-2576-5day-P9-CH      3  4.851649\n",
      "80         plasma-3915-P9-CH     11  8.702833\n",
      "81 plasma-2568-r1-4day-P9-CH      3  2.450381\n",
      "82          plasma-3917-P9-N      3  4.689708\n",
      "Fold 1 Pearson correlation: R = 0.59 | R² = 0.34 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Actual vs Predicted Values:\n",
      "                    PlasmaAlias Actual Predicted\n",
      "1             plasma-2300-P9-LV      3 4.5146078\n",
      "2              plasma-2104-P9-N      4 6.5764557\n",
      "3             plasma-3627-P9-LV      3 4.3547911\n",
      "4              plasma-3134-P9-N      3 3.6126528\n",
      "5              plasma-2370-P9-N      3 3.1822957\n",
      "6              plasma-3427-P9-N      3 2.4790334\n",
      "7              plasma-2273-P9-N      5 3.8549240\n",
      "8              plasma-1970-P9-N      3 4.0952849\n",
      "9             plasma-3608-P9-LV      5 7.8454826\n",
      "10             plasma-2757-P9-N      4 3.0421551\n",
      "11             plasma-4027-P9-N      3 2.4874124\n",
      "12             plasma-3050-P9-N      3 7.3111463\n",
      "13    plasma-2198-t7-5day-P9-CH      3 5.1260709\n",
      "14             plasma-746-P9-CH      6 6.5915240\n",
      "15             plasma-749-P9-CH     12 7.1777228\n",
      "16             plasma-750-P9-CH      6 6.9909734\n",
      "17            plasma-1624-P9-LV      3 6.4162452\n",
      "18            plasma-1265-P9-LV      5 5.4761906\n",
      "19            plasma-1424-P9-CH      3 4.5145026\n",
      "20            plasma-1425-P9-CH      3 5.1344562\n",
      "21            plasma-1628-P9-CH      3 2.1543875\n",
      "22            plasma-1427-P9-CH      3 1.7454996\n",
      "23            plasma-2102-P9-CH      3 4.1347221\n",
      "24            plasma-1244-P9-CH      3 3.5327682\n",
      "25            plasma-1635-P9-CH      3 4.6657006\n",
      "26            plasma-1431-P9-CH      3 5.2510298\n",
      "27            plasma-3112-P9-CH      3 2.0169221\n",
      "28            plasma-2932-P9-CH      3 3.9319198\n",
      "29             plasma-739-P9-CH      5 4.0551993\n",
      "30            plasma-1224-P9-CH      4 8.5180038\n",
      "31            plasma-1246-P9-CH      3 4.9308689\n",
      "32            plasma-1646-P9-CH      3 2.6336685\n",
      "33            plasma-1647-P9-CH      3 6.2549305\n",
      "34            plasma-1437-P9-CH      3 3.4313455\n",
      "35            plasma-1208-P9-CH      8 9.5316380\n",
      "36            plasma-2057-P9-CH      6 8.6012766\n",
      "37            plasma-1441-P9-CH      5 5.0966883\n",
      "38            plasma-2260-P9-CH      3 4.1197139\n",
      "39            plasma-2740-P9-CH      9 7.0972086\n",
      "40            plasma-1206-P9-CH     12 8.3011047\n",
      "41            plasma-1124-P9-CH      5 6.2593110\n",
      "42            plasma-2486-P9-CH      7 6.2029282\n",
      "43            plasma-1650-P9-CH      6 7.5729416\n",
      "44            plasma-2791-P9-CH      5 3.9680996\n",
      "45            plasma-2823-P9-CH      8 7.8730234\n",
      "46            plasma-2227-P9-CH      5 1.9825954\n",
      "47            plasma-1207-P9-CH      7 8.3276036\n",
      "48            plasma-1203-P9-CH      5 5.4651266\n",
      "49            plasma-2483-P9-CH      5 8.6925477\n",
      "50            plasma-2241-P9-CH      3 4.1334983\n",
      "51            plasma-1446-P9-CH      5 6.1417759\n",
      "52            plasma-2286-P9-CH      8 6.3959460\n",
      "53            plasma-3059-P9-CH      3 2.1716187\n",
      "54            plasma-2503-P9-CH      5 6.1364556\n",
      "55            plasma-2490-P9-CH      6 6.7552340\n",
      "56            plasma-2462-P9-CH      6 6.6952572\n",
      "57            plasma-2381-P9-CH      5 6.8816225\n",
      "58            plasma-2017-P9-CH      6 6.2416929\n",
      "59            plasma-2078-P9-CH      5 3.1793913\n",
      "60            plasma-1662-P9-CH     11 5.1880504\n",
      "61            plasma-2202-P9-CH      3 4.5190545\n",
      "62 plasma-2564-r1-t4-4day-P9-CH      3 1.6603956\n",
      "63            plasma-2459-P9-CH      5 4.3563132\n",
      "64            plasma-2787-P9-CH      3 2.0322043\n",
      "65            plasma-2779-P9-CH      3 4.4804004\n",
      "66            plasma-2074-P9-CH      7 6.2447147\n",
      "67            plasma-1956-P9-CH      3 1.4879344\n",
      "68            plasma-2488-P9-CH      5 5.5519188\n",
      "69            plasma-2294-P9-CH      8 7.4637686\n",
      "70            plasma-2451-P9-CH      8 7.6589666\n",
      "71            plasma-2215-P9-CH      3 4.4718586\n",
      "72            plasma-2796-P9-CH      3 3.3614448\n",
      "73            plasma-2332-P9-CH      7 6.3274582\n",
      "74            plasma-2277-P9-CH      3 3.0815158\n",
      "75            plasma-2458-P9-CH      7 5.1266766\n",
      "76            plasma-2785-P9-CH      3 4.3435823\n",
      "77            plasma-3409-P9-CH      3 4.0671098\n",
      "78            plasma-2010-P9-CH      3 3.2930426\n",
      "79            plasma-2738-P9-CH      3 0.4839879\n",
      "80            plasma-2304-P9-CH      5 5.1431591\n",
      "81       plasma-2574-5day-P9-CH      8 5.3196648\n",
      "82            plasma-3448-P9-CH      8 8.9210976\n",
      "Fold 2 Pearson correlation: R = 0.63 | R² = 0.39 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Actual vs Predicted Values:\n",
      "                    PlasmaAlias Actual   Predicted\n",
      "1              plasma-3931-P9-N      6  1.53612000\n",
      "2              plasma-2357-P9-N      3  5.91794724\n",
      "3              plasma-2873-P9-N      3  2.48202340\n",
      "4             plasma-3038-P9-LV      3  3.93087133\n",
      "5              plasma-2091-P9-N      3  5.27295866\n",
      "6              plasma-2733-P9-N      7  9.06291592\n",
      "7              plasma-2743-P9-N      5  4.04973943\n",
      "8              plasma-1991-P9-N      3  3.87102097\n",
      "9              plasma-2222-P9-N      3  6.26525546\n",
      "10             plasma-2794-P9-N      3  3.72160259\n",
      "11             plasma-3077-P9-N      4  3.93258938\n",
      "12            plasma-3599-P9-LV      3  4.07982976\n",
      "13            plasma-3637-P9-LV      3  4.46999884\n",
      "14  plasma-636-r1-t7-4day-P9-CH      3 -0.06942872\n",
      "15             plasma-640-P9-CH      8  3.33364575\n",
      "16  plasma-650-r1-t7-4day-P9-CH      3  0.64319724\n",
      "17            plasma-3967-P9-CH      3  5.63598142\n",
      "18            plasma-1279-P9-LV      5  4.78657479\n",
      "19            plasma-1621-P9-CH      3  2.31141101\n",
      "20             plasma-752-P9-CH      7  5.26517194\n",
      "21            plasma-1627-P9-CH      3  2.80168552\n",
      "22            plasma-2505-P9-CH      3  2.40047014\n",
      "23             plasma-743-P9-CH      6  6.18106992\n",
      "24             plasma-742-P9-CH      8  4.60809576\n",
      "25            plasma-1636-P9-CH      3  1.99428260\n",
      "26             plasma-757-P9-CH     10  6.20201558\n",
      "27            plasma-3869-P9-CH      3  1.74433091\n",
      "28            plasma-1433-P9-CH      3  3.21519635\n",
      "29            plasma-2751-P9-CH      3  3.96750444\n",
      "30            plasma-2047-P9-CH      3  2.29398033\n",
      "31            plasma-1310-P9-CH      3  1.86419054\n",
      "32            plasma-1434-P9-CH      6  6.78882902\n",
      "33             plasma-766-P9-CH      6  6.32569766\n",
      "34             plasma-740-P9-CH      7  4.00684707\n",
      "35             plasma-767-P9-CH      5  7.96836775\n",
      "36            plasma-1638-P9-CH      3  3.53115944\n",
      "37             plasma-769-P9-LV      5  3.12045493\n",
      "38            plasma-1645-P9-LV      5  1.96484848\n",
      "39            plasma-1990-P9-CH      3  1.45387563\n",
      "40            plasma-1125-P9-CH      6  6.16093802\n",
      "41            plasma-2312-P9-CH      3  4.81000996\n",
      "42            plasma-1649-P9-CH      8  4.55035707\n",
      "43            plasma-1442-P9-CH      5  4.65617304\n",
      "44            plasma-1280-P9-CH      5  5.79768265\n",
      "45         plasma-1188-r1-P9-CH     12  6.23325302\n",
      "46            plasma-1443-P9-CH      5  6.56090919\n",
      "47            plasma-2427-P9-CH      6  4.24203156\n",
      "48            plasma-2477-P9-CH      5  5.87554763\n",
      "49            plasma-2041-P9-CH      6  4.41295968\n",
      "50            plasma-2748-P9-CH      3  2.69438560\n",
      "51            plasma-1123-P9-CH      5  4.11446285\n",
      "52            plasma-1958-P9-CH      5  4.26630966\n",
      "53            plasma-2907-P9-CH      3  4.54690171\n",
      "54         plasma-1139-r1-P9-CH      7  5.26677694\n",
      "55            plasma-2237-P9-CH      8  6.28226803\n",
      "56            plasma-1248-P9-CH      3  2.45147769\n",
      "57            plasma-1658-P9-CH      3  3.16078603\n",
      "58            plasma-2768-P9-CH      5  7.17266743\n",
      "59            plasma-2461-P9-CH      7  4.87200139\n",
      "60            plasma-2870-P9-CH     10  8.00127483\n",
      "61            plasma-1661-P9-LV      5  5.30459771\n",
      "62            plasma-2135-P9-CH      3  4.01060016\n",
      "63            plasma-2282-P9-CH      3  4.90969726\n",
      "64            plasma-1859-P9-CH      5  1.34865828\n",
      "65            plasma-2782-P9-CH      5  6.06763476\n",
      "66            plasma-2521-P9-CH      3  3.94423731\n",
      "67            plasma-2440-P9-CH      5  4.10746653\n",
      "68            plasma-2760-P9-CH      3  2.30511496\n",
      "69            plasma-1302-P9-CH      8  5.52865083\n",
      "70            plasma-2804-P9-CH      3  4.66139887\n",
      "71            plasma-2030-P9-CH      9  7.54797224\n",
      "72            plasma-2176-P9-CH      3  4.94671851\n",
      "73            plasma-1982-P9-CH      3  3.19875872\n",
      "74            plasma-2848-P9-CH      3  2.65315050\n",
      "75            plasma-2407-P9-CH      3  3.52362356\n",
      "76            plasma-2274-P9-CH      3  2.38705521\n",
      "77    plasma-2572-t2-6day-P9-CH      3  5.06873818\n",
      "78            plasma-2039-P9-CH     10  6.65472719\n",
      "79            plasma-2250-P9-CH      8  4.35008255\n",
      "80            plasma-2368-P9-CH      3  3.02151817\n",
      "81 plasma-2560-r1-t1-4day-P9-CH      5  3.36434542\n",
      "82       plasma-2577-5day-P9-CH      3  3.21818583\n",
      "Fold 3 Pearson correlation: R = 0.54 | R² = 0.29 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Actual vs Predicted Values:\n",
      "                    PlasmaAlias Actual Predicted\n",
      "1             plasma-2333-P9-LV      3  4.313330\n",
      "2             plasma-2270-P9-LV      3  4.539652\n",
      "3              plasma-1976-P9-N      3  4.631728\n",
      "4              plasma-1959-P9-N      4  6.922639\n",
      "5             plasma-2778-P9-PR      3  4.853542\n",
      "6              plasma-2090-P9-N      6  7.316025\n",
      "7              plasma-2296-P9-N      3  4.200451\n",
      "8              plasma-3956-P9-N      4  5.459959\n",
      "9              plasma-3880-P9-N      3  7.343430\n",
      "10             plasma-3901-P9-N      3  5.014862\n",
      "11            plasma-3462-P9-LV      3  4.823576\n",
      "12             plasma-3377-P9-N      6  3.563204\n",
      "13             plasma-642-P9-CH      3  2.794440\n",
      "14             plasma-644-P9-CH      3  1.903975\n",
      "15             plasma-655-P9-CH      6  5.386734\n",
      "16    plasma-2569-t7-5day-P9-CH      3  3.277842\n",
      "17             plasma-748-P9-CH      5  7.575279\n",
      "18            plasma-1622-P9-CH      3  4.450365\n",
      "19            plasma-2099-P9-CH      3  2.312539\n",
      "20             plasma-771-P9-LV      9  8.161538\n",
      "21            plasma-1327-P9-CH      3  3.192185\n",
      "22            plasma-1626-P9-CH      3  3.319337\n",
      "23            plasma-2970-P9-CH      3  1.804455\n",
      "24            plasma-2446-P9-CH      3  2.840388\n",
      "25             plasma-775-P9-LV      5  5.006015\n",
      "26            plasma-1430-P9-CH      3  6.563637\n",
      "27            plasma-1273-P9-CH      3  4.786956\n",
      "28            plasma-2050-P9-CH      5  6.208354\n",
      "29             plasma-780-P9-LV      5  3.626387\n",
      "30             plasma-762-P9-CH      3  3.749369\n",
      "31            plasma-1435-P9-CH      3  5.073185\n",
      "32            plasma-2208-P9-CH      3  4.115786\n",
      "33             plasma-768-P9-CH      8  6.742738\n",
      "34            plasma-1639-P9-CH      3  4.479225\n",
      "35            plasma-1640-P9-CH      3  3.468131\n",
      "36            plasma-1641-P9-CH      3  4.230307\n",
      "37            plasma-1642-P9-CH      3  3.022107\n",
      "38            plasma-1648-P9-CH      3  3.433587\n",
      "39            plasma-1140-P9-CH      5  5.077259\n",
      "40            plasma-2450-P9-CH      5  8.167677\n",
      "41            plasma-1981-P9-CH      6  5.092182\n",
      "42            plasma-1135-P9-CH      8  4.535127\n",
      "43         plasma-1189-r1-P9-CH      9  5.494329\n",
      "44            plasma-2008-P9-CH      6  5.514030\n",
      "45            plasma-1205-P9-CH      9  6.428347\n",
      "46            plasma-1652-P9-CH      8  6.275537\n",
      "47            plasma-2172-P9-CH      3  3.753218\n",
      "48            plasma-2831-P9-CH      9  5.259137\n",
      "49            plasma-1129-P9-CH      7  8.478213\n",
      "50            plasma-1204-P9-CH     11  9.187367\n",
      "51            plasma-2758-P9-CH      8  6.264543\n",
      "52            plasma-1660-P9-LV      6  7.924666\n",
      "53            plasma-2409-P9-CH      5  4.275661\n",
      "54            plasma-1880-P9-CH      5  5.023058\n",
      "55            plasma-2323-P9-CH      3  3.967514\n",
      "56            plasma-2485-P9-CH      3  4.278246\n",
      "57            plasma-2258-P9-CH      3  5.384906\n",
      "58            plasma-2419-P9-CH      5  6.920882\n",
      "59            plasma-2275-P9-CH     12 10.504137\n",
      "60            plasma-2480-P9-CH      5  3.108298\n",
      "61            plasma-2065-P9-CH      5 10.692194\n",
      "62            plasma-2431-P9-CH      5  4.321451\n",
      "63            plasma-2352-P9-CH     10  8.294447\n",
      "64            plasma-2321-P9-CH      3  3.197316\n",
      "65            plasma-2379-P9-CH      5  1.425402\n",
      "66 plasma-2565-r1-t3-4day-P9-CH      5  4.759407\n",
      "67            plasma-2405-P9-CH     11  8.509217\n",
      "68            plasma-2182-P9-CH      5  5.709180\n",
      "69            plasma-2493-P9-CH      3  6.544010\n",
      "70            plasma-2437-P9-CH      3  4.498166\n",
      "71            plasma-2160-P9-CH      3  2.654166\n",
      "72            plasma-2489-P9-CH      3  5.115556\n",
      "73            plasma-2367-P9-CH      3  3.284409\n",
      "74            plasma-2363-P9-CH      3  4.170017\n",
      "75            plasma-2106-P9-CH      6  3.123366\n",
      "76 plasma-2563-r1-t2-4day-P9-CH      3  3.536088\n",
      "77 plasma-2567-r1-t1-4day-P9-CH      7  4.605254\n",
      "78            plasma-3896-P9-CH      9  7.907459\n",
      "79            plasma-3944-P9-CH      9  8.356541\n",
      "80            plasma-3366-P9-CH      6  8.319669\n",
      "81       plasma-2575-5day-P9-CH      3  3.273879\n",
      "Fold 4 Pearson correlation: R = 0.65 | R² = 0.42 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Actual vs Predicted Values:\n",
      "                    PlasmaAlias Actual  Predicted\n",
      "1              plasma-2742-P9-N      3  6.0115671\n",
      "2              plasma-2800-P9-N      3  4.2220313\n",
      "3              plasma-2518-P9-N      3  3.4624508\n",
      "4             plasma-3390-P9-LV      3  4.8273377\n",
      "5             plasma-2188-P9-BD      3  6.6950447\n",
      "6              plasma-2441-P9-N      3  4.9932358\n",
      "7              plasma-2770-P9-N      5  3.3798212\n",
      "8              plasma-4036-P9-N      5  7.8492929\n",
      "9              plasma-3835-P9-N      4  1.4274760\n",
      "10            plasma-3450-P9-BC      3  4.2539116\n",
      "11             plasma-632-P9-CH      7  6.3450229\n",
      "12     plasma-649-t8-6day-P9-CH      3  1.7784142\n",
      "13             plasma-652-P9-CH      7  5.6315160\n",
      "14            plasma-3163-P9-CH      3  1.7698328\n",
      "15            plasma-2776-P9-CH      3  4.3419335\n",
      "16             plasma-753-P9-CH      8  5.9877573\n",
      "17            plasma-1625-P9-CH      3  3.7428955\n",
      "18  plasma-756-r1-t4-4day-P9-CH      5  6.1681106\n",
      "19 plasma-2566-r1-t7-4day-P9-CH      3  3.6087007\n",
      "20            plasma-1426-P9-CH      3  2.1326893\n",
      "21            plasma-1348-P9-CH      3  4.0553741\n",
      "22            plasma-1629-P9-CH      3  6.0022415\n",
      "23            plasma-1632-P9-LV      5  4.2935866\n",
      "24            plasma-2307-P9-CH      3  2.8221544\n",
      "25             plasma-759-P9-CH      6  9.6837416\n",
      "26             plasma-760-P9-CH      6  5.8116334\n",
      "27             plasma-763-P9-CH      5  5.3786511\n",
      "28             plasma-765-P9-CH      5  7.0191108\n",
      "29             plasma-735-P9-CH      5  4.6116597\n",
      "30            plasma-1436-P9-CH      3  4.0204475\n",
      "31            plasma-1643-P9-CH      3  3.2525126\n",
      "32            plasma-1236-P9-CH      4  4.8602159\n",
      "33            plasma-1127-P9-CH      7  6.1229599\n",
      "34            plasma-1438-P9-CH      3  4.1678666\n",
      "35            plasma-1439-P9-CH      3  0.3336202\n",
      "36            plasma-1440-P9-CH      5  3.7793994\n",
      "37            plasma-1136-P9-CH      5  6.4689309\n",
      "38            plasma-1069-P9-CH      7  6.9882447\n",
      "39            plasma-2156-P9-CH     10 10.5387493\n",
      "40            plasma-2517-P9-CH     10  7.9948061\n",
      "41            plasma-2802-P9-CH      3  2.7037541\n",
      "42            plasma-2544-P9-CH      3  4.4223567\n",
      "43            plasma-1202-P9-CH      8  5.9970951\n",
      "44            plasma-1651-P9-CH      3  4.7547385\n",
      "45            plasma-2511-P9-CH      7  8.1305426\n",
      "46            plasma-1445-P9-CH      3  1.2217954\n",
      "47            plasma-1869-P9-CH      7  3.7256029\n",
      "48            plasma-2221-P9-CH      6  7.0107744\n",
      "49            plasma-1654-P9-CH     11  7.8140070\n",
      "50            plasma-2246-P9-CH      3  2.8376885\n",
      "51            plasma-2256-P9-CH      6  8.7901585\n",
      "52            plasma-1448-P9-CH      6  7.8504806\n",
      "53            plasma-1965-P9-CH      3  4.1279211\n",
      "54            plasma-2173-P9-CH      3  4.1153760\n",
      "55            plasma-2285-P9-CH      9  9.3570316\n",
      "56            plasma-2212-P9-CH      3  4.3100292\n",
      "57            plasma-2143-P9-CH      6  3.8833026\n",
      "58            plasma-2898-P9-CH      3  4.0667764\n",
      "59            plasma-1664-P9-LV      5  4.8926072\n",
      "60            plasma-2096-P9-CH      5  7.4539351\n",
      "61            plasma-2391-P9-CH      3  1.5530113\n",
      "62 plasma-2559-r1-t4-4day-P9-CH      3  3.2177917\n",
      "63            plasma-2268-P9-CH      3  1.9202919\n",
      "64            plasma-2331-P9-CH      9  7.6271800\n",
      "65            plasma-2067-P9-CH      6  7.2606816\n",
      "66            plasma-2463-P9-CH      5  3.9262762\n",
      "67            plasma-2235-P9-CH      7  9.3916246\n",
      "68            plasma-2347-P9-CH      3  2.7408500\n",
      "69            plasma-1222-P9-LV      4  6.9367162\n",
      "70            plasma-2449-P9-CH      3  0.6534420\n",
      "71            plasma-2259-P9-CH      5  4.5901975\n",
      "72            plasma-2355-P9-CH      6  4.6197961\n",
      "73            plasma-2239-P9-CH      8  7.4147288\n",
      "74            plasma-2428-P9-CH      6  6.4384773\n",
      "75            plasma-2371-P9-CH      3  5.3699272\n",
      "76            plasma-3331-P9-CH      3  5.9096986\n",
      "77            plasma-2867-P9-CH      3  3.2802899\n",
      "78            plasma-2502-P9-CH      3  3.4181398\n",
      "79            plasma-2094-P9-CH      5  7.5032575\n",
      "80    plasma-2573-t2-5day-P9-CH      3  3.6748416\n",
      "Fold 5 Pearson correlation: R = 0.7 | R² = 0.49 \n",
      "Total Pearson correlation: R = 0.61 | R² = 0.37 \n",
      "Total Actual vs Predicted Values across all folds:\n",
      "    Actual   Predicted\n",
      "1        6  8.58640447\n",
      "2        3  2.23884954\n",
      "3        3  4.54635036\n",
      "4        5  5.17144154\n",
      "5        5  7.09561769\n",
      "6        4  6.95415699\n",
      "7        3  4.83875232\n",
      "8        5  6.16933878\n",
      "9        5  4.90854160\n",
      "10       3  3.86649713\n",
      "11       3  3.26606542\n",
      "12       3  3.05542400\n",
      "13       5  4.55644226\n",
      "14      10  8.09015566\n",
      "15       3  3.06806923\n",
      "16       3  2.26506772\n",
      "17       6  7.98733570\n",
      "18       3  4.11458591\n",
      "19       3  3.40504193\n",
      "20       3  3.19130566\n",
      "21       3  4.46674942\n",
      "22       3  3.03877508\n",
      "23       3  1.09256567\n",
      "24       8  5.67622991\n",
      "25       3  3.48319170\n",
      "26       3  3.77042491\n",
      "27       3  5.01188259\n",
      "28       3  3.75549335\n",
      "29       3  3.14374441\n",
      "30       3  4.25067839\n",
      "31       5  1.86594349\n",
      "32       3  5.11004404\n",
      "33       3  1.94847948\n",
      "34       3  2.38762115\n",
      "35       5  7.70362110\n",
      "36       3  7.62762991\n",
      "37       6  5.81907339\n",
      "38       6  7.37776778\n",
      "39       3  1.65735460\n",
      "40       5  3.46957204\n",
      "41       5  6.13800798\n",
      "42       6  2.96165512\n",
      "43       3  1.26372403\n",
      "44       7  5.09402489\n",
      "45       7  2.71776520\n",
      "46       3  5.01483460\n",
      "47      11  6.88461516\n",
      "48       9  6.38611345\n",
      "49       3  5.90127217\n",
      "50       3  0.94268595\n",
      "51       3  3.67691021\n",
      "52       8  8.12353815\n",
      "53       6  2.07691092\n",
      "54       6  6.89022361\n",
      "55       5  6.44835852\n",
      "56       5  3.09592069\n",
      "57       7  7.86906525\n",
      "58       3  3.49754593\n",
      "59       3  2.58799850\n",
      "60       9  4.55939329\n",
      "61       9  7.30826247\n",
      "62       9  6.81944935\n",
      "63       3  4.09831028\n",
      "64       9  7.15252390\n",
      "65      10  4.45766417\n",
      "66       6  6.57250368\n",
      "67       3  2.39702484\n",
      "68       5  6.65162549\n",
      "69       5  2.19716865\n",
      "70       3  4.15214684\n",
      "71       6  6.31956672\n",
      "72       5  7.81811307\n",
      "73       8  6.91994086\n",
      "74       5  3.28848541\n",
      "75       3  5.37786240\n",
      "76       3  3.39421454\n",
      "77       7  5.77025712\n",
      "78       3  2.66500782\n",
      "79       3  4.85164947\n",
      "80      11  8.70283252\n",
      "81       3  2.45038106\n",
      "82       3  4.68970758\n",
      "83       3  4.51460781\n",
      "84       4  6.57645571\n",
      "85       3  4.35479108\n",
      "86       3  3.61265282\n",
      "87       3  3.18229567\n",
      "88       3  2.47903340\n",
      "89       5  3.85492402\n",
      "90       3  4.09528493\n",
      "91       5  7.84548257\n",
      "92       4  3.04215506\n",
      "93       3  2.48741240\n",
      "94       3  7.31114630\n",
      "95       3  5.12607089\n",
      "96       6  6.59152397\n",
      "97      12  7.17772285\n",
      "98       6  6.99097337\n",
      "99       3  6.41624518\n",
      "100      5  5.47619057\n",
      "101      3  4.51450264\n",
      "102      3  5.13445618\n",
      "103      3  2.15438752\n",
      "104      3  1.74549962\n",
      "105      3  4.13472209\n",
      "106      3  3.53276816\n",
      "107      3  4.66570057\n",
      "108      3  5.25102977\n",
      "109      3  2.01692208\n",
      "110      3  3.93191980\n",
      "111      5  4.05519926\n",
      "112      4  8.51800379\n",
      "113      3  4.93086887\n",
      "114      3  2.63366847\n",
      "115      3  6.25493050\n",
      "116      3  3.43134553\n",
      "117      8  9.53163798\n",
      "118      6  8.60127663\n",
      "119      5  5.09668826\n",
      "120      3  4.11971388\n",
      "121      9  7.09720861\n",
      "122     12  8.30110473\n",
      "123      5  6.25931095\n",
      "124      7  6.20292825\n",
      "125      6  7.57294161\n",
      "126      5  3.96809965\n",
      "127      8  7.87302339\n",
      "128      5  1.98259542\n",
      "129      7  8.32760362\n",
      "130      5  5.46512656\n",
      "131      5  8.69254767\n",
      "132      3  4.13349826\n",
      "133      5  6.14177590\n",
      "134      8  6.39594599\n",
      "135      3  2.17161870\n",
      "136      5  6.13645562\n",
      "137      6  6.75523402\n",
      "138      6  6.69525721\n",
      "139      5  6.88162252\n",
      "140      6  6.24169290\n",
      "141      5  3.17939133\n",
      "142     11  5.18805045\n",
      "143      3  4.51905449\n",
      "144      3  1.66039558\n",
      "145      5  4.35631319\n",
      "146      3  2.03220428\n",
      "147      3  4.48040038\n",
      "148      7  6.24471471\n",
      "149      3  1.48793440\n",
      "150      5  5.55191881\n",
      "151      8  7.46376859\n",
      "152      8  7.65896663\n",
      "153      3  4.47185858\n",
      "154      3  3.36144478\n",
      "155      7  6.32745816\n",
      "156      3  3.08151575\n",
      "157      7  5.12667659\n",
      "158      3  4.34358233\n",
      "159      3  4.06710982\n",
      "160      3  3.29304265\n",
      "161      3  0.48398787\n",
      "162      5  5.14315907\n",
      "163      8  5.31966483\n",
      "164      8  8.92109765\n",
      "165      6  1.53612000\n",
      "166      3  5.91794724\n",
      "167      3  2.48202340\n",
      "168      3  3.93087133\n",
      "169      3  5.27295866\n",
      "170      7  9.06291592\n",
      "171      5  4.04973943\n",
      "172      3  3.87102097\n",
      "173      3  6.26525546\n",
      "174      3  3.72160259\n",
      "175      4  3.93258938\n",
      "176      3  4.07982976\n",
      "177      3  4.46999884\n",
      "178      3 -0.06942872\n",
      "179      8  3.33364575\n",
      "180      3  0.64319724\n",
      "181      3  5.63598142\n",
      "182      5  4.78657479\n",
      "183      3  2.31141101\n",
      "184      7  5.26517194\n",
      "185      3  2.80168552\n",
      "186      3  2.40047014\n",
      "187      6  6.18106992\n",
      "188      8  4.60809576\n",
      "189      3  1.99428260\n",
      "190     10  6.20201558\n",
      "191      3  1.74433091\n",
      "192      3  3.21519635\n",
      "193      3  3.96750444\n",
      "194      3  2.29398033\n",
      "195      3  1.86419054\n",
      "196      6  6.78882902\n",
      "197      6  6.32569766\n",
      "198      7  4.00684707\n",
      "199      5  7.96836775\n",
      "200      3  3.53115944\n",
      "201      5  3.12045493\n",
      "202      5  1.96484848\n",
      "203      3  1.45387563\n",
      "204      6  6.16093802\n",
      "205      3  4.81000996\n",
      "206      8  4.55035707\n",
      "207      5  4.65617304\n",
      "208      5  5.79768265\n",
      "209     12  6.23325302\n",
      "210      5  6.56090919\n",
      "211      6  4.24203156\n",
      "212      5  5.87554763\n",
      "213      6  4.41295968\n",
      "214      3  2.69438560\n",
      "215      5  4.11446285\n",
      "216      5  4.26630966\n",
      "217      3  4.54690171\n",
      "218      7  5.26677694\n",
      "219      8  6.28226803\n",
      "220      3  2.45147769\n",
      "221      3  3.16078603\n",
      "222      5  7.17266743\n",
      "223      7  4.87200139\n",
      "224     10  8.00127483\n",
      "225      5  5.30459771\n",
      "226      3  4.01060016\n",
      "227      3  4.90969726\n",
      "228      5  1.34865828\n",
      "229      5  6.06763476\n",
      "230      3  3.94423731\n",
      "231      5  4.10746653\n",
      "232      3  2.30511496\n",
      "233      8  5.52865083\n",
      "234      3  4.66139887\n",
      "235      9  7.54797224\n",
      "236      3  4.94671851\n",
      "237      3  3.19875872\n",
      "238      3  2.65315050\n",
      "239      3  3.52362356\n",
      "240      3  2.38705521\n",
      "241      3  5.06873818\n",
      "242     10  6.65472719\n",
      "243      8  4.35008255\n",
      "244      3  3.02151817\n",
      "245      5  3.36434542\n",
      "246      3  3.21818583\n",
      "247      3  4.31332977\n",
      "248      3  4.53965200\n",
      "249      3  4.63172833\n",
      "250      4  6.92263886\n",
      "251      3  4.85354162\n",
      "252      6  7.31602498\n",
      "253      3  4.20045082\n",
      "254      4  5.45995893\n",
      "255      3  7.34342977\n",
      "256      3  5.01486197\n",
      "257      3  4.82357639\n",
      "258      6  3.56320446\n",
      "259      3  2.79443973\n",
      "260      3  1.90397533\n",
      "261      6  5.38673383\n",
      "262      3  3.27784173\n",
      "263      5  7.57527910\n",
      "264      3  4.45036480\n",
      "265      3  2.31253907\n",
      "266      9  8.16153835\n",
      "267      3  3.19218458\n",
      "268      3  3.31933667\n",
      "269      3  1.80445477\n",
      "270      3  2.84038798\n",
      "271      5  5.00601488\n",
      "272      3  6.56363717\n",
      "273      3  4.78695622\n",
      "274      5  6.20835424\n",
      "275      5  3.62638720\n",
      "276      3  3.74936867\n",
      "277      3  5.07318546\n",
      "278      3  4.11578598\n",
      "279      8  6.74273753\n",
      "280      3  4.47922548\n",
      "281      3  3.46813123\n",
      "282      3  4.23030748\n",
      "283      3  3.02210650\n",
      "284      3  3.43358669\n",
      "285      5  5.07725916\n",
      "286      5  8.16767671\n",
      "287      6  5.09218156\n",
      "288      8  4.53512688\n",
      "289      9  5.49432907\n",
      "290      6  5.51403001\n",
      "291      9  6.42834724\n",
      "292      8  6.27553723\n",
      "293      3  3.75321759\n",
      "294      9  5.25913731\n",
      "295      7  8.47821315\n",
      "296     11  9.18736721\n",
      "297      8  6.26454292\n",
      "298      6  7.92466629\n",
      "299      5  4.27566097\n",
      "300      5  5.02305764\n",
      "301      3  3.96751373\n",
      "302      3  4.27824609\n",
      "303      3  5.38490585\n",
      "304      5  6.92088198\n",
      "305     12 10.50413727\n",
      "306      5  3.10829753\n",
      "307      5 10.69219420\n",
      "308      5  4.32145129\n",
      "309     10  8.29444731\n",
      "310      3  3.19731571\n",
      "311      5  1.42540249\n",
      "312      5  4.75940742\n",
      "313     11  8.50921659\n",
      "314      5  5.70917953\n",
      "315      3  6.54400996\n",
      "316      3  4.49816559\n",
      "317      3  2.65416610\n",
      "318      3  5.11555569\n",
      "319      3  3.28440892\n",
      "320      3  4.17001675\n",
      "321      6  3.12336590\n",
      "322      3  3.53608755\n",
      "323      7  4.60525419\n",
      "324      9  7.90745864\n",
      "325      9  8.35654137\n",
      "326      6  8.31966888\n",
      "327      3  3.27387931\n",
      "328      3  6.01156707\n",
      "329      3  4.22203134\n",
      "330      3  3.46245082\n",
      "331      3  4.82733774\n",
      "332      3  6.69504474\n",
      "333      3  4.99323584\n",
      "334      5  3.37982122\n",
      "335      5  7.84929288\n",
      "336      4  1.42747605\n",
      "337      3  4.25391163\n",
      "338      7  6.34502288\n",
      "339      3  1.77841421\n",
      "340      7  5.63151596\n",
      "341      3  1.76983282\n",
      "342      3  4.34193350\n",
      "343      8  5.98775728\n",
      "344      3  3.74289547\n",
      "345      5  6.16811063\n",
      "346      3  3.60870072\n",
      "347      3  2.13268934\n",
      "348      3  4.05537413\n",
      "349      3  6.00224147\n",
      "350      5  4.29358656\n",
      "351      3  2.82215436\n",
      "352      6  9.68374157\n",
      "353      6  5.81163341\n",
      "354      5  5.37865111\n",
      "355      5  7.01911075\n",
      "356      5  4.61165970\n",
      "357      3  4.02044748\n",
      "358      3  3.25251259\n",
      "359      4  4.86021591\n",
      "360      7  6.12295990\n",
      "361      3  4.16786660\n",
      "362      3  0.33362019\n",
      "363      5  3.77939938\n",
      "364      5  6.46893094\n",
      "365      7  6.98824467\n",
      "366     10 10.53874925\n",
      "367     10  7.99480613\n",
      "368      3  2.70375410\n",
      "369      3  4.42235675\n",
      "370      8  5.99709506\n",
      "371      3  4.75473847\n",
      "372      7  8.13054256\n",
      "373      3  1.22179539\n",
      "374      7  3.72560285\n",
      "375      6  7.01077440\n",
      "376     11  7.81400704\n",
      "377      3  2.83768845\n",
      "378      6  8.79015845\n",
      "379      6  7.85048061\n",
      "380      3  4.12792114\n",
      "381      3  4.11537595\n",
      "382      9  9.35703163\n",
      "383      3  4.31002923\n",
      "384      6  3.88330262\n",
      "385      3  4.06677639\n",
      "386      5  4.89260716\n",
      "387      5  7.45393505\n",
      "388      3  1.55301135\n",
      "389      3  3.21779168\n",
      "390      3  1.92029185\n",
      "391      9  7.62717998\n",
      "392      6  7.26068157\n",
      "393      5  3.92627618\n",
      "394      7  9.39162458\n",
      "395      3  2.74085005\n",
      "396      4  6.93671623\n",
      "397      3  0.65344200\n",
      "398      5  4.59019748\n",
      "399      6  4.61979610\n",
      "400      8  7.41472884\n",
      "401      6  6.43847725\n",
      "402      3  5.36992715\n",
      "403      3  5.90969862\n",
      "404      3  3.28028994\n",
      "405      3  3.41813976\n",
      "406      5  7.50325748\n",
      "407      3  3.67484159\n"
     ]
    }
   ],
   "source": [
    "# Initialize vectors to store total actual and predicted values across all folds\n",
    "total_actual <- c()\n",
    "total_predicted <- c()\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "     # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "    \n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Store the top 100 markers for the current fold\n",
    "    all_top_markers[[i]] <- top_100_markers\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Store the predictions and actual values\n",
    "    all_predictions[[i]] <- data.frame(\n",
    "        PlasmaAlias = testing_set$plasma_alias,\n",
    "        Actual = testing_set$child_pugh_score,\n",
    "        Predicted = predictions\n",
    "    )\n",
    "\n",
    "    # Print the actual and predicted values for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Actual vs Predicted Values:\\n\"))\n",
    "    print(all_predictions[[i]])\n",
    "\n",
    "    # Add the current fold's actual and predicted values to the total list\n",
    "    total_actual <- c(total_actual, testing_set$child_pugh_score)\n",
    "    total_predicted <- c(total_predicted, predictions)\n",
    "\n",
    "    # Calculate and store the Pearson correlation for the current fold\n",
    "    r_value <- cor(testing_set$child_pugh_score, predictions)\n",
    "    r_squared <- r_value^2\n",
    "    cat(paste(\"Fold\", i, \"Pearson correlation: R =\", round(r_value, 2), \"| R² =\", round(r_squared, 2), \"\\n\"))\n",
    "}\n",
    "\n",
    "# After all folds: Calculate Pearson correlation across all folds\n",
    "total_r_value <- cor(total_actual, total_predicted)\n",
    "total_r_squared <- total_r_value^2\n",
    "cat(paste(\"Total Pearson correlation: R =\", round(total_r_value, 2), \"| R² =\", round(total_r_squared, 2), \"\\n\"))\n",
    "\n",
    "# Print the total actual and predicted values\n",
    "cat(\"Total Actual vs Predicted Values across all folds:\\n\")\n",
    "total_results <- data.frame(\n",
    "    Actual = total_actual,\n",
    "    Predicted = total_predicted\n",
    ")\n",
    "print(total_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18435226",
   "metadata": {},
   "source": [
    "# Linear Model CV 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb36855",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type 'citation(\"pROC\")' for a citation.\n",
      "\n",
      "\n",
      "Attaching package: ‘pROC’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:IRanges’:\n",
      "\n",
      "    cov, var\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:S4Vectors’:\n",
      "\n",
      "    cov, var\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:BiocGenerics’:\n",
      "\n",
      "    var\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    cov, smooth, var\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Class Distribution (Classified Training Data):\n",
      "classified_training\n",
      "  1   2   3   4 \n",
      "168  94  49  14 \n",
      "Fold 1 Total Count of Individuals: 325 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 1 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35  9  2  1\n",
      "         2  6  9  6  1\n",
      "         3  1  6  4  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.1120          \n",
      "                                          \n",
      "                  Kappa : 0.3143          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.5524          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.3750  0.33333  0.00000\n",
      "Specificity            0.7000   0.7759  0.87143  1.00000\n",
      "Pos Pred Value         0.7447   0.4091  0.30769      NaN\n",
      "Neg Pred Value         0.8000   0.7500  0.88406  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4268   0.1098  0.04878  0.00000\n",
      "Detection Prevalence   0.5732   0.2683  0.15854  0.00000\n",
      "Balanced Accuracy      0.7667   0.5754  0.60238  0.50000\n",
      "Fold 1 Accuracy: 0.585365853658537 \n",
      "Fold 1 AUC: 0.698164682539682 \n",
      "Fold 1 Overall Sensitivity: 0.385416666666667 \n",
      "Fold 1 Overall Specificity: 0.836822660098522 \n",
      "Fold 2 Class Distribution (Classified Training Data):\n",
      "classified_training\n",
      "  1   2   3   4 \n",
      "168  94  48  15 \n",
      "Fold 2 Total Count of Individuals: 325 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 13  3 \n",
      "Fold 2 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 34  6  0  0\n",
      "         2  6 14  6  1\n",
      "         3  2  4  7  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6707          \n",
      "                 95% CI : (0.5581, 0.7706)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.002659        \n",
      "                                          \n",
      "                  Kappa : 0.473           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8095   0.5833  0.53846  0.00000\n",
      "Specificity            0.8500   0.7759  0.88406  1.00000\n",
      "Pos Pred Value         0.8500   0.5185  0.46667      NaN\n",
      "Neg Pred Value         0.8095   0.8182  0.91045  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4146   0.1707  0.08537  0.00000\n",
      "Detection Prevalence   0.4878   0.3293  0.18293  0.00000\n",
      "Balanced Accuracy      0.8298   0.6796  0.71126  0.50000\n",
      "Fold 2 Accuracy: 0.670731707317073 \n",
      "Fold 2 AUC: 0.792302604802605 \n",
      "Fold 2 Overall Sensitivity: 0.48282967032967 \n",
      "Fold 2 Overall Specificity: 0.877480009995003 \n",
      "Fold 3 Class Distribution (Classified Training Data):\n",
      "classified_training\n",
      "  1   2   3   4 \n",
      "168  94  49  14 \n",
      "Fold 3 Total Count of Individuals: 325 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 3 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37 13  6  0\n",
      "         2  5  9  4  3\n",
      "         3  0  2  2  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.112           \n",
      "                                          \n",
      "                  Kappa : 0.2679          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810   0.3750  0.16667  0.00000\n",
      "Specificity            0.5250   0.7931  0.95714  1.00000\n",
      "Pos Pred Value         0.6607   0.4286  0.40000      NaN\n",
      "Neg Pred Value         0.8077   0.7541  0.87013  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4512   0.1098  0.02439  0.00000\n",
      "Detection Prevalence   0.6829   0.2561  0.06098  0.00000\n",
      "Balanced Accuracy      0.7030   0.5841  0.56190  0.50000\n",
      "Fold 3 Accuracy: 0.585365853658537 \n",
      "Fold 3 AUC: 0.730696097883598 \n",
      "Fold 3 Overall Sensitivity: 0.355654761904762 \n",
      "Fold 3 Overall Specificity: 0.81881157635468 \n",
      "Fold 4 Class Distribution (Classified Training Data):\n",
      "classified_training\n",
      "  1   2   3   4 \n",
      "168  95  49  14 \n",
      "Fold 4 Total Count of Individuals: 326 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  4 \n",
      "Fold 4 Total Count of Individuals: 81 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 33  8  2  0\n",
      "         2  8  9  6  0\n",
      "         3  1  5  4  3\n",
      "         4  0  1  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5802          \n",
      "                 95% CI : (0.4654, 0.6891)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.1585          \n",
      "                                          \n",
      "                  Kappa : 0.322           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7857   0.3913  0.33333  0.25000\n",
      "Specificity            0.7436   0.7586  0.86957  0.98701\n",
      "Pos Pred Value         0.7674   0.3913  0.30769  0.50000\n",
      "Neg Pred Value         0.7632   0.7586  0.88235  0.96203\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4074   0.1111  0.04938  0.01235\n",
      "Detection Prevalence   0.5309   0.2840  0.16049  0.02469\n",
      "Balanced Accuracy      0.7647   0.5750  0.60145  0.61851\n",
      "Fold 4 Accuracy: 0.580246913580247 \n",
      "Fold 4 AUC: 0.817467506326202 \n",
      "Fold 4 Overall Sensitivity: 0.440087991718426 \n",
      "Fold 4 Overall Specificity: 0.839697159412302 \n",
      "Fold 5 Class Distribution (Classified Training Data):\n",
      "classified_training\n",
      "  1   2   3   4 \n",
      "168  95  49  15 \n",
      "Fold 5 Total Count of Individuals: 327 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  3 \n",
      "Fold 5 Total Count of Individuals: 80 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36  9  1  0\n",
      "         2  6  5  6  0\n",
      "         3  0  9  5  2\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5875          \n",
      "                 95% CI : (0.4718, 0.6965)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.1568          \n",
      "                                          \n",
      "                  Kappa : 0.3199          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.2174   0.4167   0.3333\n",
      "Specificity            0.7368   0.7895   0.8382   1.0000\n",
      "Pos Pred Value         0.7826   0.2941   0.3125   1.0000\n",
      "Neg Pred Value         0.8235   0.7143   0.8906   0.9747\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4500   0.0625   0.0625   0.0125\n",
      "Detection Prevalence   0.5750   0.2125   0.2000   0.0125\n",
      "Balanced Accuracy      0.7970   0.5034   0.6275   0.6667\n",
      "Fold 5 Accuracy: 0.5875 \n",
      "Fold 5 AUC: 0.834943639291465 \n",
      "Fold 5 Overall Sensitivity: 0.456133540372671 \n",
      "Fold 5 Overall Specificity: 0.841137770897833 \n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6018421 \n",
      "Average AUC: 0.7747149 \n",
      "Average Sensitivity: 0.4240245 \n",
      "Average Specificity: 0.8427898 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(limma)\n",
    "library(pROC)\n",
    "\n",
    "k<-5\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "classified_training_data <- list()\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Loop through the folds for training and testing\n",
    "for (i in 1:k) {\n",
    "    # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "    \n",
    "    # Classify the actual training data\n",
    "    classified_training <- sapply(as.numeric(as.character(training_set$child_pugh_score)), classify)\n",
    "    classified_training <- factor(classified_training, levels = 1:4)\n",
    "\n",
    "    # Print the class distribution (i.e., how many individuals in each class)\n",
    "    cat(paste(\"Fold\", i, \"Class Distribution (Classified Training Data):\\n\"))\n",
    "    print(table(classified_training))\n",
    "    total_count_classified_training <- length(classified_training)\n",
    "    cat(paste(\"Fold\", i, \"Total Count of Individuals:\", total_count_classified_training, \"\\n\"))\n",
    "    \n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Classify predictions\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "\n",
    "    # Convert `child_pugh_score` to numeric before classification\n",
    "    classified_actuals <- sapply(as.numeric(as.character(testing_set$child_pugh_score)), classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "    # Print the number of individuals in each class for the confusion matrix\n",
    "    class_count <- table(classified_actuals)\n",
    "    cat(paste(\"Fold\", i, \"Class Distribution (Actual):\\n\"))\n",
    "    print(class_count)\n",
    "    total_count <- length(classified_actuals)\n",
    "    cat(paste(\"Fold\", i, \"Total Count of Individuals:\", total_count, \"\\n\"))\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "}\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b09cea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCs:  0.6982\t0.7923\t0.7307\t0.8175\t0.8349 \n",
      "Sensitivities:  0.3854\t0.4828\t0.3557\t0.4401\t0.4561 \n",
      "Specificities:  0.8368\t0.8775\t0.8188\t0.8397\t0.8411 \n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6018421 \n",
      "Average AUC: 0.7747149 \n",
      "Average Sensitivity: 0.4240245 \n",
      "Average Specificity: 0.8427898 \n",
      "Overall Confusion Matrix:\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 175  45  11   1\n",
      "         2  31  46  28   5\n",
      "         3   4  26  22  10\n",
      "         4   0   1   0   2\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(limma)\n",
    "library(pROC)\n",
    "\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "classified_training_data <- list()\n",
    "\n",
    "# Initialize confusion matrix storage\n",
    "overall_confusion_matrix <- matrix(0, nrow = 4, ncol = 4)\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Loop through the folds for training and testing\n",
    "for (i in 1:k) {\n",
    "    # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "    \n",
    "    # Classify the actual training data\n",
    "    classified_training <- sapply(as.numeric(as.character(training_set$child_pugh_score)), classify)\n",
    "    classified_training <- factor(classified_training, levels = 1:4)\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the linear model using the filtered training data\n",
    "    model <- lm(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Classify predictions\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "\n",
    "    # Convert `child_pugh_score` to numeric before classification\n",
    "    classified_actuals <- sapply(as.numeric(as.character(testing_set$child_pugh_score)), classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "    \n",
    "    # Add the confusion matrix to the overall confusion matrix\n",
    "    overall_confusion_matrix <- overall_confusion_matrix + as.matrix(conf_matrix$table)\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "}\n",
    "\n",
    "# Print AUCs, Sensitivities, Specificities side by side\n",
    "cat(\"AUCs: \", paste(round(all_auc, 4), collapse = \"\\t\"), \"\\n\")\n",
    "cat(\"Sensitivities: \", paste(round(all_sensitivity, 4), collapse = \"\\t\"), \"\\n\")\n",
    "cat(\"Specificities: \", paste(round(all_specificity, 4), collapse = \"\\t\"), \"\\n\")\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "\n",
    "# Print the overall confusion matrix\n",
    "cat(\"Overall Confusion Matrix:\\n\")\n",
    "print(overall_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e68ef5",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9de04a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  408 (303 variable)\n",
      "initial  value 450.545667 \n",
      "iter  10 value 223.867262\n",
      "iter  20 value 105.340633\n",
      "iter  30 value 70.719003\n",
      "iter  40 value 45.839796\n",
      "iter  50 value 11.438474\n",
      "iter  60 value 0.318738\n",
      "iter  70 value 0.001459\n",
      "final  value 0.000057 \n",
      "converged\n",
      "\n",
      "Fold 1 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 1 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 31  5  3  1\n",
      "         2  6 14  4  1\n",
      "         3  3  2  3  1\n",
      "         4  2  3  2  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5976          \n",
      "                 95% CI : (0.4834, 0.7044)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.07514         \n",
      "                                          \n",
      "                  Kappa : 0.3713          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.87685         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.7381   0.5833  0.25000  0.25000\n",
      "Specificity            0.7750   0.8103  0.91429  0.91026\n",
      "Pos Pred Value         0.7750   0.5600  0.33333  0.12500\n",
      "Neg Pred Value         0.7381   0.8246  0.87671  0.95946\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.3780   0.1707  0.03659  0.01220\n",
      "Detection Prevalence   0.4878   0.3049  0.10976  0.09756\n",
      "Balanced Accuracy      0.7565   0.6968  0.58214  0.58013\n",
      "Fold 1 Accuracy: 0.597560975609756 \n",
      "Fold 1 AUC: 0.660342261904762 \n",
      "Fold 1 Overall Sensitivity: 0.455357142857143 \n",
      "Fold 1 Overall Specificity: 0.852471738032083 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  408 (303 variable)\n",
      "initial  value 450.545667 \n",
      "iter  10 value 185.742263\n",
      "iter  20 value 123.565409\n",
      "iter  30 value 95.230134\n",
      "iter  40 value 63.098905\n",
      "iter  50 value 24.679331\n",
      "iter  60 value 2.254243\n",
      "iter  70 value 0.018054\n",
      "final  value 0.000068 \n",
      "converged\n",
      "\n",
      "Fold 2 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 13  3 \n",
      "Fold 2 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 27  9  2  0\n",
      "         2  9  6  1  0\n",
      "         3  2  5  5  2\n",
      "         4  4  4  5  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.4756          \n",
      "                 95% CI : (0.3641, 0.5889)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.78032         \n",
      "                                          \n",
      "                  Kappa : 0.2199          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.06304         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6429  0.25000  0.38462  0.33333\n",
      "Specificity            0.7250  0.82759  0.86957  0.83544\n",
      "Pos Pred Value         0.7105  0.37500  0.35714  0.07143\n",
      "Neg Pred Value         0.6591  0.72727  0.88235  0.97059\n",
      "Prevalence             0.5122  0.29268  0.15854  0.03659\n",
      "Detection Rate         0.3293  0.07317  0.06098  0.01220\n",
      "Detection Prevalence   0.4634  0.19512  0.17073  0.17073\n",
      "Balanced Accuracy      0.6839  0.53879  0.62709  0.58439\n",
      "Fold 2 Accuracy: 0.475609756097561 \n",
      "Fold 2 AUC: 0.73509996947497 \n",
      "Fold 2 Overall Sensitivity: 0.402701465201465 \n",
      "Fold 2 Overall Specificity: 0.814398615565635 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  408 (303 variable)\n",
      "initial  value 450.545667 \n",
      "iter  10 value 201.978207\n",
      "iter  20 value 123.617826\n",
      "iter  30 value 83.018304\n",
      "iter  40 value 42.988961\n",
      "iter  50 value 10.903551\n",
      "iter  60 value 0.145855\n",
      "iter  70 value 0.001476\n",
      "final  value 0.000085 \n",
      "converged\n",
      "\n",
      "Fold 3 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 3 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 26 12  4  1\n",
      "         2  9  4  5  0\n",
      "         3  3  7  1  3\n",
      "         4  4  1  2  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.378           \n",
      "                 95% CI : (0.2732, 0.4919)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.9946          \n",
      "                                          \n",
      "                  Kappa : 0.0252          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.6896          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.6190  0.16667  0.08333  0.00000\n",
      "Specificity            0.5750  0.75862  0.81429  0.91026\n",
      "Pos Pred Value         0.6047  0.22222  0.07143  0.00000\n",
      "Neg Pred Value         0.5897  0.68750  0.83824  0.94667\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.3171  0.04878  0.01220  0.00000\n",
      "Detection Prevalence   0.5244  0.21951  0.17073  0.08537\n",
      "Balanced Accuracy      0.5970  0.46264  0.44881  0.45513\n",
      "Fold 3 Accuracy: 0.378048780487805 \n",
      "Fold 3 AUC: 0.633267195767196 \n",
      "Fold 3 Overall Sensitivity: 0.217261904761905 \n",
      "Fold 3 Overall Specificity: 0.764540703549324 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  408 (303 variable)\n",
      "initial  value 451.931962 \n",
      "iter  10 value 234.079186\n",
      "iter  20 value 151.680054\n",
      "iter  30 value 120.238843\n",
      "iter  40 value 83.343250\n",
      "iter  50 value 56.793893\n",
      "iter  60 value 29.741004\n",
      "iter  70 value 4.638130\n",
      "iter  80 value 0.016265\n",
      "final  value 0.000085 \n",
      "converged\n",
      "\n",
      "Fold 4 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  4 \n",
      "Fold 4 Total Count of Individuals: 81 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 24  6  0  0\n",
      "         2 12  7  6  1\n",
      "         3  3  5  5  2\n",
      "         4  3  5  1  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.4568          \n",
      "                 95% CI : (0.3456, 0.5713)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.88933         \n",
      "                                          \n",
      "                  Kappa : 0.205           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.08561         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.5714  0.30435  0.41667  0.25000\n",
      "Specificity            0.8462  0.67241  0.85507  0.88312\n",
      "Pos Pred Value         0.8000  0.26923  0.33333  0.10000\n",
      "Neg Pred Value         0.6471  0.70909  0.89394  0.95775\n",
      "Prevalence             0.5185  0.28395  0.14815  0.04938\n",
      "Detection Rate         0.2963  0.08642  0.06173  0.01235\n",
      "Detection Prevalence   0.3704  0.32099  0.18519  0.12346\n",
      "Balanced Accuracy      0.7088  0.48838  0.63587  0.56656\n",
      "Fold 4 Accuracy: 0.45679012345679 \n",
      "Fold 4 AUC: 0.708689182194617 \n",
      "Fold 4 Overall Sensitivity: 0.385610766045549 \n",
      "Fold 4 Overall Specificity: 0.814189246535573 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  408 (303 variable)\n",
      "initial  value 453.318256 \n",
      "iter  10 value 170.364034\n",
      "iter  20 value 101.412673\n",
      "iter  30 value 68.944831\n",
      "iter  40 value 29.959212\n",
      "iter  50 value 4.103191\n",
      "iter  60 value 0.048300\n",
      "iter  70 value 0.001349\n",
      "final  value 0.000055 \n",
      "converged\n",
      "\n",
      "Fold 5 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  3 \n",
      "Fold 5 Total Count of Individuals: 80 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 23  7  1  0\n",
      "         2  9  8  4  1\n",
      "         3  2  7  6  2\n",
      "         4  8  1  1  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.4625          \n",
      "                 95% CI : (0.3503, 0.5776)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.8909          \n",
      "                                          \n",
      "                  Kappa : 0.2106          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.1363          \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.5476   0.3478   0.5000   0.0000\n",
      "Specificity            0.7895   0.7544   0.8382   0.8701\n",
      "Pos Pred Value         0.7419   0.3636   0.3529   0.0000\n",
      "Neg Pred Value         0.6122   0.7414   0.9048   0.9571\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.2875   0.1000   0.0750   0.0000\n",
      "Detection Prevalence   0.3875   0.2750   0.2125   0.1250\n",
      "Balanced Accuracy      0.6685   0.5511   0.6691   0.4351\n",
      "Fold 5 Accuracy: 0.4625 \n",
      "Fold 5 AUC: 0.653360075914424 \n",
      "Fold 5 Overall Sensitivity: 0.348861283643892 \n",
      "Fold 5 Overall Specificity: 0.813056203342581 \n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.4741019 \n",
      "Average AUC: 0.6781517 \n",
      "Average Sensitivity: 0.3619585 \n",
      "Average Specificity: 0.8117313 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(nnet)  # For multinomial logistic regression\n",
    "library(limma) # For linear model fitting on methylation data\n",
    "\n",
    "# Define classification thresholds (same as before)\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_top_markers <- list()\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "  # Use the preloaded training and testing sets\n",
    "  training_set <- training_sets[[i]]\n",
    "  testing_set <- testing_sets[[i]]\n",
    "\n",
    "  # Ensure the column names are correct for column removal\n",
    "  columns_to_remove <- colnames(filtered_pheno_data)\n",
    "  columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "  # Remove specified columns from the training data\n",
    "  methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "  # Create the design matrix\n",
    "  design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "  # Remove 'child_pugh_score' column from methylation data\n",
    "  methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "  # Fit the linear model\n",
    "  fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "  fit <- eBayes(fit)\n",
    "\n",
    "  # Get top 100 significant markers\n",
    "  top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "  top_100_markers <- rownames(top_100_results)\n",
    "  all_top_markers[[i]] <- top_100_markers\n",
    "\n",
    "  # Filter training and testing data to include only the top 100 markers\n",
    "  training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "  testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "  # **Classify training `child_pugh_score` before model training**\n",
    "  classified_train_scores <- sapply(as.numeric(as.character(training_set_filtered$child_pugh_score)), classify)\n",
    "  training_set_filtered$child_pugh_score <- factor(classified_train_scores, levels = 1:4)\n",
    "\n",
    "  # Fit a multinomial logistic regression model using the filtered training data\n",
    "  model <- multinom(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "  # Predict on the filtered testing set (probabilities)\n",
    "  prob_predictions <- predict(model, newdata = testing_set_filtered, type = \"probs\")\n",
    "\n",
    "  # Classify predictions based on the highest probability\n",
    "  predicted_classes <- apply(prob_predictions, 1, function(row) {\n",
    "    which.max(row) # This returns the class with the highest probability\n",
    "  })\n",
    "\n",
    "  # Convert `child_pugh_score` to numeric before classification for testing set\n",
    "  classified_actuals <- sapply(as.numeric(as.character(testing_set$child_pugh_score)), classify)\n",
    "  classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "  # Ensure they are factors with levels 1, 2, 3, 4\n",
    "  classified_predictions <- factor(predicted_classes, levels = 1:4)\n",
    "\n",
    "  # Check for NA values in classified predictions and actual values\n",
    "  valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "  classified_predictions <- classified_predictions[valid_indices]\n",
    "  classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "  # Print the number of individuals in each class for the confusion matrix\n",
    "  class_count <- table(classified_actuals)\n",
    "  cat(paste(\"\\nFold\", i, \"Class Distribution (Actual):\\n\"))\n",
    "  print(class_count)\n",
    "  total_count <- length(classified_actuals)\n",
    "  cat(paste(\"Fold\", i, \"Total Count of Individuals:\", total_count, \"\\n\"))\n",
    "    \n",
    "  # Create a confusion matrix\n",
    "  conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "  all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "  # Calculate and store accuracy\n",
    "  accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "  all_accuracy[i] <- accuracy\n",
    "\n",
    "  # Calculate and store AUC\n",
    "  roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "  auc_value <- auc(roc_multiclass)\n",
    "  all_auc[i] <- auc_value\n",
    "\n",
    "  # Extract sensitivity and specificity from the confusion matrix\n",
    "  sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "  overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "  all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "  specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "  overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "  all_specificity[i] <- overall_specificity\n",
    "\n",
    "  # Print results for the current fold\n",
    "  cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "  print(conf_matrix)\n",
    "  cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "  cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "  cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "  cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "}\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "shared_markers <- Reduce(intersect, all_top_markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916d075",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0f653",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read .rds files\n",
    "training_sets <- lapply(1:k, function(i) readRDS(paste0(\"/u/home/c/ctang04/Liver Project/data/childpugh_training_set_fold_\", i, \".rds\")))\n",
    "testing_sets <- lapply(1:k, function(i) readRDS(paste0(\"/u/home/c/ctang04/Liver Project/data/childpugh_testing_set_fold_\", i, \".rds\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c094fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "\n",
      "Attaching package: ‘randomForest’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    margin\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:minfi’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:Biobase’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:BiocGenerics’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 1 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 39 12  1  0\n",
      "         2  3 10  7  1\n",
      "         3  0  2  4  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6463         \n",
      "                 95% CI : (0.533, 0.7488)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.009787       \n",
      "                                         \n",
      "                  Kappa : 0.3946         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9286   0.4167  0.33333  0.00000\n",
      "Specificity            0.6750   0.8103  0.92857  1.00000\n",
      "Pos Pred Value         0.7500   0.4762  0.44444      NaN\n",
      "Neg Pred Value         0.9000   0.7705  0.89041  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4756   0.1220  0.04878  0.00000\n",
      "Detection Prevalence   0.6341   0.2561  0.10976  0.00000\n",
      "Balanced Accuracy      0.8018   0.6135  0.63095  0.50000\n",
      "Fold 1 Predicted vs Actual Values:\n",
      "                 PlasmaAlias Actual Predicted\n",
      "1          plasma-1969-P9-LV      6  7.953067\n",
      "2           plasma-2741-P9-N      3  4.269433\n",
      "3           plasma-2248-P9-N      3  4.146767\n",
      "4           plasma-2549-P9-N      5  4.010400\n",
      "5           plasma-2349-P9-N      5  4.418667\n",
      "6          plasma-3620-P9-BD      4  6.007033\n",
      "7           plasma-2072-P9-N      3  4.453167\n",
      "8          plasma-3629-P9-LV      5  3.449500\n",
      "9           plasma-3013-P9-N      5  4.636400\n",
      "10          plasma-637-P9-CH      3  3.544400\n",
      "11          plasma-641-P9-CH      3  4.100900\n",
      "12          plasma-646-P9-CH      3  3.439433\n",
      "13          plasma-654-P9-CH      5  4.216800\n",
      "14          plasma-657-P9-CH     10  8.784533\n",
      "15          plasma-658-P9-CH      3  3.791967\n",
      "16         plasma-2026-P9-CH      3  3.065633\n",
      "17          plasma-770-P9-LV      6  7.390200\n",
      "18         plasma-1215-P9-CH      3  3.677200\n",
      "19         plasma-4000-P9-CH      3  3.300067\n",
      "20         plasma-2411-P9-CH      3  3.537500\n",
      "21         plasma-1422-P9-CH      3  5.722200\n",
      "22         plasma-1623-P9-CH      3  3.255467\n",
      "23         plasma-1993-P9-CH      3  3.663800\n",
      "24          plasma-754-P9-CH      8  6.889067\n",
      "25         plasma-1423-P9-CH      3  3.199533\n",
      "26         plasma-1428-P9-CH      3  3.223033\n",
      "27         plasma-1630-P9-CH      3  4.117633\n",
      "28         plasma-1429-P9-CH      3  3.678333\n",
      "29         plasma-1631-P9-CH      3  4.882233\n",
      "30         plasma-1633-P9-CH      3  3.455633\n",
      "31         plasma-1634-P9-CH      5  3.207600\n",
      "32         plasma-1432-P9-CH      3  3.518500\n",
      "33         plasma-2083-P9-CH      3  3.177900\n",
      "34         plasma-1637-P9-CH      3  3.115133\n",
      "35          plasma-764-P9-CH      5  6.476800\n",
      "36         plasma-3922-P9-CH      3  4.584400\n",
      "37          plasma-738-P9-CH      6  6.155667\n",
      "38          plasma-737-P9-LV      6  6.715667\n",
      "39         plasma-1217-P9-CH      3  3.801033\n",
      "40         plasma-1137-P9-CH      5  5.538667\n",
      "41         plasma-1644-P9-LV      5  4.997100\n",
      "42         plasma-1066-P9-CH      6  3.647867\n",
      "43         plasma-3132-P9-CH      3  3.417333\n",
      "44         plasma-1138-P9-CH      7  5.141767\n",
      "45          plasma-842-P9-LV      7  5.231833\n",
      "46         plasma-3321-P9-CH      3  5.128500\n",
      "47         plasma-1126-P9-CH     11  7.965167\n",
      "48         plasma-1653-P9-CH      9  6.641533\n",
      "49         plasma-1655-P9-LV      3  3.439533\n",
      "50         plasma-1656-P9-CH      3  3.583500\n",
      "51         plasma-1329-P9-CH      3  3.333633\n",
      "52         plasma-1336-P9-LV      8  8.141800\n",
      "53         plasma-1192-P9-CH      6  3.335667\n",
      "54         plasma-2432-P9-CH      6  5.407500\n",
      "55         plasma-1657-P9-LV      5  5.695100\n",
      "56         plasma-2435-P9-CH      5  4.256267\n",
      "57         plasma-1659-P9-LV      7  6.186700\n",
      "58         plasma-2920-P9-CH      3  3.390300\n",
      "59         plasma-2337-P9-CH      3  3.348200\n",
      "60         plasma-2541-P9-CH      9  4.689333\n",
      "61         plasma-2038-P9-CH      9  7.642800\n",
      "62         plasma-1964-P9-CH      9  7.089233\n",
      "63         plasma-2510-P9-CH      3  3.697667\n",
      "64         plasma-2533-P9-CH      9  6.066867\n",
      "65         plasma-1663-P9-CH     10  5.238000\n",
      "66         plasma-2501-P9-CH      6  5.837167\n",
      "67         plasma-1980-P9-CH      3  3.408933\n",
      "68         plasma-2025-P9-CH      5  4.662967\n",
      "69         plasma-2389-P9-CH      5  3.921533\n",
      "70         plasma-2495-P9-CH      3  3.139300\n",
      "71         plasma-1879-P9-CH      6  5.182600\n",
      "72         plasma-2082-P9-CH      5  6.292967\n",
      "73         plasma-1665-P9-LV      8  7.015633\n",
      "74         plasma-2018-P9-CH      5  5.182300\n",
      "75         plasma-2297-P9-CH      3  3.685300\n",
      "76         plasma-2536-P9-CH      3  3.542567\n",
      "77         plasma-3087-P9-CH      7  5.263933\n",
      "78         plasma-3964-P9-CH      3  3.369333\n",
      "79    plasma-2576-5day-P9-CH      3  3.419100\n",
      "80         plasma-3915-P9-CH     11  8.132400\n",
      "81 plasma-2568-r1-4day-P9-CH      3  3.053200\n",
      "82          plasma-3917-P9-N      3  3.968000\n",
      "Fold 1 Accuracy: 0.646341463414634 \n",
      "Fold 1 AUC: 0.835152116402116 \n",
      "Fold 1 Overall Sensitivity: 0.419642857142857 \n",
      "Fold 1 Overall Specificity: 0.853479064039409 \n",
      "\n",
      "Fold 2 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 13  3 \n",
      "Fold 2 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38  7  0  1\n",
      "         2  4 14  8  1\n",
      "         3  0  3  5  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                         \n",
      "               Accuracy : 0.6951         \n",
      "                 95% CI : (0.5836, 0.792)\n",
      "    No Information Rate : 0.5122         \n",
      "    P-Value [Acc > NIR] : 0.0005892      \n",
      "                                         \n",
      "                  Kappa : 0.4909         \n",
      "                                         \n",
      " Mcnemar's Test P-Value : NA             \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048   0.5833  0.38462  0.00000\n",
      "Specificity            0.8000   0.7759  0.94203  1.00000\n",
      "Pos Pred Value         0.8261   0.5185  0.55556      NaN\n",
      "Neg Pred Value         0.8889   0.8182  0.89041  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4634   0.1707  0.06098  0.00000\n",
      "Detection Prevalence   0.5610   0.3293  0.10976  0.00000\n",
      "Balanced Accuracy      0.8524   0.6796  0.66332  0.50000\n",
      "Fold 2 Predicted vs Actual Values:\n",
      "                    PlasmaAlias Actual Predicted\n",
      "1             plasma-2300-P9-LV      3  4.783300\n",
      "2              plasma-2104-P9-N      4  4.100633\n",
      "3             plasma-3627-P9-LV      3  5.029267\n",
      "4              plasma-3134-P9-N      3  4.751867\n",
      "5              plasma-2370-P9-N      3  4.038800\n",
      "6              plasma-3427-P9-N      3  3.149633\n",
      "7              plasma-2273-P9-N      5  3.760933\n",
      "8              plasma-1970-P9-N      3  3.749967\n",
      "9             plasma-3608-P9-LV      5  7.741133\n",
      "10             plasma-2757-P9-N      4  3.063300\n",
      "11             plasma-4027-P9-N      3  3.466000\n",
      "12             plasma-3050-P9-N      3  6.341233\n",
      "13    plasma-2198-t7-5day-P9-CH      3  3.235567\n",
      "14             plasma-746-P9-CH      6  5.307833\n",
      "15             plasma-749-P9-CH     12  7.059267\n",
      "16             plasma-750-P9-CH      6  6.295833\n",
      "17            plasma-1624-P9-LV      3  4.272967\n",
      "18            plasma-1265-P9-LV      5  5.881867\n",
      "19            plasma-1424-P9-CH      3  4.839367\n",
      "20            plasma-1425-P9-CH      3  5.514800\n",
      "21            plasma-1628-P9-CH      3  3.315433\n",
      "22            plasma-1427-P9-CH      3  3.231133\n",
      "23            plasma-2102-P9-CH      3  3.506400\n",
      "24            plasma-1244-P9-CH      3  4.831900\n",
      "25            plasma-1635-P9-CH      3  3.316333\n",
      "26            plasma-1431-P9-CH      3  4.483733\n",
      "27            plasma-3112-P9-CH      3  3.820300\n",
      "28            plasma-2932-P9-CH      3  3.837433\n",
      "29             plasma-739-P9-CH      5  4.976933\n",
      "30            plasma-1224-P9-CH      4  6.755600\n",
      "31            plasma-1246-P9-CH      3  4.998767\n",
      "32            plasma-1646-P9-CH      3  3.184633\n",
      "33            plasma-1647-P9-CH      3  4.291100\n",
      "34            plasma-1437-P9-CH      3  3.297233\n",
      "35            plasma-1208-P9-CH      8  6.550267\n",
      "36            plasma-2057-P9-CH      6  7.036467\n",
      "37            plasma-1441-P9-CH      5  4.086367\n",
      "38            plasma-2260-P9-CH      3  4.611267\n",
      "39            plasma-2740-P9-CH      9  8.064933\n",
      "40            plasma-1206-P9-CH     12  6.660933\n",
      "41            plasma-1124-P9-CH      5  5.617700\n",
      "42            plasma-2486-P9-CH      7  6.191567\n",
      "43            plasma-1650-P9-CH      6  6.620333\n",
      "44            plasma-2791-P9-CH      5  4.170233\n",
      "45            plasma-2823-P9-CH      8  8.332433\n",
      "46            plasma-2227-P9-CH      5  3.619233\n",
      "47            plasma-1207-P9-CH      7  6.158133\n",
      "48            plasma-1203-P9-CH      5  5.740200\n",
      "49            plasma-2483-P9-CH      5  6.423333\n",
      "50            plasma-2241-P9-CH      3  3.784900\n",
      "51            plasma-1446-P9-CH      5  6.345933\n",
      "52            plasma-2286-P9-CH      8  7.034300\n",
      "53            plasma-3059-P9-CH      3  3.313533\n",
      "54            plasma-2503-P9-CH      5  5.673667\n",
      "55            plasma-2490-P9-CH      6  6.156333\n",
      "56            plasma-2462-P9-CH      6  7.297133\n",
      "57            plasma-2381-P9-CH      5  5.057733\n",
      "58            plasma-2017-P9-CH      6  6.059300\n",
      "59            plasma-2078-P9-CH      5  4.002633\n",
      "60            plasma-1662-P9-CH     11  3.879600\n",
      "61            plasma-2202-P9-CH      3  4.057300\n",
      "62 plasma-2564-r1-t4-4day-P9-CH      3  3.596333\n",
      "63            plasma-2459-P9-CH      5  4.418600\n",
      "64            plasma-2787-P9-CH      3  3.669533\n",
      "65            plasma-2779-P9-CH      3  4.701500\n",
      "66            plasma-2074-P9-CH      7  5.936367\n",
      "67            plasma-1956-P9-CH      3  3.107200\n",
      "68            plasma-2488-P9-CH      5  5.701867\n",
      "69            plasma-2294-P9-CH      8  6.910567\n",
      "70            plasma-2451-P9-CH      8  6.550567\n",
      "71            plasma-2215-P9-CH      3  4.672833\n",
      "72            plasma-2796-P9-CH      3  3.133100\n",
      "73            plasma-2332-P9-CH      7  7.427800\n",
      "74            plasma-2277-P9-CH      3  3.704400\n",
      "75            plasma-2458-P9-CH      7  5.450367\n",
      "76            plasma-2785-P9-CH      3  3.245767\n",
      "77            plasma-3409-P9-CH      3  3.589900\n",
      "78            plasma-2010-P9-CH      3  4.427133\n",
      "79            plasma-2738-P9-CH      3  3.826033\n",
      "80            plasma-2304-P9-CH      5  5.584067\n",
      "81       plasma-2574-5day-P9-CH      8  5.983100\n",
      "82            plasma-3448-P9-CH      8  8.587133\n",
      "Fold 2 Accuracy: 0.695121951219512 \n",
      "Fold 2 AUC: 0.70528083028083 \n",
      "Fold 2 Overall Sensitivity: 0.468177655677656 \n",
      "Fold 2 Overall Specificity: 0.879472763618191 \n",
      "\n",
      "Fold 3 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 3 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 40 16  0  0\n",
      "         2  2  7  9  4\n",
      "         3  0  1  3  0\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6098          \n",
      "                 95% CI : (0.4957, 0.7156)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.0483          \n",
      "                                          \n",
      "                  Kappa : 0.3087          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9524  0.29167  0.25000  0.00000\n",
      "Specificity            0.6000  0.74138  0.98571  1.00000\n",
      "Pos Pred Value         0.7143  0.31818  0.75000      NaN\n",
      "Neg Pred Value         0.9231  0.71667  0.88462  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4878  0.08537  0.03659  0.00000\n",
      "Detection Prevalence   0.6829  0.26829  0.04878  0.00000\n",
      "Balanced Accuracy      0.7762  0.51652  0.61786  0.50000\n",
      "Fold 3 Predicted vs Actual Values:\n",
      "                    PlasmaAlias Actual Predicted\n",
      "1              plasma-3931-P9-N      6  3.895433\n",
      "2              plasma-2357-P9-N      3  5.526400\n",
      "3              plasma-2873-P9-N      3  3.416467\n",
      "4             plasma-3038-P9-LV      3  4.811667\n",
      "5              plasma-2091-P9-N      3  3.214200\n",
      "6              plasma-2733-P9-N      7  7.641233\n",
      "7              plasma-2743-P9-N      5  4.396533\n",
      "8              plasma-1991-P9-N      3  3.721033\n",
      "9              plasma-2222-P9-N      3  3.809867\n",
      "10             plasma-2794-P9-N      3  3.736267\n",
      "11             plasma-3077-P9-N      4  3.337067\n",
      "12            plasma-3599-P9-LV      3  4.014400\n",
      "13            plasma-3637-P9-LV      3  5.409000\n",
      "14  plasma-636-r1-t7-4day-P9-CH      3  3.557533\n",
      "15             plasma-640-P9-CH      8  5.286767\n",
      "16  plasma-650-r1-t7-4day-P9-CH      3  3.873567\n",
      "17            plasma-3967-P9-CH      3  4.941167\n",
      "18            plasma-1279-P9-LV      5  5.561400\n",
      "19            plasma-1621-P9-CH      3  3.429967\n",
      "20             plasma-752-P9-CH      7  6.013500\n",
      "21            plasma-1627-P9-CH      3  3.491500\n",
      "22            plasma-2505-P9-CH      3  3.183800\n",
      "23             plasma-743-P9-CH      6  5.097733\n",
      "24             plasma-742-P9-CH      8  7.030600\n",
      "25            plasma-1636-P9-CH      3  3.539567\n",
      "26             plasma-757-P9-CH     10  6.921333\n",
      "27            plasma-3869-P9-CH      3  3.584700\n",
      "28            plasma-1433-P9-CH      3  3.769833\n",
      "29            plasma-2751-P9-CH      3  3.695767\n",
      "30            plasma-2047-P9-CH      3  3.426900\n",
      "31            plasma-1310-P9-CH      3  4.110967\n",
      "32            plasma-1434-P9-CH      6  5.491267\n",
      "33             plasma-766-P9-CH      6  6.535133\n",
      "34             plasma-740-P9-CH      7  6.134767\n",
      "35             plasma-767-P9-CH      5  6.711600\n",
      "36            plasma-1638-P9-CH      3  3.198700\n",
      "37             plasma-769-P9-LV      5  4.726600\n",
      "38            plasma-1645-P9-LV      5  5.058333\n",
      "39            plasma-1990-P9-CH      3  3.488267\n",
      "40            plasma-1125-P9-CH      6  4.439500\n",
      "41            plasma-2312-P9-CH      3  3.797533\n",
      "42            plasma-1649-P9-CH      8  5.130267\n",
      "43            plasma-1442-P9-CH      5  4.336667\n",
      "44            plasma-1280-P9-CH      5  4.970933\n",
      "45         plasma-1188-r1-P9-CH     12  6.801933\n",
      "46            plasma-1443-P9-CH      5  5.628000\n",
      "47            plasma-2427-P9-CH      6  4.970033\n",
      "48            plasma-2477-P9-CH      5  4.977800\n",
      "49            plasma-2041-P9-CH      6  7.299033\n",
      "50            plasma-2748-P9-CH      3  3.433600\n",
      "51            plasma-1123-P9-CH      5  4.588400\n",
      "52            plasma-1958-P9-CH      5  3.217333\n",
      "53            plasma-2907-P9-CH      3  4.020800\n",
      "54         plasma-1139-r1-P9-CH      7  5.897467\n",
      "55            plasma-2237-P9-CH      8  6.058233\n",
      "56            plasma-1248-P9-CH      3  3.768000\n",
      "57            plasma-1658-P9-CH      3  3.322000\n",
      "58            plasma-2768-P9-CH      5  4.458167\n",
      "59            plasma-2461-P9-CH      7  5.328233\n",
      "60            plasma-2870-P9-CH     10  6.148533\n",
      "61            plasma-1661-P9-LV      5  3.642233\n",
      "62            plasma-2135-P9-CH      3  3.425600\n",
      "63            plasma-2282-P9-CH      3  3.479967\n",
      "64            plasma-1859-P9-CH      5  3.744667\n",
      "65            plasma-2782-P9-CH      5  4.681133\n",
      "66            plasma-2521-P9-CH      3  3.610567\n",
      "67            plasma-2440-P9-CH      5  4.763367\n",
      "68            plasma-2760-P9-CH      3  4.677267\n",
      "69            plasma-1302-P9-CH      8  7.227333\n",
      "70            plasma-2804-P9-CH      3  4.058167\n",
      "71            plasma-2030-P9-CH      9  6.148600\n",
      "72            plasma-2176-P9-CH      3  3.268567\n",
      "73            plasma-1982-P9-CH      3  3.413267\n",
      "74            plasma-2848-P9-CH      3  3.134200\n",
      "75            plasma-2407-P9-CH      3  3.428733\n",
      "76            plasma-2274-P9-CH      3  3.100567\n",
      "77    plasma-2572-t2-6day-P9-CH      3  3.081233\n",
      "78            plasma-2039-P9-CH     10  6.842567\n",
      "79            plasma-2250-P9-CH      8  5.189467\n",
      "80            plasma-2368-P9-CH      3  4.069600\n",
      "81 plasma-2560-r1-t1-4day-P9-CH      5  3.938367\n",
      "82       plasma-2577-5day-P9-CH      3  3.405533\n",
      "Fold 3 Accuracy: 0.609756097560976 \n",
      "Fold 3 AUC: 0.773974867724868 \n",
      "Fold 3 Overall Sensitivity: 0.373511904761905 \n",
      "Fold 3 Overall Specificity: 0.831773399014778 \n",
      "\n",
      "Fold 4 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  4 \n",
      "Fold 4 Total Count of Individuals: 81 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35 12  2  0\n",
      "         2  6  9  5  0\n",
      "         3  1  2  5  4\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6049          \n",
      "                 95% CI : (0.4901, 0.7119)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.07373         \n",
      "                                          \n",
      "                  Kappa : 0.3352          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.3913  0.41667  0.00000\n",
      "Specificity            0.6410   0.8103  0.89855  1.00000\n",
      "Pos Pred Value         0.7143   0.4500  0.41667      NaN\n",
      "Neg Pred Value         0.7812   0.7705  0.89855  0.95062\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4321   0.1111  0.06173  0.00000\n",
      "Detection Prevalence   0.6049   0.2469  0.14815  0.00000\n",
      "Balanced Accuracy      0.7372   0.6008  0.65761  0.50000\n",
      "Fold 4 Predicted vs Actual Values:\n",
      "                    PlasmaAlias Actual Predicted\n",
      "1             plasma-2333-P9-LV      3  4.230700\n",
      "2             plasma-2270-P9-LV      3  5.436367\n",
      "3              plasma-1976-P9-N      3  3.623133\n",
      "4              plasma-1959-P9-N      4  7.385933\n",
      "5             plasma-2778-P9-PR      3  4.309733\n",
      "6              plasma-2090-P9-N      6  6.218133\n",
      "7              plasma-2296-P9-N      3  3.285833\n",
      "8              plasma-3956-P9-N      4  6.704667\n",
      "9              plasma-3880-P9-N      3  4.252233\n",
      "10             plasma-3901-P9-N      3  5.376133\n",
      "11            plasma-3462-P9-LV      3  3.228600\n",
      "12             plasma-3377-P9-N      6  3.202767\n",
      "13             plasma-642-P9-CH      3  3.636667\n",
      "14             plasma-644-P9-CH      3  3.785233\n",
      "15             plasma-655-P9-CH      6  6.372867\n",
      "16    plasma-2569-t7-5day-P9-CH      3  4.831233\n",
      "17             plasma-748-P9-CH      5  7.111300\n",
      "18            plasma-1622-P9-CH      3  3.630100\n",
      "19            plasma-2099-P9-CH      3  3.558333\n",
      "20             plasma-771-P9-LV      9  6.984400\n",
      "21            plasma-1327-P9-CH      3  3.937600\n",
      "22            plasma-1626-P9-CH      3  4.659567\n",
      "23            plasma-2970-P9-CH      3  4.034267\n",
      "24            plasma-2446-P9-CH      3  4.352967\n",
      "25             plasma-775-P9-LV      5  5.414500\n",
      "26            plasma-1430-P9-CH      3  3.990167\n",
      "27            plasma-1273-P9-CH      3  4.654667\n",
      "28            plasma-2050-P9-CH      5  4.510200\n",
      "29             plasma-780-P9-LV      5  4.279667\n",
      "30             plasma-762-P9-CH      3  3.473967\n",
      "31            plasma-1435-P9-CH      3  5.509967\n",
      "32            plasma-2208-P9-CH      3  3.402900\n",
      "33             plasma-768-P9-CH      8  7.629333\n",
      "34            plasma-1639-P9-CH      3  3.289100\n",
      "35            plasma-1640-P9-CH      3  3.350733\n",
      "36            plasma-1641-P9-CH      3  3.879167\n",
      "37            plasma-1642-P9-CH      3  3.966067\n",
      "38            plasma-1648-P9-CH      3  3.801433\n",
      "39            plasma-1140-P9-CH      5  4.825767\n",
      "40            plasma-2450-P9-CH      5  5.010767\n",
      "41            plasma-1981-P9-CH      6  5.566700\n",
      "42            plasma-1135-P9-CH      8  4.509867\n",
      "43         plasma-1189-r1-P9-CH      9  4.748933\n",
      "44            plasma-2008-P9-CH      6  5.787133\n",
      "45            plasma-1205-P9-CH      9  7.499533\n",
      "46            plasma-1652-P9-CH      8  5.635533\n",
      "47            plasma-2172-P9-CH      3  3.234700\n",
      "48            plasma-2831-P9-CH      9  6.837200\n",
      "49            plasma-1129-P9-CH      7  7.029700\n",
      "50            plasma-1204-P9-CH     11  7.969733\n",
      "51            plasma-2758-P9-CH      8  6.493333\n",
      "52            plasma-1660-P9-LV      6  6.333100\n",
      "53            plasma-2409-P9-CH      5  3.973933\n",
      "54            plasma-1880-P9-CH      5  4.651100\n",
      "55            plasma-2323-P9-CH      3  3.194467\n",
      "56            plasma-2485-P9-CH      3  3.112767\n",
      "57            plasma-2258-P9-CH      3  5.470833\n",
      "58            plasma-2419-P9-CH      5  5.924367\n",
      "59            plasma-2275-P9-CH     12  8.439433\n",
      "60            plasma-2480-P9-CH      5  4.427100\n",
      "61            plasma-2065-P9-CH      5  7.159833\n",
      "62            plasma-2431-P9-CH      5  4.639267\n",
      "63            plasma-2352-P9-CH     10  8.273733\n",
      "64            plasma-2321-P9-CH      3  3.137733\n",
      "65            plasma-2379-P9-CH      5  3.534500\n",
      "66 plasma-2565-r1-t3-4day-P9-CH      5  4.578100\n",
      "67            plasma-2405-P9-CH     11  8.296067\n",
      "68            plasma-2182-P9-CH      5  4.937433\n",
      "69            plasma-2493-P9-CH      3  3.919067\n",
      "70            plasma-2437-P9-CH      3  4.765800\n",
      "71            plasma-2160-P9-CH      3  3.099367\n",
      "72            plasma-2489-P9-CH      3  6.225467\n",
      "73            plasma-2367-P9-CH      3  4.367733\n",
      "74            plasma-2363-P9-CH      3  3.909067\n",
      "75            plasma-2106-P9-CH      6  4.984400\n",
      "76 plasma-2563-r1-t2-4day-P9-CH      3  3.248267\n",
      "77 plasma-2567-r1-t1-4day-P9-CH      7  5.770933\n",
      "78            plasma-3896-P9-CH      9  8.258033\n",
      "79            plasma-3944-P9-CH      9  8.057867\n",
      "80            plasma-3366-P9-CH      6  6.625167\n",
      "81       plasma-2575-5day-P9-CH      3  3.503833\n",
      "Fold 4 Accuracy: 0.604938271604938 \n",
      "Fold 4 AUC: 0.83211841499885 \n",
      "Fold 4 Overall Sensitivity: 0.410326086956522 \n",
      "Fold 4 Overall Specificity: 0.837480298312382 \n",
      "\n",
      "Fold 5 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  3 \n",
      "Fold 5 Total Count of Individuals: 80 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 40  8  1  0\n",
      "         2  2 13  7  1\n",
      "         3  0  2  4  2\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.7125          \n",
      "                 95% CI : (0.6005, 0.8082)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.0004838       \n",
      "                                          \n",
      "                  Kappa : 0.505           \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9524   0.5652   0.3333   0.0000\n",
      "Specificity            0.7632   0.8246   0.9412   1.0000\n",
      "Pos Pred Value         0.8163   0.5652   0.5000      NaN\n",
      "Neg Pred Value         0.9355   0.8246   0.8889   0.9625\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.5000   0.1625   0.0500   0.0000\n",
      "Detection Prevalence   0.6125   0.2875   0.1000   0.0000\n",
      "Balanced Accuracy      0.8578   0.6949   0.6373   0.5000\n",
      "Fold 5 Predicted vs Actual Values:\n",
      "                    PlasmaAlias Actual Predicted\n",
      "1              plasma-2742-P9-N      3  4.476767\n",
      "2              plasma-2800-P9-N      3  3.137833\n",
      "3              plasma-2518-P9-N      3  4.199233\n",
      "4             plasma-3390-P9-LV      3  3.982467\n",
      "5             plasma-2188-P9-BD      3  6.293367\n",
      "6              plasma-2441-P9-N      3  4.427667\n",
      "7              plasma-2770-P9-N      5  3.767333\n",
      "8              plasma-4036-P9-N      5  5.927100\n",
      "9              plasma-3835-P9-N      4  3.726967\n",
      "10            plasma-3450-P9-BC      3  3.329500\n",
      "11             plasma-632-P9-CH      7  6.836333\n",
      "12     plasma-649-t8-6day-P9-CH      3  3.156200\n",
      "13             plasma-652-P9-CH      7  5.370067\n",
      "14            plasma-3163-P9-CH      3  3.543000\n",
      "15            plasma-2776-P9-CH      3  3.749833\n",
      "16             plasma-753-P9-CH      8  6.469533\n",
      "17            plasma-1625-P9-CH      3  3.747633\n",
      "18  plasma-756-r1-t4-4day-P9-CH      5  4.149733\n",
      "19 plasma-2566-r1-t7-4day-P9-CH      3  3.348033\n",
      "20            plasma-1426-P9-CH      3  4.237867\n",
      "21            plasma-1348-P9-CH      3  4.122833\n",
      "22            plasma-1629-P9-CH      3  4.711833\n",
      "23            plasma-1632-P9-LV      5  5.591833\n",
      "24            plasma-2307-P9-CH      3  3.406767\n",
      "25             plasma-759-P9-CH      6  9.584767\n",
      "26             plasma-760-P9-CH      6  6.559167\n",
      "27             plasma-763-P9-CH      5  6.071033\n",
      "28             plasma-765-P9-CH      5  4.918667\n",
      "29             plasma-735-P9-CH      5  4.586933\n",
      "30            plasma-1436-P9-CH      3  4.118800\n",
      "31            plasma-1643-P9-CH      3  3.803600\n",
      "32            plasma-1236-P9-CH      4  4.240267\n",
      "33            plasma-1127-P9-CH      7  5.104133\n",
      "34            plasma-1438-P9-CH      3  3.256633\n",
      "35            plasma-1439-P9-CH      3  3.125667\n",
      "36            plasma-1440-P9-CH      5  4.666267\n",
      "37            plasma-1136-P9-CH      5  5.078433\n",
      "38            plasma-1069-P9-CH      7  8.197167\n",
      "39            plasma-2156-P9-CH     10  9.599300\n",
      "40            plasma-2517-P9-CH     10  7.584533\n",
      "41            plasma-2802-P9-CH      3  3.482033\n",
      "42            plasma-2544-P9-CH      3  3.377600\n",
      "43            plasma-1202-P9-CH      8  6.230500\n",
      "44            plasma-1651-P9-CH      3  3.415133\n",
      "45            plasma-2511-P9-CH      7  6.622467\n",
      "46            plasma-1445-P9-CH      3  3.143833\n",
      "47            plasma-1869-P9-CH      7  3.933400\n",
      "48            plasma-2221-P9-CH      6  6.583567\n",
      "49            plasma-1654-P9-CH     11  5.606267\n",
      "50            plasma-2246-P9-CH      3  3.530700\n",
      "51            plasma-2256-P9-CH      6  8.528800\n",
      "52            plasma-1448-P9-CH      6  6.282333\n",
      "53            plasma-1965-P9-CH      3  3.628000\n",
      "54            plasma-2173-P9-CH      3  3.516067\n",
      "55            plasma-2285-P9-CH      9  8.670600\n",
      "56            plasma-2212-P9-CH      3  3.637267\n",
      "57            plasma-2143-P9-CH      6  5.055167\n",
      "58            plasma-2898-P9-CH      3  4.793467\n",
      "59            plasma-1664-P9-LV      5  3.672433\n",
      "60            plasma-2096-P9-CH      5  6.404900\n",
      "61            plasma-2391-P9-CH      3  3.278733\n",
      "62 plasma-2559-r1-t4-4day-P9-CH      3  3.176500\n",
      "63            plasma-2268-P9-CH      3  3.375033\n",
      "64            plasma-2331-P9-CH      9  6.196067\n",
      "65            plasma-2067-P9-CH      6  6.209867\n",
      "66            plasma-2463-P9-CH      5  3.855967\n",
      "67            plasma-2235-P9-CH      7  9.471367\n",
      "68            plasma-2347-P9-CH      3  4.219000\n",
      "69            plasma-1222-P9-LV      4  6.861367\n",
      "70            plasma-2449-P9-CH      3  4.224267\n",
      "71            plasma-2259-P9-CH      5  5.524633\n",
      "72            plasma-2355-P9-CH      6  6.203267\n",
      "73            plasma-2239-P9-CH      8  7.251367\n",
      "74            plasma-2428-P9-CH      6  4.856467\n",
      "75            plasma-2371-P9-CH      3  4.068633\n",
      "76            plasma-3331-P9-CH      3  3.543300\n",
      "77            plasma-2867-P9-CH      3  3.949933\n",
      "78            plasma-2502-P9-CH      3  4.779333\n",
      "79            plasma-2094-P9-CH      5  6.710733\n",
      "80    plasma-2573-t2-5day-P9-CH      3  3.397700\n",
      "Fold 5 Accuracy: 0.7125 \n",
      "Fold 5 AUC: 0.828056705774097 \n",
      "Fold 5 Overall Sensitivity: 0.462732919254658 \n",
      "Fold 5 Overall Specificity: 0.882223942208462 \n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6537316 \n",
      "Average AUC: 0.7949166 \n",
      "Average Sensitivity: 0.4268783 \n",
      "Average Specificity: 0.8568859 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(randomForest)\n",
    "\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # Fit the Random Forest model using the filtered training data\n",
    "    rf_model <- randomForest(child_pugh_score ~ ., data = training_set_filtered, importance = TRUE)\n",
    "\n",
    "    # Predict on the filtered testing set\n",
    "    predictions <- predict(rf_model, newdata = testing_set_filtered)\n",
    "\n",
    "    # Classify predictions and actual values\n",
    "    classified_predictions <- sapply(predictions, classify)\n",
    "    classified_actuals <- sapply(testing_set$child_pugh_score, classify)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(classified_predictions, levels = 1:4)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "    \n",
    "    # Print the number of individuals in each class for the confusion matrix\n",
    "    class_count <- table(classified_actuals)\n",
    "    cat(paste(\"\\nFold\", i, \"Class Distribution (Actual):\\n\"))\n",
    "    print(class_count)\n",
    "    total_count <- length(classified_actuals)\n",
    "    cat(paste(\"Fold\", i, \"Total Count of Individuals:\", total_count, \"\\n\"))\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Print predicted and actual values\n",
    "    results_df <- data.frame(\n",
    "    PlasmaAlias = testing_set$plasma_alias,\n",
    "    Actual = testing_set$child_pugh_score,\n",
    "    Predicted = predictions\n",
    "    )\n",
    "    cat(paste(\"Fold\", i, \"Predicted vs Actual Values:\\n\"))\n",
    "    print(results_df)\n",
    "\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "}\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e97996",
   "metadata": {},
   "source": [
    "# Random Forest Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7fb585a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 1 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 40  9  1  0\n",
      "         2  1 12  7  1\n",
      "         3  1  3  4  3\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6829          \n",
      "                 95% CI : (0.5708, 0.7813)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.001285        \n",
      "                                          \n",
      "                  Kappa : 0.4654          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9524   0.5000  0.33333  0.00000\n",
      "Specificity            0.7500   0.8448  0.90000  1.00000\n",
      "Pos Pred Value         0.8000   0.5714  0.36364      NaN\n",
      "Neg Pred Value         0.9375   0.8033  0.88732  0.95122\n",
      "Prevalence             0.5122   0.2927  0.14634  0.04878\n",
      "Detection Rate         0.4878   0.1463  0.04878  0.00000\n",
      "Detection Prevalence   0.6098   0.2561  0.13415  0.00000\n",
      "Balanced Accuracy      0.8512   0.6724  0.61667  0.50000\n",
      "Fold 1 Accuracy: 0.682926829268293 \n",
      "Fold 1 AUC: 0.828125 \n",
      "Fold 1 Overall Sensitivity: 0.446428571428571 \n",
      "Fold 1 Overall Specificity: 0.873706896551724 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 13  3 \n",
      "Fold 2 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 35  7  0  1\n",
      "         2  5 12  8  1\n",
      "         3  2  5  5  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6341          \n",
      "                 95% CI : (0.5205, 0.7378)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.01746         \n",
      "                                          \n",
      "                  Kappa : 0.4036          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : 0.42032         \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8333   0.5000  0.38462  0.00000\n",
      "Specificity            0.8000   0.7586  0.88406  1.00000\n",
      "Pos Pred Value         0.8140   0.4615  0.38462      NaN\n",
      "Neg Pred Value         0.8205   0.7857  0.88406  0.96341\n",
      "Prevalence             0.5122   0.2927  0.15854  0.03659\n",
      "Detection Rate         0.4268   0.1463  0.06098  0.00000\n",
      "Detection Prevalence   0.5244   0.3171  0.15854  0.00000\n",
      "Balanced Accuracy      0.8167   0.6293  0.63434  0.50000\n",
      "Fold 2 Accuracy: 0.634146341463415 \n",
      "Fold 2 AUC: 0.672599969474969 \n",
      "Fold 2 Overall Sensitivity: 0.42948717948718 \n",
      "Fold 2 Overall Specificity: 0.860669665167416 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 24 12  4 \n",
      "Fold 3 Total Count of Individuals: 82 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 38 14  3  0\n",
      "         2  4  8  7  3\n",
      "         3  0  2  2  1\n",
      "         4  0  0  0  0\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.5854          \n",
      "                 95% CI : (0.4712, 0.6932)\n",
      "    No Information Rate : 0.5122          \n",
      "    P-Value [Acc > NIR] : 0.112           \n",
      "                                          \n",
      "                  Kappa : 0.2713          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.9048  0.33333  0.16667  0.00000\n",
      "Specificity            0.5750  0.75862  0.95714  1.00000\n",
      "Pos Pred Value         0.6909  0.36364  0.40000      NaN\n",
      "Neg Pred Value         0.8519  0.73333  0.87013  0.95122\n",
      "Prevalence             0.5122  0.29268  0.14634  0.04878\n",
      "Detection Rate         0.4634  0.09756  0.02439  0.00000\n",
      "Detection Prevalence   0.6707  0.26829  0.06098  0.00000\n",
      "Balanced Accuracy      0.7399  0.54598  0.56190  0.50000\n",
      "Fold 3 Accuracy: 0.585365853658537 \n",
      "Fold 3 AUC: 0.761987433862434 \n",
      "Fold 3 Overall Sensitivity: 0.351190476190476 \n",
      "Fold 3 Overall Specificity: 0.822690886699507 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  4 \n",
      "Fold 4 Total Count of Individuals: 81 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 36  8  1  0\n",
      "         2  4 13  7  0\n",
      "         3  2  2  4  3\n",
      "         4  0  0  0  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6667          \n",
      "                 95% CI : (0.5532, 0.7676)\n",
      "    No Information Rate : 0.5185          \n",
      "    P-Value [Acc > NIR] : 0.004935        \n",
      "                                          \n",
      "                  Kappa : 0.4509          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8571   0.5652  0.33333  0.25000\n",
      "Specificity            0.7692   0.8103  0.89855  1.00000\n",
      "Pos Pred Value         0.8000   0.5417  0.36364  1.00000\n",
      "Neg Pred Value         0.8333   0.8246  0.88571  0.96250\n",
      "Prevalence             0.5185   0.2840  0.14815  0.04938\n",
      "Detection Rate         0.4444   0.1605  0.04938  0.01235\n",
      "Detection Prevalence   0.5556   0.2963  0.13580  0.01235\n",
      "Balanced Accuracy      0.8132   0.6878  0.61594  0.62500\n",
      "Fold 4 Accuracy: 0.666666666666667 \n",
      "Fold 4 AUC: 0.859975270301357 \n",
      "Fold 4 Overall Sensitivity: 0.501423395445135 \n",
      "Fold 4 Overall Specificity: 0.869531580363664 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“Zero sample variances detected, have been offset away from zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5 Class Distribution (Actual):\n",
      "classified_actuals\n",
      " 1  2  3  4 \n",
      "42 23 12  3 \n",
      "Fold 5 Total Count of Individuals: 80 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n",
      "Setting direction: controls < cases\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Confusion Matrix:\n",
      "Confusion Matrix and Statistics\n",
      "\n",
      "          Reference\n",
      "Prediction  1  2  3  4\n",
      "         1 37  5  1  0\n",
      "         2  5 13  7  1\n",
      "         3  0  4  2  1\n",
      "         4  0  1  2  1\n",
      "\n",
      "Overall Statistics\n",
      "                                          \n",
      "               Accuracy : 0.6625          \n",
      "                 95% CI : (0.5481, 0.7645)\n",
      "    No Information Rate : 0.525           \n",
      "    P-Value [Acc > NIR] : 0.008869        \n",
      "                                          \n",
      "                  Kappa : 0.4462          \n",
      "                                          \n",
      " Mcnemar's Test P-Value : NA              \n",
      "\n",
      "Statistics by Class:\n",
      "\n",
      "                     Class: 1 Class: 2 Class: 3 Class: 4\n",
      "Sensitivity            0.8810   0.5652   0.1667   0.3333\n",
      "Specificity            0.8421   0.7719   0.9265   0.9610\n",
      "Pos Pred Value         0.8605   0.5000   0.2857   0.2500\n",
      "Neg Pred Value         0.8649   0.8148   0.8630   0.9737\n",
      "Prevalence             0.5250   0.2875   0.1500   0.0375\n",
      "Detection Rate         0.4625   0.1625   0.0250   0.0125\n",
      "Detection Prevalence   0.5375   0.3250   0.0875   0.0500\n",
      "Balanced Accuracy      0.8615   0.6686   0.5466   0.6472\n",
      "Fold 5 Accuracy: 0.6625 \n",
      "Fold 5 AUC: 0.802564987347596 \n",
      "Fold 5 Overall Sensitivity: 0.486542443064182 \n",
      "Fold 5 Overall Specificity: 0.875386159248388 \n",
      "Aggregate Results Across All Folds:\n",
      "Average Accuracy: 0.6463211 \n",
      "Average AUC: 0.7850505 \n",
      "Average Sensitivity: 0.4430144 \n",
      "Average Specificity: 0.860397 \n",
      "\n",
      "All AUC values across folds:  0.828125, 0.672599969474969, 0.761987433862434, 0.859975270301357, 0.802564987347596 \n",
      "All Sensitivities across folds:  0.446428571428571, 0.42948717948718, 0.351190476190476, 0.501423395445135, 0.486542443064182 \n",
      "All Specificities across folds:  0.873706896551724, 0.860669665167416, 0.822690886699507, 0.869531580363664, 0.875386159248388 \n",
      "\n",
      "Final Combined Confusion Matrix:\n",
      "          Reference\n",
      "Prediction   1   2   3   4\n",
      "         1 186  43   6   1\n",
      "         2  19  58  36   6\n",
      "         3   5  16  17   9\n",
      "         4   0   1   2   2\n",
      "Final Overall Accuracy:  0.6461916 \n",
      "Final Sensitivity by class:  0.788135593220339, 0.487394957983193, 0.361702127659574, 0.4 \n",
      "Final Specificity by class:  0.885714285714286, 0.491525423728814, 0.278688524590164, 0.111111111111111 \n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(pROC)\n",
    "library(randomForest)\n",
    "\n",
    "\n",
    "# Define classification thresholds\n",
    "classify <- function(value) {\n",
    "  if (value < 5) {\n",
    "    return(1)\n",
    "  } else if (value >= 5 & value < 7) {\n",
    "    return(2)\n",
    "  } else if (value >= 7 & value < 10) {\n",
    "    return(3)\n",
    "  } else if (value >= 10 & value < 15) {\n",
    "    return(4)\n",
    "  } else {\n",
    "    return(5)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Number of folds\n",
    "k <- 5\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_conf_matrices <- list()\n",
    "all_auc <- numeric(k)\n",
    "all_accuracy <- numeric(k)\n",
    "all_sensitivity <- numeric(k)\n",
    "all_specificity <- numeric(k)\n",
    "all_top_markers <- list()\n",
    "\n",
    "# Perform k-fold cross-validation manually\n",
    "for (i in 1:k) {\n",
    "    # Use the preloaded training and testing sets\n",
    "    training_set <- training_sets[[i]]\n",
    "    testing_set <- testing_sets[[i]]\n",
    "\n",
    "    # Ensure the column names are correct for column removal\n",
    "    columns_to_remove <- colnames(filtered_pheno_data)\n",
    "    columns_to_remove <- c(columns_to_remove, \"rowname\") # Adjust if needed\n",
    "\n",
    "    # Remove specified columns from the training data\n",
    "    methylation_training_data_filtered <- training_set[, !colnames(training_set) %in% columns_to_remove]\n",
    "\n",
    "    # Create the design matrix\n",
    "    design_train <- model.matrix(~ child_pugh_score, data = training_set)\n",
    "\n",
    "    # Remove 'child_pugh_score' column from methylation data\n",
    "    methylation_data_no_child_pugh <- methylation_training_data_filtered[, !colnames(methylation_training_data_filtered) %in% \"child_pugh_score\"]\n",
    "\n",
    "    # Fit the linear model\n",
    "    fit <- lmFit(t(methylation_data_no_child_pugh), design_train)\n",
    "    fit <- eBayes(fit)\n",
    "\n",
    "    # Get top 100 significant markers\n",
    "    top_100_results <- topTable(fit, coef = 2, number = 100)\n",
    "    top_100_markers <- rownames(top_100_results)\n",
    "    all_top_markers[[i]] <- top_100_markers\n",
    "\n",
    "    # Filter training and testing data to include only the top 100 markers\n",
    "    training_set_filtered <- training_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "    testing_set_filtered <- testing_set[, c(\"child_pugh_score\", top_100_markers)]\n",
    "\n",
    "    # **Classify training `child_pugh_score` before model training**\n",
    "    classified_train_scores <- sapply(as.numeric(as.character(training_set_filtered$child_pugh_score)), classify)\n",
    "    training_set_filtered$child_pugh_score <- factor(classified_train_scores, levels = 1:4)\n",
    "\n",
    "    # Fit the Random Forest model using the filtered training data\n",
    "    model <- randomForest(child_pugh_score ~ ., data = training_set_filtered)\n",
    "\n",
    "    # Predict on the filtered testing set (probabilities)\n",
    "    prob_predictions <- predict(model, newdata = testing_set_filtered, type = \"prob\")\n",
    "\n",
    "    # Classify predictions based on the highest probability\n",
    "    predicted_classes <- apply(prob_predictions, 1, function(row) {\n",
    "    which.max(row) # This returns the class with the highest probability\n",
    "    })\n",
    "\n",
    "    # Convert `child_pugh_score` to numeric before classification for testing set\n",
    "    classified_actuals <- sapply(as.numeric(as.character(testing_set$child_pugh_score)), classify)\n",
    "    classified_actuals <- factor(classified_actuals, levels = 1:4)\n",
    "\n",
    "    # Ensure they are factors with levels 1, 2, 3, 4\n",
    "    classified_predictions <- factor(predicted_classes, levels = 1:4)\n",
    "\n",
    "    # Check for NA values in classified predictions and actual values\n",
    "    valid_indices <- !is.na(classified_predictions) & !is.na(classified_actuals)\n",
    "    classified_predictions <- classified_predictions[valid_indices]\n",
    "    classified_actuals <- classified_actuals[valid_indices]\n",
    "\n",
    "    # Debugging: Print predicted and actual values before classification\n",
    "    #cat(paste(\"Fold\", i, \"Predicted Values (probabilities):\\n\"))\n",
    "    #print(prob_predictions)\n",
    "    #cat(paste(\"Fold\", i, \"Actual Values (before classification):\\n\"))\n",
    "    #print(as.numeric(as.character(testing_set$child_pugh_score)))\n",
    "\n",
    "    # Debugging: Print classified predicted and actual values\n",
    "    #cat(paste(\"Fold\", i, \"Classified Predicted Values:\\n\"))\n",
    "    #print(classified_predictions)\n",
    "    #cat(paste(\"Fold\", i, \"Classified Actual Values:\\n\"))\n",
    "    #print(classified_actuals)\n",
    "\n",
    "    # Print the number of individuals in each class for the confusion matrix\n",
    "    class_count <- table(classified_actuals)\n",
    "    cat(paste(\"\\nFold\", i, \"Class Distribution (Actual):\\n\"))\n",
    "    print(class_count)\n",
    "    total_count <- length(classified_actuals)\n",
    "    cat(paste(\"Fold\", i, \"Total Count of Individuals:\", total_count, \"\\n\"))\n",
    "    \n",
    "    # Create a confusion matrix\n",
    "    conf_matrix <- confusionMatrix(classified_predictions, classified_actuals)\n",
    "    all_conf_matrices[[i]] <- conf_matrix\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy <- sum(classified_predictions == classified_actuals) / length(classified_actuals)\n",
    "    all_accuracy[i] <- accuracy\n",
    "\n",
    "    # Calculate and store AUC\n",
    "    roc_multiclass <- multiclass.roc(classified_actuals, as.numeric(classified_predictions))\n",
    "    auc_value <- auc(roc_multiclass)\n",
    "    all_auc[i] <- auc_value\n",
    "\n",
    "    # Extract sensitivity and specificity from the confusion matrix\n",
    "    sensitivity_by_class <- conf_matrix$byClass[,\"Sensitivity\"]\n",
    "    overall_sensitivity <- mean(sensitivity_by_class, na.rm = TRUE)\n",
    "    all_sensitivity[i] <- overall_sensitivity\n",
    "\n",
    "    specificity_by_class <- conf_matrix$byClass[,\"Specificity\"]\n",
    "    overall_specificity <- mean(specificity_by_class, na.rm = TRUE)\n",
    "    all_specificity[i] <- overall_specificity\n",
    "\n",
    "    # Print results for the current fold\n",
    "    cat(paste(\"Fold\", i, \"Confusion Matrix:\\n\"))\n",
    "    print(conf_matrix)\n",
    "    cat(paste(\"Fold\", i, \"Accuracy:\", accuracy, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"AUC:\", auc_value, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Sensitivity:\", overall_sensitivity, \"\\n\"))\n",
    "    cat(paste(\"Fold\", i, \"Overall Specificity:\", overall_specificity, \"\\n\"))\n",
    "}\n",
    "\n",
    "# Print aggregate results\n",
    "cat(\"Aggregate Results Across All Folds:\\n\")\n",
    "cat(\"Average Accuracy:\", mean(all_accuracy), \"\\n\")\n",
    "cat(\"Average AUC:\", mean(all_auc), \"\\n\")\n",
    "cat(\"Average Sensitivity:\", mean(all_sensitivity), \"\\n\")\n",
    "cat(\"Average Specificity:\", mean(all_specificity), \"\\n\")\n",
    "shared_markers <- Reduce(intersect, all_top_markers)\n",
    "\n",
    "# Print all AUCs, sensitivities, and specificities next to one another\n",
    "cat(\"\\nAll AUC values across folds: \", paste(all_auc, collapse = \", \"), \"\\n\")\n",
    "cat(\"All Sensitivities across folds: \", paste(all_sensitivity, collapse = \", \"), \"\\n\")\n",
    "cat(\"All Specificities across folds: \", paste(all_specificity, collapse = \", \"), \"\\n\")\n",
    "\n",
    "# Combine the confusion matrices\n",
    "# Initialize an empty matrix for summing the confusion matrices\n",
    "final_conf_matrix <- all_conf_matrices[[1]]$table\n",
    "for (i in 2:k) {\n",
    "    final_conf_matrix <- final_conf_matrix + all_conf_matrices[[i]]$table\n",
    "}\n",
    "\n",
    "# Print the final combined confusion matrix\n",
    "cat(\"\\nFinal Combined Confusion Matrix:\\n\")\n",
    "print(final_conf_matrix)\n",
    "\n",
    "# Aggregate performance metrics from the final confusion matrix\n",
    "total_predictions <- sum(final_conf_matrix)\n",
    "overall_accuracy <- sum(diag(final_conf_matrix)) / total_predictions\n",
    "\n",
    "# Calculate overall sensitivity and specificity for each class\n",
    "sensitivity_final <- diag(final_conf_matrix) / rowSums(final_conf_matrix)\n",
    "specificity_final <- diag(final_conf_matrix) / colSums(final_conf_matrix)\n",
    "\n",
    "# Print final performance metrics\n",
    "cat(\"Final Overall Accuracy: \", overall_accuracy, \"\\n\")\n",
    "cat(\"Final Sensitivity by class: \", paste(sensitivity_final, collapse = \", \"), \"\\n\")\n",
    "cat(\"Final Specificity by class: \", paste(specificity_final, collapse = \", \"), \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 4.3.0 gcc 10.2.0",
   "language": "R",
   "name": "ir430-gcc102"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
